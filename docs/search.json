[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rachid Sahli",
    "section": "",
    "text": "Je suis √©tudiant en Science des Donn√©es √† l‚ÄôIUT Paris Rives de Seine et alternant √† l‚ÄôINSEE. Au sein de la Direction des Statistiques D√©mographiques et Sociales, je travaille sur des probl√©matiques li√©es √† la mesure de la couverture des bases de sondage.\nJe travaille principalement avec R et Python et j‚Äôai cr√©√© un site de ressources appel√© PratiqueR. Vous pouvez √©galement me retrouver sur YouTube, o√π je partage des vid√©os sur R et son environnement.\nEn-dehors de mes √©tudes, je consacre mon temps libre √† la construction de robots et je partage mes id√©es sur mon blog, Jiqiren.\nPassionn√© par les statistiques, la robotique ü¶øü§ñ, le v√©lo üö≤, le cin√©ma üéûÔ∏è, et la programmation üëæ, j‚Äôaime explorer et partager mes d√©couvertes dans ces domaines.\nJe suis tr√®s curieux et toujours ouvert √† de nouvelles id√©es. N‚Äôh√©sitez pas √† me contacter pour discuter d‚Äôun projet ou explorer des opportunit√©s de collaboration."
  },
  {
    "objectID": "projets/fontaines_eau.html",
    "href": "projets/fontaines_eau.html",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "",
    "text": "Ce mini-projet utilise le langage R pour cr√©er une carte interactive des points d‚Äôeau publics dans la ville de Paris. Il inclut √©galement une application Shiny, permettant une exploration dynamique et interactive des donn√©es."
  },
  {
    "objectID": "projets/fontaines_eau.html#aper√ßu-de-lapplication",
    "href": "projets/fontaines_eau.html#aper√ßu-de-lapplication",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "1 Aper√ßu de l‚Äôapplication",
    "text": "1 Aper√ßu de l‚Äôapplication\n\n\n\n\n\nL‚Äôobjectif principal de ce projet est de proposer une visualisation g√©ographique claire et accessible des points d‚Äôeau r√©partis dans Paris, gr√¢ce √† une interface conviviale d√©velopp√©e avec Shiny. Les donn√©es exploit√©es proviennent de la Ville de Paris et sont accessibles sur le site data.gouv. Cette application, √† la fois fluide et interactive, permet aux utilisateurs d‚Äôexplorer facilement les diff√©rents points d‚Äôeau pr√©sents dans chaque arrondissement de la capitale."
  },
  {
    "objectID": "projets/fontaines_eau.html#acc√©dez-√†-lapplication",
    "href": "projets/fontaines_eau.html#acc√©dez-√†-lapplication",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "2 Acc√©dez √† l‚Äôapplication",
    "text": "2 Acc√©dez √† l‚Äôapplication\nAcc√©der √† l‚Äôapplication ici\nRepo github"
  },
  {
    "objectID": "projets/projets_but/louvre/louvre.html",
    "href": "projets/projets_but/louvre/louvre.html",
    "title": "Nocturne au mus√©e du Louvre",
    "section": "",
    "text": "Tous les vendredis, le mus√©e du Louvre offre un moment de magie √† ses visiteurs au milieu de ses collections. Au programme de ces nocturnes hebdomadaires, de nombreuses activit√©s pour petits et grands. Le 24 novembre 2023, mes camarades de l‚ÄôIUT et moi avons pu participer √† l‚Äôune de ces soir√©es. Nous avons d√©cid√© de pr√©senter certains tableaux du c√©l√®bre mus√©e au filtre de l‚ÄôIA. Au moment o√π cette technologie prend une place active dans notre soci√©t√©, avec par exemple ChatGPT, DALL-E ou encore Mistral AI, il est int√©ressant de pouvoir approfondir l‚Äôutilisation de ces outils permettant de g√©n√©rer des √©l√©ments en lien, ici, avec l‚Äôart, par exemple. De plus, ce sujet fascinant qu‚Äôest l‚Äôintelligence artificielle est plus ou moins en lien avec notre formation en science des donn√©es. Explorer ce domaine o√π se m√™lent math√©matiques, informatique et donn√©es √©tait particuli√®rement captivant.\n\nQu‚Äôavons nous fait ?\nL‚Äôobjectif final √©tait de pouvoir le 23 novembre 2024, pr√©senter une ≈ìuvre en rapport avec l‚ÄôIA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons d√©cid√© de choisir un sujet d‚Äôintelligence artificielle qui pouvait co√Øncider avec une ≈ìuvre du mus√©e. Apr√®s de longues recherches passionnantes, nous avons choisi de pr√©senter les GAN (Generative adversarial networks). C‚Äôest une classe d‚Äôalgorithmes d‚Äôapprentissage non supervis√©. Ils permettent de g√©n√©rer des images avec un fort degr√© de r√©alisme. Nous avons trouv√© la technologie et les m√©thodes tr√®s int√©ressantes, d‚Äôautant plus qu‚Äôelles aboutissent √† des applications concr√®tes, telles que la g√©n√©ration d‚Äôimages artistiques. Cependant, ces algorithmes sont utilis√©s dans bien d‚Äôautres domaines tels que la m√©decine ou encore la finance. Mais concr√®tement, comment √ßa marche ?\n\n\nQu‚Äôest-ce qu‚Äôun GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL‚Äôartiste (G√©n√©rateur) : L‚Äôobjectif du joueur est de cr√©er des images qui se rapprochent le plus de la r√©alit√©. Pour cela, il apprend de ses erreurs et r√©essaie en continu d‚Äôobtenir l‚Äô≈ìuvre la plus proche du r√©el.\nLe juge (Discriminateur) : L‚Äôobjectif de ce joueur est de v√©rifier si les ≈ìuvres r√©alis√©es par l‚Äôartiste peuvent para√Ætre r√©elles ou si elles sont encore trop fausses. Pour cela, il compare les ≈ìuvres de l‚Äôartiste √† des ≈ìuvres r√©elles faites par des peintres.\n\nTant que l‚Äôimage n‚Äôest pas accept√© par le juge, l‚Äôartiste continue √† produire des ≈ìuvres. Voil√†, le fonctionnement d‚Äôun GAN ou deux r√©seaux de neurones se font concurrence. De cette mani√®re, il est possible de cr√©er des images, des vid√©os ou d‚Äôautres contenues de tr√®s bonnes qualit√©s.\n\n\n\n\n\n\n\nSch√©ma GAN\n\n\n\n\nPr√©sentation au public\nNous avons d√©cid√© de pr√©senter au public le travail d‚ÄôObvious. Ce collectif de chercheurs, d‚Äôartistes travaille avec des mod√®les d‚Äôapprentissage profond pour explorer le potentiel cr√©atif de l‚Äôintelligence artificielle. Ils ont justement utilis√© des GAN pour g√©n√©rer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d‚ÄôEdmond de Belamy. Ce tableau est une impression sur toile qui est rentr√©e dans l‚Äôhistoire de l‚Äôart moderne. Cela, car c‚Äôest la premi√®re ≈ìuvre d‚Äôart produite par un logiciel d‚Äôintelligence artificielle √† √™tre pr√©sent√©e dans une salle des ventes. Pour couronner le tout, il a √©t√© vendu 432 500 dollars chez Christie‚Äôs le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est tr√®s difficile √† premi√®re vue de d√©terminer qu‚Äôune machine a pu en √™tre l‚Äôauteur.   Obvious √† utiliser un GAN, en l‚Äôentra√Ænant sur 15 000 portraits classiques r√©alis√©s entre le 14e et 20e si√®cle. L‚Äôalgorithme devait donc produire un tableau en sortie qui serait tr√®s ressemblant aux portraits classiques. Nous avons d√©cid√© de comparer le portrait d‚ÄôEdmond de Belamy √† une ≈ìuvre du Louvre se trouvant dans la salle 846 de l‚Äôaile Richelieu du mus√©e. C‚Äôest une peinture datant du 17e si√®cle r√©alis√©e par Jean Bray, peintre n√©erlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\nPortrait d‚ÄôEdmond de Belamy, Collectif Obvious\n\n\n\n\n\n\n\n\n\n\n\nPortrait d‚Äôhomme, 1658\n\n\n\n\n\nLes visiteurs √©taient agr√©ablement surpris par le r√©alisme du portrait d‚ÄôEdmond de Belamy, mais aussi par le prix de vente de l‚Äô≈ìuvre. Ils pensaient pouvoir reconna√Ætre la r√©alisation d‚Äôune IA."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "",
    "text": "L‚Äôobjectif est d‚Äôanalyser et de mod√©liser statistiquement une s√©rie temporelle li√©e √† la production d‚Äô√©lectricit√© par combustion aux √âtats-Unis entre 2001 et 2022. Nous √©tudions l‚Äô√©volution de cette s√©rie au fil du temps afin d‚Äôidentifier ses principales tendances et composantes. Cette s√©rie est issue du site web de l‚ÄôAdministration am√©ricaine de l‚Äôinformation sur l‚Äô√©nergie. Nous r√©alisons ensuite une application Shiny afin de visualiser les r√©sultats interactivement.\nLe d√©pot github du projet contenant le code est disponible ici."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#tendance-de-la-s√©rie",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#tendance-de-la-s√©rie",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "Tendance de la s√©rie",
    "text": "Tendance de la s√©rie\nAfin de mod√©liser la s√©rie, nous commen√ßons par analyser la tendance. Nous utilisons un filtre de moyennes mobiles simples et centr√©es, ainsi qu‚Äôune r√©gression des moyennes annuelles, dans le but de visualiser l‚Äô√©volution g√©n√©rale de la s√©rie.\nLes moyennes mobiles permettent de lisser la s√©rie et d‚Äôatt√©nuer les fluctuations al√©atoires. La moyenne mobile simple est calcul√©e sur une fen√™tre fixe, tandis que la moyenne mobile centr√©e est d√©termin√©e de mani√®re sym√©trique autour de chaque point. Cela permet d‚Äôidentifier la tendance sous-jacente de la s√©rie entre 2001 et 2022, qui s‚Äôest av√©r√©e √™tre d√©croissante. On se sert √©galement de la moyenne mobile pour √©l√©miner la composante saisonni√®re de p√©riode \\(p\\) de notre s√©rie et r√©duire au maximum l‚Äôamplitude des fluctuations irr√©guli√®res.\nLa moyenne mobile qui s‚Äôajuste le mieux √† la s√©rie est celle d‚Äôordre 12.\nDe plus, la courbe de r√©gression des moyennes annuelles permet de mod√©liser la tendance √† long terme de la s√©rie en s‚Äôappuyant sur les moyennes calcul√©es pour chaque ann√©e. On trace sur notre nuage de point une courbe lin√©aire qui nous indique une tendance d√©croissante de la production d‚Äô√©l√©ctricit√© par combustion."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#d√©composition-de-la-s√©rie",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#d√©composition-de-la-s√©rie",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "D√©composition de la s√©rie",
    "text": "D√©composition de la s√©rie\n√Ä pr√©sent, nous proc√©dons √† la d√©composition de notre s√©rie √† l‚Äôaide d‚Äôun mod√®le additif. Il s‚Äô√©crit de la mani√®re suivante :\n\\[ y_i = f_i + s_i + e_i \\quad \\text{pour } i = 1, \\dots, n \\quad \\text{avec }  \\sum_{j=1}^{p} s_j = 0 \\quad \\text{et} \\quad \\sum_{j=1}^{n} e_j = 0 \\]\nDans ce mod√®le, l‚Äôamplitude de la composante saisonni√®re et du bruit reste constante au cours du temps. Cela se traduit graphiquement par des fluctuations autour de la tendance d‚Äôamplitude constante. L‚Äôutilisation de la m√©thode de la bande, qui consiste √† tracer la courbe reliant les minima sur une p√©riode ainsi que celle reliant les maxima, a montr√© que ces deux courbes sont parall√®les, indiquant ainsi que le mod√®le est additif.\nOn rappelle qu‚Äôune s√©rie chronologique r√©sulte de trois composantes fondamentales : la tendance (\\(f_i\\)), la composante saisonni√®re (\\(s_i\\)) ou saisonnalit√©, et la composante r√©siduelle (\\(e_i\\)).\nNous analysons d‚Äôabord l‚Äôeffet des saisons en calculant les coefficients de variation saisonni√®re (CVS). Ces variations, qui reviennent chaque ann√©e √† la m√™me p√©riode, sont influenc√©es par des ph√©nom√®nes naturels ou √©conomiques (comme la m√©t√©o ou les f√™tes) mais n‚Äôont pas d‚Äôimpact durable sur la tendance.\nEnfin, nous nous int√©ressons √† la composante r√©siduelle, qui regroupe toutes les variations non expliqu√©es par la tendance et la saisonnalit√©. Ces fluctuations peuvent √™tre dues √† des √©v√©nements impr√©vus (crises √©conomiques, accidents, conditions exceptionnelles) ou √† des variations al√©atoires.\n\n\n\nD√©composition de la s√©rie"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#pr√©vision-de-la-s√©rie",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#pr√©vision-de-la-s√©rie",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "Pr√©vision de la s√©rie",
    "text": "Pr√©vision de la s√©rie\nL‚Äôobjectif de cette √©tape est de r√©aliser deux pr√©visions (2022 et 2023) de la production d‚Äô√©lectricit√© par combustion en utilisant trois m√©thodes diff√©rentes : Trend + Saison, ARMA et Holt-Winters.\nLa m√©thode Trend + Saison, identifie deux composantes principales dans notre s√©rie qui sont la tendance et la saisonnalit√©. La m√©thode ARMA repose sur l‚Äôid√©e que la production d‚Äô√©lectricit√© √† un instant donn√© d√©pend des valeurs pass√©es et des erreurs pass√©es. Pour garantir des pr√©visions optimales, nous avons s√©lectionn√© les param√®tres du mod√®le ARMA en fonction des performances statistiques obtenues. Enfin, le mod√®le Holt-Winters est une extension de lissage exponentiel qui prend en compte trois √©l√©ments, la tendance, la saisonnalit√© et la moyenne ajust√©e de la s√©rie.\nLa pr√©vision sur les donn√©es de 2022 a permis de comparer les valeurs estim√©es avec les valeurs r√©elles de la s√©rie, afin d‚Äô√©valuer la pr√©cision des trois m√©thodes utilis√©es. Pour cela, nous avons calcul√© l‚Äôerreur quadratique moyenne (RMSE) pour chacune des m√©thodes, ce qui nous a permis d‚Äôidentifier celle offrant les pr√©visions les plus fiables.\nParmi les trois approches test√©es, la m√©thode Trend + Saison s‚Äôest r√©v√©l√©e √™tre la plus performante, car elle a obtenu l‚Äôerreur quadratique moyenne la plus faible. Cela s‚Äôexplique par sa capacit√© √† capturer √† la fois la tendance de long terme et les variations saisonni√®res, qui jouent un r√¥le cl√© dans l‚Äô√©volution de la production d‚Äô√©lectricit√©. En revanche, les mod√®les ARMA et Holt-Winters ont montr√© des √©carts plus importants avec les valeurs r√©elles, probablement en raison de leurs hypoth√®ses sous-jacentes qui s‚Äôadaptent moins bien aux dynamiques sp√©cifiques de notre s√©rie temporelle."
  },
  {
    "objectID": "blog/carte_volontaires/carte_volontaires.html",
    "href": "blog/carte_volontaires/carte_volontaires.html",
    "title": "Carte d‚Äôaide pour les Volontaires de Paris 2024",
    "section": "",
    "text": "Lors des Jeux Olympiques de 2024 √† Paris, plus de 45 000 volontaires ont √©t√© les v√©ritables hommes et femmes de l‚Äôombre. On les a vus partout √† Paris gr√¢ce √† leur tenue bleue. Ils ont contribu√© au succ√®s des Jeux Olympiques. Venant des quatre coins de la France, ils devaient pouvoir se rep√©rer. La carte d‚Äôaide pour les volontaires est con√ßue √† cet effet et leur a √©t√© tr√®s utile durant leur s√©jour √† Paris.\n\n\n\n\n\n\n Cette carte a √©t√© cr√©√©e √† l‚Äôaide du package leaflet, en utilisant les donn√©es disponibles en open data fournies par les acteurs suivants :\n\nFontaines √† eau : opendata de la Ville de Paris\nToilettes publiques : opendata de la Ville de Paris\nDistributeurs automatiques de billets : Opendatasoft hub\nParkings v√©lo : opendata de Paris 2024\nSite de comp√©titions des Jeux Olympiques\nParalympiques : opendata de Paris 2024\n\nAu moment de la r√©alisation de la carte, la derni√®re mise √† jour des donn√©es datait du 27 juillet 2024."
  },
  {
    "objectID": "projets.html",
    "href": "projets.html",
    "title": "Projets",
    "section": "",
    "text": "Projets BUT SD\n\n\n\nScience des donn√©es\n\n\nStatistique\n\n\n\nVoici une liste de mes projets r√©alis√©es durant mes 3 ann√©es √† l‚ÄôInstitut Universitaire de Technologie de Paris.\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "blog/knn/knn.html",
    "href": "blog/knn/knn.html",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "",
    "text": "Introduction\nL‚Äôalgorithme des \\(k\\) plus proches voisins est une m√©thode d‚Äôapprentissage supervis√©. Il peut √™tre utilis√© pour la classification lorsque la variable √† expliquer (\\(Y\\)) est qualitative, mais aussi pour effectuer une r√©gression lorsque \\(Y \\in \\mathbb{R}\\).\n\nEn apprentissage supervis√©, une variable \\(Y\\) est √©tudi√©e √† partir de variables explicatives \\(X\\) √† des fins de description ou de pr√©diction. En ce qui concerne la pr√©diction, l‚Äôobjectif est de pr√©voir l‚Äô√©tiquette (classification) ou la valeur (r√©gression) de \\(Y\\) associ√©e √† une nouvelle entr√©e \\(x\\). En apprentissage non supervis√©, le probl√®me est beaucoup moins bien pos√©. Il s‚Äôagit de d√©couvrir des structures int√©ressantes dans des donn√©es non √©tiquet√©es, notamment √† travers l‚Äôanalyse exploratoire multidimensionnelle et la classification non supervis√©\nIci, nous sommes face √† un probl√®me d‚Äôapprentissage supervis√© : nous disposons d‚Äôun jeu de donn√©es constitu√© de \\(N\\) lignes repr√©sentant chacune un ‚Äúindividu‚Äù. Pour chaque individu, on dispose de \\(n\\) caract√©ristiques (les entr√©es) et d‚Äôune donn√©e repr√©sentant l‚Äô√©tiquette (ou la classe) √† laquelle ce dernier appartient. Chaque ligne est donc constitu√©e de \\(n+1\\) donn√©es. Notre objectif est de construire un mod√®le pr√©dictif prenant en entr√©e \\(n\\) valeurs correspondant aux caract√©ristiques d‚Äôun ‚Äúindividu‚Äù et donnant en sortie la classe √† laquelle il appartient.\n\n\nM√©thode des \\(k\\) plus proches voisins\nPour estimer la sortie (√©tiquette ou valeur) associ√©e √† \\(n\\) entr√©es \\((x_1, ..., x_n)\\), la m√©thode des \\(k\\) plus proches voisins consiste √† d√©terminer les \\(k\\) lignes du jeu de donn√©es dont les \\(n\\) entr√©es sont les plus proches des valeurs \\((x_1, ..., x_n)\\) √† travers le calcul d‚Äôune distance.\n\n\n\n\n\n\n\n\n\nSource : Cornell Computer Science\n\n\n\n\nEnsuite, l‚Äôalgorithme regarde les \\(k\\) voisins les plus proches et d√©termine leur sortie. En classification, il attribue √† l‚Äôindividu la classe la plus fr√©quente parmi ces \\(k\\) voisins (on parle de vote majoritaire). En r√©gression, il calcule simplement la moyenne des valeurs de sortie.\n\n\n\nIl existe diff√©rents types de distances pouvant √™tre utilis√©s pour l‚Äôalgorithme des \\(k\\) plus proches voisins.\n\nNous utiliserons la distance Euclidienne. C‚Äôest tout simplement la racine carr√©e de la somme des carr√©s des diff√©rences entre chaque coordonn√©e des deux points. Elle est donn√©e par la formule ci-dessous et repr√©sente la distance la plus courte entre deux points. Elle est √©galement connue sous le nom de norme L2 d‚Äôun vecteur.\n\\[d(x, y) = \\sqrt{ \\sum_{i=1}^{n} (x_i - y_i)^2 }\\]\nNous appliquerons la m√©thode des \\(k\\) plus proches voisins √† un cas de classification et √† un cas de r√©gression, en nous appuyant sur deux jeux de donn√©es distincts.\n\n\nClassification\nNous commen√ßons par importer le package class. Ce dernier ne contient que des fonctions pour l‚Äôalgorithme des \\(k\\) plus proches voisins et nous sera utile pour le cas de classification.\n---\nlibrary(class)\n---\nLe jeu de donn√©es Iris, √©galement connu sous le nom d‚ÄôIris de Fisher, contient 150 observations de trois esp√®ces d‚Äôiris : setosa, virginica et versicolor. Pour chaque fleur, quatre caract√©ristiques mesur√©es en centim√®tres sont renseign√©es : la longueur et la largeur des s√©pales, ainsi que la longueur et la largeur des p√©tales.\nCe jeu de donn√©es est initialement int√©gr√© √† R. Nous l‚Äôimportons √† l‚Äôaide de la commande suivante :\n---\ndata(iris)\n---\nNous disposons donc d‚Äôun jeu de donn√©es avec 4 variables explicatives, qui sont les caract√©ristiques de chaque fleur, et une variable √† pr√©dire, qui est l‚Äôesp√®ce. On observe ci-dessous que la variable Species comporte trois modalit√©s. Afin d‚Äô√©viter un probl√®me de classification multiclasse, nous choisissons d‚Äôexclure la modalit√© Setosa. L‚Äôobjectif est de construire un mod√®le qui se r√©sume √† une r√®gle de classification binaire.\n---\nstr(iris)\n---\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nOn supprime les 50 observations associ√©es √† l‚Äôesp√®ce Setosa, puis la modalit√© correspondante de la variable.\n---\niris &lt;- iris[!iris$Species == \"setosa\",]\n\niris$Species &lt;- droplevels(iris$Species)\n---\nNotre jeu de donn√©es compte d√©sormais 50 observations.\n---\ndim(iris)\n---\n\n\n[1] 100   5\n\n\nIris est un jeu de donn√©es souvent utilis√© √† des fins p√©dagogiques, car il est d√©j√† propre, √©quilibr√© et bien structur√©. Il ne comporte pas de valeurs manquantes, les variables sont d√©j√† au format num√©rique, leurs √©chelles sont relativement comparables, et la r√©partition des classes est √©quilibr√©e. Nous ne r√©aliserons donc pas d‚Äôanalyses descriptives approfondies, mis √† part le r√©sum√© statistique de notre data frame pr√©sent√© ci-dessous.\n---\nsummary(iris)\n---\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.900   Min.   :2.000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:5.800   1st Qu.:2.700   1st Qu.:4.375   1st Qu.:1.300  \n Median :6.300   Median :2.900   Median :4.900   Median :1.600  \n Mean   :6.262   Mean   :2.872   Mean   :4.906   Mean   :1.676  \n 3rd Qu.:6.700   3rd Qu.:3.025   3rd Qu.:5.525   3rd Qu.:2.000  \n Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n       Species  \n versicolor:50  \n virginica :50  \n                \n                \n                \n                \n\n\n\n\n\n\n\n\n\n\n\nAfin de pouvoir √©valuer notre mod√®le, nous divisons notre jeu de donn√©es en deux sous-ensembles : Un jeu de donn√©es d‚Äôapprentissage r√©pr√©sentant 80 % du jeu de donn√©es initial. L‚Äôalgorithme s‚Äôentra√Ænera √† partir de ces donn√©es. Puis un jeu de donn√©es de test correspondant aux 20 % restants. Il servira √† √©valuer les performances du mod√®le de classification sur des donn√©es jamais vues pendant l‚Äôapprentissage.\n\n\n\n---\nN &lt;- 80\nidx1 &lt;- sample(1:50, N/2, replace = F)\nidx1 &lt;- sample(1:50, N/2, replace = F) # Tirage aleatoire de 40 indices entre 1 et 50 \n                                       # (classe : versicolor)\nidx0 &lt;- sample(51:100, N/2, replace = F) # Tirage aleatoire de 40 indices entre 51 et 100 \n                                         # (classe : virginica)\ndataL &lt;- iris[c(idx1,idx0),] # Apprentissage (80 %)\ndataV &lt;- iris[-c(idx1,idx0),] # Test (20 %)\n---\nNous pouvons maintenant cr√©er notre fonction de pr√©diction et l‚Äôappliquer √† nos donn√©es √† l‚Äôaide de la fonction knn(). Cette fonction prend cinq param√®tres principaux :\n\ntrain : le jeu de donn√©es d‚Äôapprentissage,\ntest : le jeu de donn√©es sur lequel on souhaite faire des pr√©dictions,\ncl : le vecteur des classes associ√©es aux donn√©es d‚Äôapprentissage \\((Y)\\),\n\\(k\\) : le nombre de voisins √† consid√©rer pour la classification,\nprob : un param√®tre optionnel qui, s‚Äôil est d√©fini √† TRUE, renvoie √©galement la probabilit√© associ√©e √† la pr√©diction.\n\n---\nprediction = knn(train = dataL[,-5], test = dataV[,-5],\n                 cl = dataL[,5], k = 3)\n---\nIci, nous avons choisi de consid√©rer les 3 plus proches voisins pour pr√©dire la classe de la prochaine observation. On voit sur l‚Äôimage ci-dessous que la valeur de \\(k\\) influence fortement la performance du mod√®le et la qualit√© des pr√©dictions.\n Nous verrons par la suite comment d√©terminer la valeur optimale de \\(k\\) afin de minimiser le taux de mauvaise classification.\nNous calculons l‚Äôerreur de notre classification, c‚Äôest-√†-dire le taux de mauvaise classification sur l‚Äô√©chantillon test. Il s‚Äôagit simplement de comparer les pr√©dictions aux vraies valeurs, puis de calculer la proportion d‚Äôobservations mal class√©es.\n---\nmean(prediction != dataV[,5])\n---\n\n\n[1] 0.15\n\n\nLe taux d‚Äôerreur de notre classification est de 15 %. Cependant, ce taux peut varier √† chaque r√©p√©tition de l‚Äôexp√©rience, car la r√©partition des donn√©es entre l‚Äô√©chantillon d‚Äôapprentissage et celui de test change √† chaque tirage al√©atoire. Nous d√©cidons donc de r√©p√©ter l‚Äôexp√©rience 100 fois, en utilisant des √©chantillons diff√©rents √† chaque it√©ration. √Ä l‚Äôissue de ces r√©p√©titions, nous calculerons la moyenne des taux de mauvaise classification obtenus afin d‚Äôobtenir une estimation plus fiable de la performance du mod√®le.\n---\nrep &lt;- 100 # Nb de r√©p√©titions\nerror &lt;- rep(NA, rep)\n\nfor (i in 1:rep){\n  N &lt;- 80\n  idx1 &lt;- sample(1:50, N/2, replace = F)\n  idx0 &lt;- sample(51:100, N/2, replace = F)\n  dataL &lt;- iris[c(idx1,idx0),]\n  dataV &lt;- iris[-c(idx1,idx0),]\n  \n  prediction_100 &lt;- knn(train = dataL[,-5], test = dataV[,-5],\n                 cl = dataL[,5], k = 3)\n  \n  error[i] &lt;- c(mean(prediction_100 != dataV[,5]))\n}\n\nmean(error)\n---\n\n\n[1] 0.053\n\n\nLe taux d‚Äôerreur de notre classification est de 5,3 % lorsque nous consid√©rons les 3 plus proches voisins. Il existe de nombreux types de visualisations sur R, tant pour les pr√©dictions que pour les erreurs, que nous ne d√©taillerons pas ici.\nNous souhaitons maintenant d√©terminer le nombre de voisins qui permet la meilleure classification de la nouvelle observation. Pour ce faire, nous allons chercher √† identifier la valeur optimale de \\(k\\), ce qui permettra de r√©duire le taux d‚Äôerreur du mod√®le pr√©c√©dent. Afin de gagner du temps, nous calculerons le taux d‚Äôerreur moyen pour chaque valeur de \\(k\\) √† partir de 100 √©chantillons diff√©rents.\n---\nrep &lt;- 100 # Nb de r√©p√©titions\nkval &lt;- seq(1, 79, by = 2)\nerror &lt;- matrix(NA, rep, length(kval))\n\nfor (i in 1:rep){\n  N &lt;- 80\n  idx1 &lt;- sample(1:50, N/2, replace = F)\n  idx0 &lt;- sample(51:100, N/2, replace = F)\n  dataL &lt;- iris[c(idx1,idx0),]\n  dataV &lt;- iris[-c(idx1,idx0),]\n  for (j in 1:(length(kval)))\n  {\n    prediction_2 &lt;- knn(train = dataL[,-5], test = dataV[,-5], \n                    cl = dataL[,5], k = kval[j])\n    error[i,j] &lt;- mean(dataV[,5] != prediction_2)\n  }\n}\n---\nOn visualise les r√©sultats sur le graphique ci-dessous.\n\n\n\n\n\n\n\n\n\nIci, il est n√©cessaire de consid√©rer 11 voisins pour obtenir le taux de classification le plus faible. Nous r√©entra√Ænerons donc le mod√®le en utilisant ce param√®tre modifi√©.\n---\nrep &lt;- 100 # Nb de r√©p√©titions\nerror &lt;- rep(NA, rep)\n\nfor (i in 1:rep){\n  N &lt;- 80\n  idx1 &lt;- sample(1:50, N/2, replace = F)\n  idx0 &lt;- sample(51:100, N/2, replace = F)\n  dataL &lt;- iris[c(idx1,idx0),]\n  dataV &lt;- iris[-c(idx1,idx0),]\n  \n  prediction_11 &lt;- knn(train = dataL[,-5], test = dataV[,-5],\n                 cl = dataL[,5], k = 11)\n  \n  error[i] &lt;- c(mean(prediction_11 != dataV[,5]))\n}\n\nmean(error)\n---\n\n\n[1] 0.039\n\n\nNous obtenons un taux d‚Äôerreur plus faible (3,9 %) par rapport au mod√®le pr√©c√©dent, ce qui confirme notre petite recherche du \\(k\\) optimal.\n\n\nR√©gression"
  },
  {
    "objectID": "projets/but_sd.html",
    "href": "projets/but_sd.html",
    "title": "Projets BUT SD",
    "section": "",
    "text": "Aucun article correspondant"
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html",
    "href": "projets/projets_but/mirgation/migration.html",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "",
    "text": "Ce projet vise √† migrer des donn√©es d‚Äôun environnement SQL vers un environnement NoSQL. Concr√®tement, il s‚Äôagit de transf√©rer les informations stock√©es dans une base de donn√©es relationnelle traditionnelle, o√π les donn√©es sont organis√©es en tables avec des relations fixes, vers une base de donn√©es NoSQL, qui offre une structure plus flexible adapt√©e aux donn√©es non structur√©es ou semi-structur√©es.\nNous travaillons avec les donn√©es d‚Äôune entreprise de voitures qui rencontre des probl√®mes avec sa base de donn√©es actuelle : les requ√™tes sont lentes et des d√©faillances serveur entra√Ænent des pertes de donn√©es. Pour r√©soudre ces probl√®mes, nous avons d√©cid√© de passer √† un environnement NoSQL. Cette technologie permet de stocker des donn√©es sous une forme non structur√©e, offrant ainsi plus de flexibilit√© et de performance. Cette migration vise √† am√©liorer la performance des requ√™tes et √† pr√©parer l‚Äôinfrastructure pour une croissance future.\nLe d√©p√¥t GitHub contenant le rapport complet du projet, ainsi que les requ√™tes SQL et NoSQL associ√©es, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html#base-de-donn√©es-relationnelle",
    "href": "projets/projets_but/mirgation/migration.html#base-de-donn√©es-relationnelle",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Base de donn√©es relationnelle",
    "text": "Base de donn√©es relationnelle\nLa base de donn√©es relationnelle initiale contient des informations sur les v√©hicules, les clients, les commandes, les employ√©s‚Ä¶ La repr√©sentation des donn√©es est claire et bien organis√©e. Chaque table dispose de relations avec d‚Äôautres tables, ce qui permet de structurer efficacement les informations et de faciliter les requ√™tes complexes.\n\n\n\nSch√©ma relationnel de la bdd initial\n\n\nDans un premier temps, nous avons cr√©√© des requ√™tes SQL sur cette base de donn√©es. Ces requ√™tes serviront de tests pour √©valuer le succ√®s de la migration. Nous comparerons les r√©sultats obtenus dans la base de donn√©es relationnelle avec ceux obtenus dans la base NoSQL pour v√©rifier l‚Äôint√©grit√© et la performance de la migration.\n\n\n\nRequ√™tes SQL\n\n\nLa base de donn√©es est au format SQLite. Nous avons import√© le module sqlite3 en Python pour √©tablir la connexion et interagir avec la base. Ensuite, nous avons utilis√© la biblioth√®que Pandas et notamment sa fonction read_sql_query(), pour ex√©cuter et lire les r√©sultats des requ√™tes SQL."
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html#algorithme-de-migration",
    "href": "projets/projets_but/mirgation/migration.html#algorithme-de-migration",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Algorithme de migration",
    "text": "Algorithme de migration\nIl existe plusieurs types de bases de donn√©es NoSQL (Cl√©-valeur, Document, Colonne et Graphe), chacun adapt√© √† ses propres cas d‚Äôusage et ayant ses propres avantages et inconv√©nients, notamment en termes de scalabilit√© et de flexibilit√©. Le choix d√©pend donc de plusieurs facteurs cl√©s, comme la structure des donn√©es, les exigences de performance‚Ä¶\nEn ce qui nous concerne, nous pouvons r√©aliser une migration vers un environnement NoSQL, car l‚Äôentreprise dispose d‚Äôune grande quantit√© de donn√©es structur√©es en constante croissance. De plus, il est possible d‚Äôam√©liorer significativement les performances d‚Äôacc√®s aux donn√©es en optimisant le traitement de donn√©es plus importantes et en r√©duisant le temps de latence. On souhaite donc une solution qui offre plus de flexibilit√© et √©volutivit√©, tout en pr√©servant l‚Äôint√©grit√© des donn√©es de notre base initial.\nApr√®s m√ªre r√©flexion, le format document est celui s‚Äôadaptant le mieux √† notre objectif. En effet, il permet de structurer naturellement les entit√©s de mani√®re hi√©rarchique. Par exemple, un client peut √™tre repr√©sent√© par un document contenant ses commandes, chaque commande incluant les produits associ√©s. De plus, il offre une grande flexibilit√©, permettant de traiter diff√©rents types de donn√©es sans modifications complexes du sch√©ma. Enfin, il permet une scalabilit√© horizontale gr√¢ce √† la partition de document, c-√†-d si les donn√©es augmentent, on peut facilement ajouter de nouveaux serveurs pour stocker et g√©rer plus de documents, sans tout restructurer.\n\n\n\nExemple d‚Äôune mod√©lisation au format Document\n\n\nCe mod√®le pr√©sente quelques inconv√©nients, notamment des performances limit√©es pour les requ√™tes complexes ou les jointures entre documents. De plus, les mises √† jour simultan√©es de documents imbriqu√©s peuvent √™tre plus difficiles √† g√©rer.\nNous avons d√©cid√© de structurer nos donn√©es autour de quatres collections : customers, payments, orders et employees."
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html#script-de-migration",
    "href": "projets/projets_but/mirgation/migration.html#script-de-migration",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Script de migration",
    "text": "Script de migration\nAvant de d√©velopper un script de migration, nous avons d‚Äôabord √©tabli un pseudo-algorithme dans l‚Äôobjecitf de structurer et organiser la logique de notre programme.\nEnsuite, pour la migration de nos donn√©es, nous avons utilis√© SQLite comme source et MongoDB comme destination, en exploitant les biblioth√®ques Python sqlite3, pymongo et pandas. Le processus inclut l‚Äôextraction des donn√©es de SQLite, leur transformation au format document compatible avec MongoDB, et leur insertion dans les collections appropri√©es."
  },
  {
    "objectID": "projets/projets_but/essai_clinique/essai_clinique.html",
    "href": "projets/projets_but/essai_clinique/essai_clinique.html",
    "title": "Etude statistique dans un essai clinique",
    "section": "",
    "text": "Ce projet consiste en la r√©alisation d‚Äôune √©tude statistique dans le cadre d‚Äôun essai clinique. Nous travaillons sur un jeu de donn√©es simul√©es pour effectuer l‚Äôanalyse statistique d‚Äôune √©tude de phase 3. L‚Äôobjectif est de fournir au laboratoire une Autorisation de Mise sur le March√© (AMM) avec une indication dans la prise en charge de la dr√©panocytose.\nLa dr√©panocytose est une maladie g√©n√©tique qui affecte les globules rouges et peut entra√Æner des complications graves. Elle se manifeste notamment par une an√©mie, des crises douloureuses et un risque accru d‚Äôinfections.\n\n\n\nSch√©ma de la dr√©panocytose\n\n\nLors d‚Äôune √©tude de phase 3, les chercheurs comparent un nouveau traitement prometteur au traitement standard, qui est le traitement reconnu et g√©n√©ralement administr√© pour une affection ou une maladie. Dans notre √©tude statistique, le nouveau traitement s‚Äôappelle le Voxelotor et nous comparons ces effets √† un placebo (traitement qui n‚Äôa aucune action sp√©cifique sur le trouble qu‚Äôil vise √† soulager). Pour cela, nous r√©alisons des tests statistiques sur des hypoth√®ses d√©finit en amont.\nLe d√©p√¥t GitHub contenant le rapport complet du projet, ainsi que le code R, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/essai_clinique/essai_clinique.html#plan-danalyse-statistique",
    "href": "projets/projets_but/essai_clinique/essai_clinique.html#plan-danalyse-statistique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Plan d‚Äôanalyse statistique",
    "text": "Plan d‚Äôanalyse statistique\nLe plan d‚Äôanalyse statistique (SAP, disponible ici) d√©crit de mani√®re d√©taill√©e la strat√©gie d‚Äôanalyse des donn√©es de l‚Äô√©tude. Il garantit la rigueur, la transparence et la reproductibilit√© des analyses statistiques. Ce plan est g√©n√©ralement r√©dig√© avant la fin de l‚Äôessai, afin d‚Äô√©viter tout biais pouvant d√©couler des r√©sultats observ√©s.\nPour √©valuer l‚Äôeffet du voxelotor sur l‚Äôam√©lioration de l‚Äôh√©moglobine des patients par rapport au placebo, les analyses d√©finies dans le SAP sont toutes r√©alis√©es. L‚Äôanalyse principale porte sur la r√©ponse en h√©moglobine √† la semaine 72, qui correspond au maximum du suivi des patients. L‚Äôanalyse secondaire quant √† elle examine le changement de l‚Äôh√©moglobine entre le d√©but de l‚Äô√©tude et la semaine 72 sur plusieurs marqueurs cliniques.\n\nAnalyse principal\nLa r√©ponse en h√©moglobine est d√©finie comme une augmentation de plus de 1g/dL par rapport √† la valeur de r√©f√©rence mesur√©e avant le d√©but du traitement. Cela signifie qu‚Äôun patient est consid√©r√© comme ayant r√©pondu favorablement au traitement si son taux d‚Äôh√©moglobine a augment√© d‚Äôau moins 1g/dL par rapport √† la valeur initiale, mesur√©e avant l‚Äôadministration du voxelotor. Si aucune mesure d‚Äôh√©moglobine n‚Äôest disponible √† la semaine 72, le patient est imput√© comme non-r√©pondeur. Par exemple, si le patient ne se pr√©sente pas √† la consultation, si les donn√©es sont perdues ou invalides, ce patient sera consid√©r√© comme n‚Äôayant pas montr√© de r√©ponse au traitement.\nNous utilisons un test statistique du Chi-2 avec correction de Yates pour comparer les taux de r√©ponse en h√©moglobine entre le groupe trait√© par VOX_1500 et le groupe par le placebo, afin de d√©terminer s‚Äôil existe une diff√©rence significative.\n\n\n\nR√©partition des traitements dans notre population\n\n\nLes r√©sultats du test indiquent qu‚Äôil n‚Äôy a pas de preuve statistiquement significative d‚Äôune diff√©rence entre les taux de r√©ponse en h√©moglobine des deux groupes.\nCependant, certains facteurs pourraient √™tre amen√©s √† faire varier l‚Äôefficacit√© du VOX_1500. C‚Äôest pourquoi, nous r√©alisons le m√™me test statistique avec les m√™mes hypoth√®ses sur des sous-populations d√©finis √† partir des facteurs suivants : l‚Äô√¢ge des patients, l‚Äôutilisation pr√©alable d‚Äôhydroxyur√©e (HU) et l‚Äôhistorique des crises vaso-occlusives (VOC).\nLes r√©sultats de cette √©tude, montrent que le VOX_1500 semble plus efficace chez certains groupes de patients. Premi√®rement, on observe une diff√©rence significative en faveur du traitement chez les non-utilisateurs d‚ÄôHU. Cela indique une meilleure r√©ponse en h√©moglobine par rapport au placebo. En revanche, chez les utilisateurs d‚ÄôHU, aucune diff√©rence notable n‚Äôa √©t√© trouv√©e entre les deux traitements.\nConcernant les crises vaso-occlusives, les patients ayant eu une seule crise r√©pondent mieux au VOX_15000, alors que ceux ayant eu plusieurs crises ne montrent aucun signe positif suppl√©mentaire par rapport au placebo.\nEnfin, l‚Äôanalyse par tranche d‚Äô√¢ge r√©v√®le des r√©sultats similaires entre les groupes VOX_1500 et placebo, tant chez les adolescents que chez les adultes.\nEn somme, ces observations sugg√®rent que l‚Äôefficacit√© du VOX_1500 pourrait d√©pendre de certains facteurs cliniques, mais des √©tudes suppl√©mentaires seront n√©cessaires pour valider ces conclusions.\n\n\nAnalyse secondaire\nDans cette secone partie, nous essayons de d√©terminer s‚Äôil existe une diff√©rence dans le changement moyen des taux d‚Äôh√©moglobine entre les deux groupes (VOX_1500 et placebo). Pour r√©pondre √† cette question, nous r√©alisons une analyse statistique √† l‚Äôaide de l‚ÄôANOVA. Cette derni√®re est une m√©thode statistique permettant de comparer les moyennes de plusieurs groupes afin de d√©terminer si au moins un groupe diff√®re significativement des autres. Plus concr√®tement, elle permet d‚Äô√©valuer si les variations observ√©es dans les donn√©es sont dues √† des diff√©rences entre les groupes ou √† des variations al√©atoires au sein des groupes eux-m√™mes. Elle nous est tr√®s utile ici, car elle permet d‚Äôajuster les r√©sultats en tenant compte des valeurs d‚Äôh√©moglobine de d√©part (baseline) et de l‚Äôutilisation pr√©alable d‚Äôhydroxyur√©e (HU) comme facteur suppl√©mentaire.\nLes r√©sultats r√©v√®lent que le VOX_1500 entra√Æne une augmentation significative du taux d‚Äôh√©moglobine par rapport au placebo.\n\n\n\nRepr√©sentation graphique du changement ajust√© d‚Äôh√©moglobine entre le d√©part et la semaine 72\n\n\nLes patients trait√©s par VOX_1500 pr√©sentent une augmentation moyenne de 0,51 g/dL d‚Äôh√©moglobine, contre 0,23 g/dL pour le groupe placebo. Cette diff√©rence est statistiquement significative au risque 5 %. Nous observons, un tr√®s faible impact des taux d‚Äôh√©moglobine au d√©part sur l‚Äô√©volution des niveaux observ√©s. Cependant, l‚Äôutilisation pr√©alable d‚Äôhydroxyur√©e influence fortement les r√©sultats, les patients sous ce traitement montrant des √©volutions diff√©rentes de ceux n‚Äôen prenant pas. Le traitement peut donc offrir un b√©n√©fice notable en termes d‚Äôaugmentation de l‚Äôh√©moglobine, en particulier chez les patients n‚Äôutilisant pas d‚Äôhydroxyur√©e.\n\n\nMarqueurs d‚Äôh√©molyse\nNous analysons ensuite l‚Äô√©volution des marqueurs d‚Äôh√©molyse en utilisant la m√™me m√©thode. Ces indicateurs biologiques permettent d‚Äô√©valuer la destruction des globules rouges dans le sang. Deux marqueurs cl√©sretiennent notre attention :\n\nBilirubine non conjugu√©e : elle augmente lorsque les globules rouges sont d√©truits, refl√©tant une h√©molyse active.\nPourcentage de r√©ticulocytes : ces globules rouges immatures sont produits en r√©ponse √† la destruction des globules matures. Un taux √©lev√© traduit une compensation de l‚Äôorganisme face √† l‚Äôh√©molyse.\n\nL‚Äôanalyse de ces param√®tres permet de mieux comprendre l‚Äôimpact du traitement sur la sant√© des globules rouges.\n\n\n\nVariations dans les marqueurs d‚Äôh√©molyse entre la baseline et la semaine 72\n\n\nL‚Äôanalyse des marqueurs d‚Äôh√©molyse r√©v√®le que le traitement VOX_1500 a un effet significatif sur la r√©duction de la bilirubine non conjugu√©e et des r√©ticulocytes, deux indicateurs cl√©s de la destruction des globules rouges.\n\n\n\nRepr√©sentation graphique du changement dans les marqueurs d‚Äôh√©molyse\n\n\nLe traitement VOX_1500 entra√Æne une diminution moyenne de -8,09 g/dL contre -4,56 g/dL pour le placebo. Cette diff√©rence statistiquement significative confirme l‚Äôeffet notable du traitement. De plus, le VOX_1500 montre √©galement un impact significatif, sans que les valeurs initiales ou l‚Äôutilisation d‚ÄôHydroxyur√©e n‚Äôinfluencent les r√©sultats.\nOn en conclut que le traitement VOX_1500 a un effet plus marqu√© que le placebo sur la r√©duction de la bilirubine non conjugu√©e et des r√©ticulocytes, comme le montre la diff√©rence significative dans les changements moyens ajust√©s pour ces deux marqueurs d‚Äôh√©molyse. Le traitement pourrait ainsi √™tre plus efficace pour r√©duire les caract√©ristiques d‚Äôh√©molyse par rapport au placebo."
  },
  {
    "objectID": "projets/projets_but/essai_clinique/essai_clinique.html#nouvel-essai-clinique",
    "href": "projets/projets_but/essai_clinique/essai_clinique.html#nouvel-essai-clinique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Nouvel essai clinique",
    "text": "Nouvel essai clinique\nAu vu des facteurs influents vus dans l‚Äôanalyse principal. Il se peut que le laboratoire envisage de lancer un nouvel essai clinique afin d‚Äô√©valuer l‚Äôefficacit√© du VOX_1500 chez les adolescents (patient de 12 √† 18 ans). En utilisant le m√™me crit√®re principal que pr√©c√©demment et en se basant sur les r√©sultats obtenus pour le voxelotor dans cette population, tout en supposant un taux de r√©ponse de 10% pour le groupe placebo, nous proc√©dons au calcul du nombre de patients n√©cessaires pour d√©montrer l‚Äôefficacit√© du traitement. Ce calcul est effectu√© en visant une puissance statistique de 90% et un risque alpha de 5%, avec un ratio de traitement 1:1.\n\n\n\nProgramme R du calcul du nombre de patients\n\n\nLa taille de l‚Äô√©chantillon n√©cessaire pour ce nouvel essai clinique est de 48 patients au total, r√©partis √©galement entre les deux groupes, soit 24 patients par groupe (placebo et le VOX_1500). Cette estimation garantit une puissance de 90% pour d√©tecter une diff√©rence significative entre les groupes 29 avec un risque alpha de 5%."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Trier par\n       Ordre par d√©faut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus r√©cent\n        \n         \n          Auteur¬∑rice\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nL‚Äôalgorithme des \\(k\\) plus proches voisins\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication sur R de l‚Äôalgorithme des \\(k\\) plus proches voisins.\n\n\n\n\n\n29 sept. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarte d‚Äôaide pour les Volontaires de Paris 2024\n\n\n\n\n\n\nR\n\n\nOpendata\n\n\n\nPlus de 45 000 volontaires ont contribu√© au bon d√©roulement des Jeux Olympiques. Venus des quatre coins de la France, cette carte interactive leur permet de se pr√©parer efficacement et d‚Äôaccomplir au mieux leur mission.\n\n\n\n\n\n9 sept. 2024\n\n\n\n\n\n\nAucun article correspondant"
  }
]