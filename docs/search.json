[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Trier par\n       Ordre par d√©faut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus r√©cent\n        \n         \n          Auteur¬∑rice\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeut-on pr√©dire les survivants du Titanic ?\n\n\n\n\n\n\nPython\n\n\nMachine learning\n\n\nScikit-Learn\n\n\n\nBien que la chance ait jou√© un r√¥le dans la survie des passagers, certaines cat√©gories de personnes semblent avoir eu un avantage. Est-il possible de pr√©dire ces chances de survie √† l‚Äôavance ?\n\n\n\n\n\n1 mars 2025\n\n\n\n\n\n\n\n\n\n\n\n\nR√©gression logistique en pratique\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication d‚Äôun mod√®le de r√©gression logistique pour pr√©dire si un patient est atteint de diab√®te. Le mod√®le est √©galement compar√© √† la m√©thode des \\(k\\) plus proches voisins.\n\n\n\n\n\n10 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôalgorithme des \\(k\\) plus proches voisins\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication sur R de l‚Äôalgorithme des \\(k\\) plus proches voisins.\n\n\n\n\n\n29 sept. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarte d‚Äôaide pour les Volontaires de Paris 2024\n\n\n\n\n\n\nR\n\n\nOpendata\n\n\n\nPlus de 45 000 volontaires ont contribu√© au bon d√©roulement des Jeux Olympiques. Venus des quatre coins de la France, cette carte interactive leur permet de se pr√©parer efficacement et d‚Äôaccomplir au mieux leur mission.\n\n\n\n\n\n9 sept. 2024\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "blog/carte_volontaires.html",
    "href": "blog/carte_volontaires.html",
    "title": "Carte d‚Äôaide pour les Volontaires de Paris 2024",
    "section": "",
    "text": "Lors des Jeux Olympiques de 2024 √† Paris, plus de 45 000 volontaires ont √©t√© les v√©ritables hommes et femmes de l‚Äôombre. On les a vus partout √† Paris gr√¢ce √† leur tenue bleue. Ils ont contribu√© au succ√®s des Jeux Olympiques. Venant des quatre coins de la France, ils devaient pouvoir se rep√©rer. La carte d‚Äôaide pour les volontaires est con√ßue √† cet effet et leur a √©t√© tr√®s utile durant leur s√©jour √† Paris.\n\n\n\n\n\n\n Cette carte a √©t√© cr√©√©e √† l‚Äôaide du package leaflet, en utilisant les donn√©es disponibles en open data fournies par les acteurs suivants :\n\nFontaines √† eau : opendata de la Ville de Paris\nToilettes publiques : opendata de la Ville de Paris\nDistributeurs automatiques de billets : Opendatasoft hub\nParkings v√©lo : opendata de Paris 2024\nSite de comp√©titions des Jeux Olympiques\nParalympiques : opendata de Paris 2024\n\n(Deni√®re mis √† jour des donn√©es le 27 juillet 2024)"
  },
  {
    "objectID": "blog/knn.html",
    "href": "blog/knn.html",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "",
    "text": "Le but de l‚Äôapprentissage supervis√© est de pr√©voir l‚Äô√©tiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (r√©gression) associ√©e √† une nouvelle entr√©e \\(X\\), o√π il est sous-entendu que (\\(X,Y\\)) est une nouvelle r√©alisation des donn√©es, ind√©pendante de l‚Äô√©chantillon observ√©.\nL‚Äôalgorithme des \\(k\\) plus proches voisins est une m√©thode d‚Äôapprentissage supervis√©. On peut l‚Äôutiliser pour classifier quand \\(Y_i\\) est une variable qualitative, les \\(Y_i\\) sont appel√©s √©tiquettes. On peut √©galement l‚Äôutiliser pour pr√©dire si \\(Y_i \\in \\mathbb{R}\\). C‚Äôest donc une r√©gression et les \\(Y_i\\) sont appel√©s variables √† expliquer.\nRemarque : On parle d‚Äôapprentissage supervis√© car pour chaque \\(X_i\\) de l‚Äô√©chantillon d‚Äôapprentissage on dispose de \\(Y_i\\), l‚Äô√©tiquette. Au contraire, on parlera d‚Äôapprentissage non-supervis√© lorsque l‚Äô√©chantillon est simplement constitu√© des \\(X_i\\)."
  },
  {
    "objectID": "blog/knn.html#analyse-descriptive",
    "href": "blog/knn.html#analyse-descriptive",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "Analyse descriptive",
    "text": "Analyse descriptive\nOn proc√®de √† une succinte analyse descritpive de notre jeu de donn√©es.\n\nsummary(abalone)\n\n     Length          Diameter         Height        Whole_weight   \n Min.   :0.0750   Min.   :0.055   Min.   :0.0100   Min.   :0.0020  \n 1st Qu.:0.4500   1st Qu.:0.350   1st Qu.:0.1150   1st Qu.:0.4421  \n Median :0.5450   Median :0.425   Median :0.1400   Median :0.8000  \n Mean   :0.5241   Mean   :0.408   Mean   :0.1396   Mean   :0.8291  \n 3rd Qu.:0.6150   3rd Qu.:0.480   3rd Qu.:0.1650   3rd Qu.:1.1538  \n Max.   :0.8150   Max.   :0.650   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1861   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3595   Mean   :0.1807   Mean   :0.2389   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3289   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n\n\nOn observe ci-dessous la distribution de la variable Rings ainsi que la relation entre la longueur et la taille des ormeaux.\n\nggplot(abalone, aes(x = Rings)) +\n  geom_histogram(fill = \"blue\") +\n  labs(title = \"Distribution de Rings\", y = \"Fr√©quence\", x = \"Rings\") + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(abalone, aes(x = Length, y = Height)) +\n  geom_point(col = \"blue\", pch = 1) +\n  labs(title = \"Relation entre Height et Length\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/knn.html#pr√©dicition-de-la-variable-rings",
    "href": "blog/knn.html#pr√©dicition-de-la-variable-rings",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "Pr√©dicition de la variable Rings",
    "text": "Pr√©dicition de la variable Rings\nComme dans la partie pr√©c√©dente, nous commen√ßons par cr√©er deux sous-√©chantillons distincts (√©chantillon d‚Äôapprentissage et echantillon de test) √† partir du jeu de donn√©es complet.\n\nN = round((80/100)*nrow(abalone)) # Calcul du nombre d'observations a s√©lectionner (80 %) \nidx1 &lt;- sample(1:nrow(abalone), size = N, replace = FALSE) # Tirage aleatoire des indices qu'on va s√©lectionner\ndataL &lt;- abalone[idx1,] # Construction du dataset d'apprentissage\ndataV &lt;- abalone[-idx1,] # Construction du dataset de test ou de validite\n\nA pr√©sent, on utilise la fonction kknn() pour mettre en oeuvre notre algorithme de pr√©diction en fixant \\(k = 3\\).\n\npred &lt;- kknn(Rings ~., dataL, dataV, k = 3, kernel = 'rectangular')\n\nCi-dessous, nous observons nos pr√©dictions en fonction de la variable Rings.\n\nplot(dataV$Rings,pred$fitted.values, xlab = \"Rings\", ylab = \"Prediction\", col = \"blue\")\nabline(0,1, col = \"red\")\n\n\n\n\n\n\n\n\nContrairement √† la classification, nous utiliserons l‚Äôerreur quadratique moyenne pour mesurer la performance de notre mod√®le sur l‚Äô√©chantillon de test.\n\nmse &lt;- mse(pred$fitted.values, dataV$Rings)\npaste0(\"Erreur quadratique moyenne = \",mse)\n\n[1] \"Erreur quadratique moyenne = 5.67212242182302\"\n\n\nEnfin, nous allons identifier la valeur de \\(k\\) pour laquelle l‚Äôerreur quadratique moyenne est la plus faible. On pourra alors d√©terminer le niveau optimal de \\(k\\) afin d‚Äôam√©liorer la pr√©cision du mod√®le. La boucle suivante permet de calculer l‚Äôerreur quadratique moyenne pour chaque valeur de \\(k\\) sur notre √©chantillon.\n\nkvec &lt;- 1:100\nerror &lt;- rep(NA, length(kvec))\n\nfor(i in 1:length(kvec)){\n  pred &lt;- kknn(Rings ~., dataL, dataV, k = i, kernel = 'rectangular')\n  error[i] &lt;- mse(dataV$Rings, pred$fitted.values)\n}\n\nOn visualise les r√©sultats sur le graphique ci-dessous.\n\nplot(kvec, error, type = \"b\", col = \"orange\")\nmin_error_niveau &lt;- which.min(error)\nabline(v = kvec[min_error_niveau], col = \"red\", lty = 2)\nlegend(\"topright\", legend = paste(\"Erreur min √† k =\", kvec[min_error_niveau]), col = \"red\", lty = 2)"
  },
  {
    "objectID": "projets.html",
    "href": "projets.html",
    "title": "Projets",
    "section": "",
    "text": "Trier par\n       Ordre par d√©faut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus r√©cent\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nProjets BUT SD\n\n\n\nScience des donn√©es\n\n\nStatistique\n\n\n\nVoici une liste de mes projets r√©alis√©es durant mes 3 ann√©es √† l‚ÄôInstitut Universitaire de Technologie de Paris.\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html",
    "href": "projets/projets_but/serie_temp_charbon.html",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "",
    "text": "L‚Äôobjectif est d‚Äôanalyser et de mod√©liser statistiquement une s√©rie temporelle li√©e √† la production d‚Äô√©lectricit√© par combustion aux √âtats-Unis entre 2001 et 2022. Nous √©tudions l‚Äô√©volution de cette s√©rie au fil du temps afin d‚Äôidentifier ses principales tendances et composantes. Cette s√©rie est issue du site web de l‚ÄôAdministration am√©ricaine de l‚Äôinformation sur l‚Äô√©nergie. Nous r√©alisons ensuite une application Shiny afin de visualiser les r√©sultats interactivement.\nLe d√©pot github du projet contenant le code est disponible ici."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html#tendance-de-la-s√©rie",
    "href": "projets/projets_but/serie_temp_charbon.html#tendance-de-la-s√©rie",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "Tendance de la s√©rie",
    "text": "Tendance de la s√©rie\nAfin de mod√©liser la s√©rie, nous commen√ßons par analyser la tendance. Nous utilisons un filtre de moyennes mobiles simples et centr√©es, ainsi qu‚Äôune r√©gression des moyennes annuelles, dans le but de visualiser l‚Äô√©volution g√©n√©rale de la s√©rie.\nLes moyennes mobiles permettent de lisser la s√©rie et d‚Äôatt√©nuer les fluctuations al√©atoires. La moyenne mobile simple est calcul√©e sur une fen√™tre fixe, tandis que la moyenne mobile centr√©e est d√©termin√©e de mani√®re sym√©trique autour de chaque point. Cela permet d‚Äôidentifier la tendance sous-jacente de la s√©rie entre 2001 et 2022, qui s‚Äôest av√©r√©e √™tre d√©croissante. On se sert √©galement de la moyenne mobile pour √©l√©miner la composante saisonni√®re de p√©riode \\(p\\) de notre s√©rie et r√©duire au maximum l‚Äôamplitude des fluctuations irr√©guli√®res.\nLa moyenne mobile qui s‚Äôajuste le mieux √† la s√©rie est celle d‚Äôordre 12.\nDe plus, la courbe de r√©gression des moyennes annuelles permet de mod√©liser la tendance √† long terme de la s√©rie en s‚Äôappuyant sur les moyennes calcul√©es pour chaque ann√©e. On trace sur notre nuage de point une courbe lin√©aire qui nous indique une tendance d√©croissante de la production d‚Äô√©l√©ctricit√© par combustion."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html#d√©composition-de-la-s√©rie",
    "href": "projets/projets_but/serie_temp_charbon.html#d√©composition-de-la-s√©rie",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "D√©composition de la s√©rie",
    "text": "D√©composition de la s√©rie\n√Ä pr√©sent, nous proc√©dons √† la d√©composition de notre s√©rie √† l‚Äôaide d‚Äôun mod√®le additif. Il s‚Äô√©crit de la mani√®re suivante :\n\\[ y_i = f_i + s_i + e_i \\quad \\text{pour } i = 1, \\dots, n \\quad \\text{avec }  \\sum_{j=1}^{p} s_j = 0 \\quad \\text{et} \\quad \\sum_{j=1}^{n} e_j = 0 \\]\nDans ce mod√®le, l‚Äôamplitude de la composante saisonni√®re et du bruit reste constante au cours du temps. Cela se traduit graphiquement par des fluctuations autour de la tendance d‚Äôamplitude constante. L‚Äôutilisation de la m√©thode de la bande, qui consiste √† tracer la courbe reliant les minima sur une p√©riode ainsi que celle reliant les maxima, a montr√© que ces deux courbes sont parall√®les, indiquant ainsi que le mod√®le est additif.\nOn rappelle qu‚Äôune s√©rie chronologique r√©sulte de trois composantes fondamentales : la tendance (\\(f_i\\)), la composante saisonni√®re (\\(s_i\\)) ou saisonnalit√©, et la composante r√©siduelle (\\(e_i\\)).\nNous analysons d‚Äôabord l‚Äôeffet des saisons en calculant les coefficients de variation saisonni√®re (CVS). Ces variations, qui reviennent chaque ann√©e √† la m√™me p√©riode, sont influenc√©es par des ph√©nom√®nes naturels ou √©conomiques (comme la m√©t√©o ou les f√™tes) mais n‚Äôont pas d‚Äôimpact durable sur la tendance.\nEnfin, nous nous int√©ressons √† la composante r√©siduelle, qui regroupe toutes les variations non expliqu√©es par la tendance et la saisonnalit√©. Ces fluctuations peuvent √™tre dues √† des √©v√©nements impr√©vus (crises √©conomiques, accidents, conditions exceptionnelles) ou √† des variations al√©atoires.\n\n\n\nD√©composition de la s√©rie"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html#pr√©vision-de-la-s√©rie",
    "href": "projets/projets_but/serie_temp_charbon.html#pr√©vision-de-la-s√©rie",
    "title": "S√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis",
    "section": "Pr√©vision de la s√©rie",
    "text": "Pr√©vision de la s√©rie\nL‚Äôobjectif de cette √©tape est de r√©aliser deux pr√©visions (2022 et 2023) de la production d‚Äô√©lectricit√© par combustion en utilisant trois m√©thodes diff√©rentes : Trend + Saison, ARMA et Holt-Winters.\nLa m√©thode Trend + Saison, identifie deux composantes principales dans notre s√©rie qui sont la tendance et la saisonnalit√©. La m√©thode ARMA repose sur l‚Äôid√©e que la production d‚Äô√©lectricit√© √† un instant donn√© d√©pend des valeurs pass√©es et des erreurs pass√©es. Pour garantir des pr√©visions optimales, nous avons s√©lectionn√© les param√®tres du mod√®le ARMA en fonction des performances statistiques obtenues. Enfin, le mod√®le Holt-Winters est une extension de lissage exponentiel qui prend en compte trois √©l√©ments, la tendance, la saisonnalit√© et la moyenne ajust√©e de la s√©rie.\nLa pr√©vision sur les donn√©es de 2022 a permis de comparer les valeurs estim√©es avec les valeurs r√©elles de la s√©rie, afin d‚Äô√©valuer la pr√©cision des trois m√©thodes utilis√©es. Pour cela, nous avons calcul√© l‚Äôerreur quadratique moyenne (RMSE) pour chacune des m√©thodes, ce qui nous a permis d‚Äôidentifier celle offrant les pr√©visions les plus fiables.\nParmi les trois approches test√©es, la m√©thode Trend + Saison s‚Äôest r√©v√©l√©e √™tre la plus performante, car elle a obtenu l‚Äôerreur quadratique moyenne la plus faible. Cela s‚Äôexplique par sa capacit√© √† capturer √† la fois la tendance de long terme et les variations saisonni√®res, qui jouent un r√¥le cl√© dans l‚Äô√©volution de la production d‚Äô√©lectricit√©. En revanche, les mod√®les ARMA et Holt-Winters ont montr√© des √©carts plus importants avec les valeurs r√©elles, probablement en raison de leurs hypoth√®ses sous-jacentes qui s‚Äôadaptent moins bien aux dynamiques sp√©cifiques de notre s√©rie temporelle."
  },
  {
    "objectID": "projets/projets_but/louvre.html",
    "href": "projets/projets_but/louvre.html",
    "title": "Nocturne au mus√©e du Louvre",
    "section": "",
    "text": "Tous les vendredis, le mus√©e du Louvre offre un moment de magie √† ses visiteurs au milieu de ses collections. Au programme de ces nocturnes hebdomadaires, de nombreuses activit√©s pour petits et grands. Le 24 novembre 2023, mes camarades de l‚ÄôIUT et moi avons pu participer √† l‚Äôune de ces soir√©es. Nous avons d√©cid√© de pr√©senter certains tableaux du c√©l√®bre mus√©e au filtre de l‚ÄôIA. Au moment o√π cette technologie prend une place active dans notre soci√©t√©, avec par exemple ChatGPT, DALL-E ou encore Mistral AI, il est int√©ressant de pouvoir approfondir l‚Äôutilisation de ces outils permettant de g√©n√©rer des √©l√©ments en lien, ici, avec l‚Äôart, par exemple. De plus, ce sujet fascinant qu‚Äôest l‚Äôintelligence artificielle est plus ou moins en lien avec notre formation en science des donn√©es. Explorer ce domaine o√π se m√™lent math√©matiques, informatique et donn√©es √©tait particuli√®rement captivant.\n\nQu‚Äôavons nous fait ?\nL‚Äôobjectif final √©tait de pouvoir le 23 novembre 2024, pr√©senter une ≈ìuvre en rapport avec l‚ÄôIA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons d√©cid√© de choisir un sujet d‚Äôintelligence artificielle qui pouvait co√Øncider avec une ≈ìuvre du mus√©e. Apr√®s de longues recherches passionnantes, nous avons choisi de pr√©senter les GAN (Generative adversarial networks). C‚Äôest une classe d‚Äôalgorithmes d‚Äôapprentissage non supervis√©. Ils permettent de g√©n√©rer des images avec un fort degr√© de r√©alisme. Nous avons trouv√© la technologie et les m√©thodes tr√®s int√©ressantes, d‚Äôautant plus qu‚Äôelles aboutissent √† des applications concr√®tes, telles que la g√©n√©ration d‚Äôimages artistiques. Cependant, ces algorithmes sont utilis√©s dans bien d‚Äôautres domaines tels que la m√©decine ou encore la finance. Mais concr√®tement, comment √ßa marche ?\n\n\nQu‚Äôest-ce qu‚Äôun GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL‚Äôartiste (G√©n√©rateur) : L‚Äôobjectif du joueur est de cr√©er des images qui se rapprochent le plus de la r√©alit√©. Pour cela, il apprend de ses erreurs et r√©essaie en continu d‚Äôobtenir l‚Äô≈ìuvre la plus proche du r√©el.\nLe juge (Discriminateur) : L‚Äôobjectif de ce joueur est de v√©rifier si les ≈ìuvres r√©alis√©es par l‚Äôartiste peuvent para√Ætre r√©elles ou si elles sont encore trop fausses. Pour cela, il compare les ≈ìuvres de l‚Äôartiste √† des ≈ìuvres r√©elles faites par des peintres.\n\nTant que l‚Äôimage n‚Äôest pas accept√© par le juge, l‚Äôartiste continue √† produire des ≈ìuvres. Voil√†, le fonctionnement d‚Äôun GAN ou deux r√©seaux de neurones se font concurrence. De cette mani√®re, il est possible de cr√©er des images, des vid√©os ou d‚Äôautres contenues de tr√®s bonnes qualit√©s.\n\n\n\n\n\n\n\nSch√©ma GAN\n\n\n\n\nPr√©sentation au public\nNous avons d√©cid√© de pr√©senter au public le travail d‚ÄôObvious. Ce collectif de chercheurs, d‚Äôartistes travaille avec des mod√®les d‚Äôapprentissage profond pour explorer le potentiel cr√©atif de l‚Äôintelligence artificielle. Ils ont justement utilis√© des GAN pour g√©n√©rer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d‚ÄôEdmond de Belamy. Ce tableau est une impression sur toile qui est rentr√©e dans l‚Äôhistoire de l‚Äôart moderne. Cela, car c‚Äôest la premi√®re ≈ìuvre d‚Äôart produite par un logiciel d‚Äôintelligence artificielle √† √™tre pr√©sent√©e dans une salle des ventes. Pour couronner le tout, il a √©t√© vendu 432 500 dollars chez Christie‚Äôs le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est tr√®s difficile √† premi√®re vue de d√©terminer qu‚Äôune machine a pu en √™tre l‚Äôauteur.   Obvious √† utiliser un GAN, en l‚Äôentra√Ænant sur 15 000 portraits classiques r√©alis√©s entre le 14e et 20e si√®cle. L‚Äôalgorithme devait donc produire un tableau en sortie qui serait tr√®s ressemblant aux portraits classiques. Nous avons d√©cid√© de comparer le portrait d‚ÄôEdmond de Belamy √† une ≈ìuvre du Louvre se trouvant dans la salle 846 de l‚Äôaile Richelieu du mus√©e. C‚Äôest une peinture datant du 17e si√®cle r√©alis√©e par Jean Bray, peintre n√©erlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\nPortrait d‚ÄôEdmond de Belamy, Collectif Obvious\n\n\n\n\n\n\n\n\n\n\n\nPortrait d‚Äôhomme, 1658\n\n\n\n\n\nLes visiteurs √©taient agr√©ablement surpris par le r√©alisme du portrait d‚ÄôEdmond de Belamy, mais aussi par le prix de vente de l‚Äô≈ìuvre. Ils pensaient pouvoir reconna√Ætre la r√©alisation d‚Äôune IA."
  },
  {
    "objectID": "projets/but_sd.html",
    "href": "projets/but_sd.html",
    "title": "Projets BUT SD",
    "section": "",
    "text": "Etude statistique dans un essai clinique\n\n\n\nR\n\n\nStatistique\n\n\n\nR√©alisation d‚Äôune √©tude statistique dans un essai clinique qui permettera de d√©terminer l‚Äôeffet d‚Äôun nouveau m√©dicament chez les patients atteint de dr√©panocytose.\n\n\n\n1 d√©c. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMigration de donn√©es : De SQL √† NoSQL\n\n\n\nSQL\n\n\n\nCe projet vise √† migrer des donn√©es d‚Äôun environnement SQL vers un environnement NoSQL pour une entreprise automobile.\n\n\n\n28 d√©c. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNocturne au mus√©e du Louvre\n\n\n\nIA\n\n\n\nPr√©sentation d‚Äôune ≈ìuvre g√©n√©r√©e par une IA aux visiteurs du Mus√©e du Louvre.\n\n\n\n24 nov. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS√©rie Temporelle : Production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis\n\n\n\nR\n\n\nRShiny\n\n\nStatistique\n\n\n\nCe projet consiste en l‚Äôanalyse de la production d‚Äô√©l√©ctricit√© par combustion aux √âtats-Unis entre 2001 et 2022. Nous r√©alisons √©galement une pr√©vision pour l‚Äôann√©e 2023.\n\n\n\n19 janv. 2024\n\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "projets/fontaines_eau.html",
    "href": "projets/fontaines_eau.html",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "",
    "text": "Ce mini-projet utilise le langage R pour cr√©er une carte interactive des points d‚Äôeau publics dans la ville de Paris. Il inclut √©galement une application Shiny, permettant une exploration dynamique et interactive des donn√©es."
  },
  {
    "objectID": "projets/fontaines_eau.html#aper√ßu-de-lapplication",
    "href": "projets/fontaines_eau.html#aper√ßu-de-lapplication",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "1 Aper√ßu de l‚Äôapplication",
    "text": "1 Aper√ßu de l‚Äôapplication\n\n\n\n\n\nL‚Äôobjectif principal de ce projet est de proposer une visualisation g√©ographique claire et accessible des points d‚Äôeau r√©partis dans Paris, gr√¢ce √† une interface conviviale d√©velopp√©e avec Shiny. Les donn√©es exploit√©es proviennent de la Ville de Paris et sont accessibles sur le site data.gouv. Cette application, √† la fois fluide et interactive, permet aux utilisateurs d‚Äôexplorer facilement les diff√©rents points d‚Äôeau pr√©sents dans chaque arrondissement de la capitale."
  },
  {
    "objectID": "projets/fontaines_eau.html#acc√©dez-√†-lapplication",
    "href": "projets/fontaines_eau.html#acc√©dez-√†-lapplication",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "2 Acc√©dez √† l‚Äôapplication",
    "text": "2 Acc√©dez √† l‚Äôapplication\nAcc√©der √† l‚Äôapplication ici\nRepo github"
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html",
    "href": "projets/projets_but/essai_clinique.html",
    "title": "Etude statistique dans un essai clinique",
    "section": "",
    "text": "Ce projet consiste en la r√©alisation d‚Äôune √©tude statistique dans le cadre d‚Äôun essai clinique. Nous travaillons sur un jeu de donn√©es simul√©es pour effectuer l‚Äôanalyse statistique d‚Äôune √©tude de phase 3. L‚Äôobjectif est de fournir au laboratoire une Autorisation de Mise sur le March√© (AMM) avec une indication dans la prise en charge de la dr√©panocytose.\nLa dr√©panocytose est une maladie g√©n√©tique qui affecte les globules rouges et peut entra√Æner des complications graves. Elle se manifeste notamment par une an√©mie, des crises douloureuses et un risque accru d‚Äôinfections.\n\n\n\nSch√©ma de la dr√©panocytose\n\n\nLors d‚Äôune √©tude de phase 3, les chercheurs comparent un nouveau traitement prometteur au traitement standard, qui est le traitement reconnu et g√©n√©ralement administr√© pour une affection ou une maladie. Dans notre √©tude statistique, le nouveau traitement s‚Äôappelle le Voxelotor et nous comparons ces effets √† un placebo (traitement qui n‚Äôa aucune action sp√©cifique sur le trouble qu‚Äôil vise √† soulager). Pour cela, nous r√©alisons des tests statistiques sur des hypoth√®ses d√©finit en amont.\nLe d√©p√¥t GitHub contenant le rapport complet du projet, ainsi que le code R, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html#plan-danalyse-statistique",
    "href": "projets/projets_but/essai_clinique.html#plan-danalyse-statistique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Plan d‚Äôanalyse statistique",
    "text": "Plan d‚Äôanalyse statistique\nLe plan d‚Äôanalyse statistique (SAP, disponible ici) d√©crit de mani√®re d√©taill√©e la strat√©gie d‚Äôanalyse des donn√©es de l‚Äô√©tude. Il garantit la rigueur, la transparence et la reproductibilit√© des analyses statistiques. Ce plan est g√©n√©ralement r√©dig√© avant la fin de l‚Äôessai, afin d‚Äô√©viter tout biais pouvant d√©couler des r√©sultats observ√©s.\nPour √©valuer l‚Äôeffet du voxelotor sur l‚Äôam√©lioration de l‚Äôh√©moglobine des patients par rapport au placebo, les analyses d√©finies dans le SAP sont toutes r√©alis√©es. L‚Äôanalyse principale porte sur la r√©ponse en h√©moglobine √† la semaine 72, qui correspond au maximum du suivi des patients. L‚Äôanalyse secondaire quant √† elle examine le changement de l‚Äôh√©moglobine entre le d√©but de l‚Äô√©tude et la semaine 72 sur plusieurs marqueurs cliniques.\n\nAnalyse principal\nLa r√©ponse en h√©moglobine est d√©finie comme une augmentation de plus de 1g/dL par rapport √† la valeur de r√©f√©rence mesur√©e avant le d√©but du traitement. Cela signifie qu‚Äôun patient est consid√©r√© comme ayant r√©pondu favorablement au traitement si son taux d‚Äôh√©moglobine a augment√© d‚Äôau moins 1g/dL par rapport √† la valeur initiale, mesur√©e avant l‚Äôadministration du voxelotor. Si aucune mesure d‚Äôh√©moglobine n‚Äôest disponible √† la semaine 72, le patient est imput√© comme non-r√©pondeur. Par exemple, si le patient ne se pr√©sente pas √† la consultation, si les donn√©es sont perdues ou invalides, ce patient sera consid√©r√© comme n‚Äôayant pas montr√© de r√©ponse au traitement.\nNous utilisons un test statistique du Chi-2 avec correction de Yates pour comparer les taux de r√©ponse en h√©moglobine entre le groupe trait√© par VOX_1500 et le groupe par le placebo, afin de d√©terminer s‚Äôil existe une diff√©rence significative.\n\n\n\nR√©partition des traitements dans notre population\n\n\nLes r√©sultats du test indiquent qu‚Äôil n‚Äôy a pas de preuve statistiquement significative d‚Äôune diff√©rence entre les taux de r√©ponse en h√©moglobine des deux groupes.\nCependant, certains facteurs pourraient √™tre amen√©s √† faire varier l‚Äôefficacit√© du VOX_1500. C‚Äôest pourquoi, nous r√©alisons le m√™me test statistique avec les m√™mes hypoth√®ses sur des sous-populations d√©finis √† partir des facteurs suivants : l‚Äô√¢ge des patients, l‚Äôutilisation pr√©alable d‚Äôhydroxyur√©e (HU) et l‚Äôhistorique des crises vaso-occlusives (VOC).\nLes r√©sultats de cette √©tude, montrent que le VOX_1500 semble plus efficace chez certains groupes de patients. Premi√®rement, on observe une diff√©rence significative en faveur du traitement chez les non-utilisateurs d‚ÄôHU. Cela indique une meilleure r√©ponse en h√©moglobine par rapport au placebo. En revanche, chez les utilisateurs d‚ÄôHU, aucune diff√©rence notable n‚Äôa √©t√© trouv√©e entre les deux traitements.\nConcernant les crises vaso-occlusives, les patients ayant eu une seule crise r√©pondent mieux au VOX_15000, alors que ceux ayant eu plusieurs crises ne montrent aucun signe positif suppl√©mentaire par rapport au placebo.\nEnfin, l‚Äôanalyse par tranche d‚Äô√¢ge r√©v√®le des r√©sultats similaires entre les groupes VOX_1500 et placebo, tant chez les adolescents que chez les adultes.\nEn somme, ces observations sugg√®rent que l‚Äôefficacit√© du VOX_1500 pourrait d√©pendre de certains facteurs cliniques, mais des √©tudes suppl√©mentaires seront n√©cessaires pour valider ces conclusions.\n\n\nAnalyse secondaire\nDans cette secone partie, nous essayons de d√©terminer s‚Äôil existe une diff√©rence dans le changement moyen des taux d‚Äôh√©moglobine entre les deux groupes (VOX_1500 et placebo). Pour r√©pondre √† cette question, nous r√©alisons une analyse statistique √† l‚Äôaide de l‚ÄôANOVA. Cette derni√®re est une m√©thode statistique permettant de comparer les moyennes de plusieurs groupes afin de d√©terminer si au moins un groupe diff√®re significativement des autres. Plus concr√®tement, elle permet d‚Äô√©valuer si les variations observ√©es dans les donn√©es sont dues √† des diff√©rences entre les groupes ou √† des variations al√©atoires au sein des groupes eux-m√™mes. Elle nous est tr√®s utile ici, car elle permet d‚Äôajuster les r√©sultats en tenant compte des valeurs d‚Äôh√©moglobine de d√©part (baseline) et de l‚Äôutilisation pr√©alable d‚Äôhydroxyur√©e (HU) comme facteur suppl√©mentaire.\nLes r√©sultats r√©v√®lent que le VOX_1500 entra√Æne une augmentation significative du taux d‚Äôh√©moglobine par rapport au placebo.\n\n\n\nRepr√©sentation graphique du changement ajust√© d‚Äôh√©moglobine entre le d√©part et la semaine 72\n\n\nLes patients trait√©s par VOX_1500 pr√©sentent une augmentation moyenne de 0,51 g/dL d‚Äôh√©moglobine, contre 0,23 g/dL pour le groupe placebo. Cette diff√©rence est statistiquement significative au risque 5 %. Nous observons, un tr√®s faible impact des taux d‚Äôh√©moglobine au d√©part sur l‚Äô√©volution des niveaux observ√©s. Cependant, l‚Äôutilisation pr√©alable d‚Äôhydroxyur√©e influence fortement les r√©sultats, les patients sous ce traitement montrant des √©volutions diff√©rentes de ceux n‚Äôen prenant pas. Le traitement peut donc offrir un b√©n√©fice notable en termes d‚Äôaugmentation de l‚Äôh√©moglobine, en particulier chez les patients n‚Äôutilisant pas d‚Äôhydroxyur√©e.\n\n\nMarqueurs d‚Äôh√©molyse\nNous analysons ensuite l‚Äô√©volution des marqueurs d‚Äôh√©molyse en utilisant la m√™me m√©thode. Ces indicateurs biologiques permettent d‚Äô√©valuer la destruction des globules rouges dans le sang. Deux marqueurs cl√©sretiennent notre attention :\n\nBilirubine non conjugu√©e : elle augmente lorsque les globules rouges sont d√©truits, refl√©tant une h√©molyse active.\nPourcentage de r√©ticulocytes : ces globules rouges immatures sont produits en r√©ponse √† la destruction des globules matures. Un taux √©lev√© traduit une compensation de l‚Äôorganisme face √† l‚Äôh√©molyse.\n\nL‚Äôanalyse de ces param√®tres permet de mieux comprendre l‚Äôimpact du traitement sur la sant√© des globules rouges.\n\n\n\nVariations dans les marqueurs d‚Äôh√©molyse entre la baseline et la semaine 72\n\n\nL‚Äôanalyse des marqueurs d‚Äôh√©molyse r√©v√®le que le traitement VOX_1500 a un effet significatif sur la r√©duction de la bilirubine non conjugu√©e et des r√©ticulocytes, deux indicateurs cl√©s de la destruction des globules rouges.\n\n\n\nRepr√©sentation graphique du changement dans les marqueurs d‚Äôh√©molyse\n\n\nLe traitement VOX_1500 entra√Æne une diminution moyenne de -8,09 g/dL contre -4,56 g/dL pour le placebo. Cette diff√©rence statistiquement significative confirme l‚Äôeffet notable du traitement. De plus, le VOX_1500 montre √©galement un impact significatif, sans que les valeurs initiales ou l‚Äôutilisation d‚ÄôHydroxyur√©e n‚Äôinfluencent les r√©sultats.\nOn en conclut que le traitement VOX_1500 a un effet plus marqu√© que le placebo sur la r√©duction de la bilirubine non conjugu√©e et des r√©ticulocytes, comme le montre la diff√©rence significative dans les changements moyens ajust√©s pour ces deux marqueurs d‚Äôh√©molyse. Le traitement pourrait ainsi √™tre plus efficace pour r√©duire les caract√©ristiques d‚Äôh√©molyse par rapport au placebo."
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html#nouvel-essai-clinique",
    "href": "projets/projets_but/essai_clinique.html#nouvel-essai-clinique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Nouvel essai clinique",
    "text": "Nouvel essai clinique\nAu vu des facteurs influents vus dans l‚Äôanalyse principal. Il se peut que le laboratoire envisage de lancer un nouvel essai clinique afin d‚Äô√©valuer l‚Äôefficacit√© du VOX_1500 chez les adolescents (patient de 12 √† 18 ans). En utilisant le m√™me crit√®re principal que pr√©c√©demment et en se basant sur les r√©sultats obtenus pour le voxelotor dans cette population, tout en supposant un taux de r√©ponse de 10% pour le groupe placebo, nous proc√©dons au calcul du nombre de patients n√©cessaires pour d√©montrer l‚Äôefficacit√© du traitement. Ce calcul est effectu√© en visant une puissance statistique de 90% et un risque alpha de 5%, avec un ratio de traitement 1:1.\n\n\n\nProgramme R du calcul du nombre de patients\n\n\nLa taille de l‚Äô√©chantillon n√©cessaire pour ce nouvel essai clinique est de 48 patients au total, r√©partis √©galement entre les deux groupes, soit 24 patients par groupe (placebo et le VOX_1500). Cette estimation garantit une puissance de 90% pour d√©tecter une diff√©rence significative entre les groupes 29 avec un risque alpha de 5%."
  },
  {
    "objectID": "projets/projets_but/migration.html",
    "href": "projets/projets_but/migration.html",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "",
    "text": "Ce projet vise √† migrer des donn√©es d‚Äôun environnement SQL vers un environnement NoSQL. Concr√®tement, il s‚Äôagit de transf√©rer les informations stock√©es dans une base de donn√©es relationnelle traditionnelle, o√π les donn√©es sont organis√©es en tables avec des relations fixes, vers une base de donn√©es NoSQL, qui offre une structure plus flexible adapt√©e aux donn√©es non structur√©es ou semi-structur√©es.\nNous travaillons avec les donn√©es d‚Äôune entreprise de voitures qui rencontre des probl√®mes avec sa base de donn√©es actuelle : les requ√™tes sont lentes et des d√©faillances serveur entra√Ænent des pertes de donn√©es. Pour r√©soudre ces probl√®mes, nous avons d√©cid√© de passer √† un environnement NoSQL. Cette technologie permet de stocker des donn√©es sous une forme non structur√©e, offrant ainsi plus de flexibilit√© et de performance. Cette migration vise √† am√©liorer la performance des requ√™tes et √† pr√©parer l‚Äôinfrastructure pour une croissance future.\nLe d√©p√¥t GitHub contenant le rapport complet du projet, ainsi que les requ√™tes SQL et NoSQL associ√©es, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/migration.html#base-de-donn√©es-relationnelle",
    "href": "projets/projets_but/migration.html#base-de-donn√©es-relationnelle",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Base de donn√©es relationnelle",
    "text": "Base de donn√©es relationnelle\nLa base de donn√©es relationnelle initiale contient des informations sur les v√©hicules, les clients, les commandes, les employ√©s‚Ä¶ La repr√©sentation des donn√©es est claire et bien organis√©e. Chaque table dispose de relations avec d‚Äôautres tables, ce qui permet de structurer efficacement les informations et de faciliter les requ√™tes complexes.\n\n\n\nSch√©ma relationnel de la bdd initial\n\n\nDans un premier temps, nous avons cr√©√© des requ√™tes SQL sur cette base de donn√©es. Ces requ√™tes serviront de tests pour √©valuer le succ√®s de la migration. Nous comparerons les r√©sultats obtenus dans la base de donn√©es relationnelle avec ceux obtenus dans la base NoSQL pour v√©rifier l‚Äôint√©grit√© et la performance de la migration.\n\n\n\nRequ√™tes SQL\n\n\nLa base de donn√©es est au format SQLite. Nous avons import√© le module sqlite3 en Python pour √©tablir la connexion et interagir avec la base. Ensuite, nous avons utilis√© la biblioth√®que Pandas et notamment sa fonction read_sql_query(), pour ex√©cuter et lire les r√©sultats des requ√™tes SQL."
  },
  {
    "objectID": "projets/projets_but/migration.html#algorithme-de-migration",
    "href": "projets/projets_but/migration.html#algorithme-de-migration",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Algorithme de migration",
    "text": "Algorithme de migration\nIl existe plusieurs types de bases de donn√©es NoSQL (Cl√©-valeur, Document, Colonne et Graphe), chacun adapt√© √† ses propres cas d‚Äôusage et ayant ses propres avantages et inconv√©nients, notamment en termes de scalabilit√© et de flexibilit√©. Le choix d√©pend donc de plusieurs facteurs cl√©s, comme la structure des donn√©es, les exigences de performance‚Ä¶\nEn ce qui nous concerne, nous pouvons r√©aliser une migration vers un environnement NoSQL, car l‚Äôentreprise dispose d‚Äôune grande quantit√© de donn√©es structur√©es en constante croissance. De plus, il est possible d‚Äôam√©liorer significativement les performances d‚Äôacc√®s aux donn√©es en optimisant le traitement de donn√©es plus importantes et en r√©duisant le temps de latence. On souhaite donc une solution qui offre plus de flexibilit√© et √©volutivit√©, tout en pr√©servant l‚Äôint√©grit√© des donn√©es de notre base initial.\nApr√®s m√ªre r√©flexion, le format document est celui s‚Äôadaptant le mieux √† notre objectif. En effet, il permet de structurer naturellement les entit√©s de mani√®re hi√©rarchique. Par exemple, un client peut √™tre repr√©sent√© par un document contenant ses commandes, chaque commande incluant les produits associ√©s. De plus, il offre une grande flexibilit√©, permettant de traiter diff√©rents types de donn√©es sans modifications complexes du sch√©ma. Enfin, il permet une scalabilit√© horizontale gr√¢ce √† la partition de document, c-√†-d si les donn√©es augmentent, on peut facilement ajouter de nouveaux serveurs pour stocker et g√©rer plus de documents, sans tout restructurer.\n\n\n\nExemple d‚Äôune mod√©lisation au format Document\n\n\nCe mod√®le pr√©sente quelques inconv√©nients, notamment des performances limit√©es pour les requ√™tes complexes ou les jointures entre documents. De plus, les mises √† jour simultan√©es de documents imbriqu√©s peuvent √™tre plus difficiles √† g√©rer.\nNous avons d√©cid√© de structurer nos donn√©es autour de quatres collections : customers, payments, orders et employees."
  },
  {
    "objectID": "projets/projets_but/migration.html#script-de-migration",
    "href": "projets/projets_but/migration.html#script-de-migration",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Script de migration",
    "text": "Script de migration\nAvant de d√©velopper un script de migration, nous avons d‚Äôabord √©tabli un pseudo-algorithme dans l‚Äôobjecitf de structurer et organiser la logique de notre programme.\nEnsuite, pour la migration de nos donn√©es, nous avons utilis√© SQLite comme source et MongoDB comme destination, en exploitant les biblioth√®ques Python sqlite3, pymongo et pandas. Le processus inclut l‚Äôextraction des donn√©es de SQLite, leur transformation au format document compatible avec MongoDB, et leur insertion dans les collections appropri√©es."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rachid Sahli",
    "section": "",
    "text": "Je suis √©tudiant en Science des Donn√©es √† l‚ÄôIUT Paris Rives de Seine et alternant √† l‚ÄôINSEE. Au sein de la Direction des Statistiques D√©mographiques et Sociales, je travaille sur des probl√©matiques li√©es √† la mesure de la couverture des bases de sondage.\nJe travaille principalement avec R et Python et j‚Äôai cr√©√© un site de ressources appel√© PratiqueR. Vous pouvez √©galement me retrouver sur YouTube, o√π je partage des vid√©os sur R et son environnement.\nEn-dehors de mes √©tudes, je consacre mon temps libre √† la construction de robots et je partage mes id√©es sur mon blog, Jiqiren.\nPassionn√© par les statistiques, la robotique ü¶øü§ñ, le v√©lo üö≤, le cin√©ma üéûÔ∏è, et la programmation üëæ, j‚Äôaime explorer et partager mes d√©couvertes dans ces domaines.\nJe suis tr√®s curieux et toujours ouvert √† de nouvelles id√©es. N‚Äôh√©sitez pas √† me contacter pour discuter d‚Äôun projet ou explorer des opportunit√©s de collaboration.\n\n\nFormation\n\n\n\n\n\n\nIUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, 75016\n\n\n\nBUT Science des donn√©es, parcours exploration et mod√©lisation statistique | Sept 2022 - Juin 2025\nCours suivis : Statistique inf√©rentielle, param√©trique et non-param√©trique, Mod√©lisation statistique, Alg√®bre lin√©aire, Analyse, Probabilit√©s, Machine learning, Data mining, Programmation, Base de donn√©es\n\n\n\n\nExp√©rience\n\n\n\n\n\n\nInstitut national de la statistique et des √©tudes √©conomiques (INSEE) | Montrouge, 92120\n\n\n\nStatisticien | Sept 2023 - Sept 2025 Au sein de la Direction des statistiques d√©mographiques et sociales, j‚Äôai men√© des travaux d‚Äôappariement de donn√©es administratives. L‚Äôobjectif de ces travaux √©tait de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j‚Äôai compar√© diff√©rents algorithmes d‚Äôappariement √† l‚Äôaide d‚Äôanalyses statistiques. De plus, j‚Äôai mis en place des mod√®les de classification exploitant les donn√©es administratives afin de r√©aliser des pr√©dictions."
  },
  {
    "objectID": "blog/titanic/titanic.html",
    "href": "blog/titanic/titanic.html",
    "title": "Peut-on pr√©dire les survivants du Titanic ?",
    "section": "",
    "text": "Le naufrage du Titanic est l‚Äôun des plus terribles qu‚Äôil ait pu exister. Le 15 avril 1912, lors de son voyage inaugural, ce paquebot de 269 m√®tres de long (une trentaine de m√®tres de moins que la tour Eiffel) a coul√©. Le trajet √©tait le suivant, le 10 avril 1912, le RMS Titanic, quittait Southampton en Angleterre pour une travers√©e qui devait le conduire jusqu‚Äô√† New York aux Etats-Unis, apr√®s une escale √† Cherbourg et en Irlande.\nIl √©tait qualifi√© d‚Äôinsubmersible, mais sa collision avec un iceberg en a d√©cid√© autrement. Malheureusement, le nombre de canots de sauvetage √©tait inf√©rieur au nombre de passagers √† bord. Cela a entra√Æn√© la mort de 1502 des 2224 passagers et membre d‚Äô√©quipage. Miracle ! Il y a eu 711 rescap√©s.\nBien que la chance ait jou√© un r√¥le dans la survie des passagers, certaines cat√©gories de personnes semblent avoir eu un avantage. Est-il possible de pr√©dire ces chances de survie √† l‚Äôavance ? Si oui, quel type de personnes avaient le plus de chance de survivre ?\nNous allons utilis√© des m√©thodes d‚Äôapprentissage supervis√© pour r√©pondre √† ces questions. L‚Äôobjectif de l‚Äôapprentissage supervis√© est de pr√©voir l‚Äô√©tiquette \\(Y\\) ou la valeur de \\(Y\\) (r√©gression) associ√©e √† une nouvelle entr√©e, o√π il est sous-entendu que (\\(X, Y\\)) est une nouvelle r√©alisation des donn√©es, ind√©pendante de l‚Äô√©chantillon observ√©. Ici, nous utilserons 4 algorithmes diff√©rents : l‚Äôalgorithme des \\(k\\) plus proches voisins, la r√©gression logistique, un arbre de d√©cision et l‚Äôalgorithme SVM.\nNous utiliserons Python et la librairie Scikit-learn, biblioth√®que open source d‚Äôapprentissage automatique d√©di√©e au machine learning pour r√©alis√© une classification. Ici, on est dans un cas de classification et non de r√©gression, car notre variable cible (\\(Y\\)) est une √©tiquette (Surv√©cu ou Non survivant)."
  },
  {
    "objectID": "blog/titanic/titanic.html#visualisation-graphique",
    "href": "blog/titanic/titanic.html#visualisation-graphique",
    "title": "Peut-on pr√©dire les survivants du Titanic ?",
    "section": "Visualisation graphique",
    "text": "Visualisation graphique\nLa section pr√©c√©dente nous a permis d‚Äôidentifier les incoh√©rences et anomalies dans nos donn√©es. Dans cette m√™me optique, nous allons maintenant les analyser visuellement afin de mieux comprendre leur r√©partition.\nNous pr√©sentons ici seulement quelques graphiques int√©ressants, mais il existe bien d‚Äôautres visualisations possibles tout aussi int√©ressantes.\n\n\nCliquer pour voir le code\ncmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\ncolors = [mcolors.rgb2hex(c) for c in cmap.colors]\n\nfig = px.scatter(titanic, \nx=\"Age\", y=\"Fare\", \ncolor=\"SibSp\", size=\"Pclass\",\ncolor_continuous_scale=colors,\nlabels={\"Age\": \"Age\", \"Fare\": \"Fare\"},\nlog_x=True, log_y=True)\n\nfig.update_traces(marker=dict(sizemode='area', \nopacity=0.7, line=dict(width=0)),\nselector=dict(mode='markers'))\n\nfig.update_layout(\n  template=\"plotly_white\",\n  title_text='Relation entre l\\'√¢ge et le prix du Billet selon le Nombre de fr√®res/s≈ìurs et conjoints',\n  )\n\nfig.show()\n\n\n                                                \n\n\nLes passagers les plus √¢g√©s, ayant pay√© les tarifs les plus √©lev√©s, voyagent principalement en 1 ≥·µâ classe et sont souvent seuls ou avec un membre de leur famille. En revanche, ceux ayant pay√© les tarifs les plus bas sont majoritairement en 2·µâ ou 3·µâ classe, avec souvent plusieurs fr√®res et s≈ìurs √† bord.\n\n\nCliquer pour voir le code\ncolor_map = {0: 'red', 1: 'green'}\n\nfig = px.strip(titanic, \n               x=\"Sex\", y=\"Age\", \n               color=\"Survived\", \n               category_orders={\"Sex\": [\"male\", \"female\"]}, \n               labels={\"Age\": \"Age\", \"Sex\": \"Sex\"},\n               color_discrete_map=color_map)\n\nfig.update_traces(marker=dict(size=8, opacity=0.7, line=dict(width=0)),\n                  selector=dict(mode='markers'))\n\nfig.update_layout(\n    yaxis_title=\"\",\n    template=\"plotly_white\",\n    title_text='R√©partition de l\\'√¢ge selon le sexe et la survie √† bord'\n)\n\nfig.show()\n\n\n                                                \n\n\nLes femmes (35,2 %) sont beaucoup moins repr√©sent√©es dans le jeu de donn√©es que les hommes (64,8 %). On observe qu‚Äôil y a une densit√© √©lev√©e de survivantes dans la tranche des jeunes adultes et des adultes moyens. Les hommes sont sur-repr√©sent√©s parmi les non-survivants, avec une concentration plus marqu√©e parmi les adultes jeunes et moyens.\nNous savons que les femmes et les enfants avaient des priorit√©s d‚Äô√©vacuation pendant le naufrage et elles avaient donc une probabilit√© de survie plus √©lev√©e que celle des hommes.\n\n\nCliquer pour voir le code\nfig = px.histogram(titanic, \n                   x=\"Age\", \n                   color=\"Survived\", \n                   facet_col=\"Survived\",\n                   nbins=20,\n                   color_discrete_map={0: 'red', 1: 'green'},\n                   labels={\"Age\": \"Age\", \"Survived\": \"Survived\"},\n                   histnorm=\"percent\")\n\nfig.update_traces(marker_line_color=\"black\",\n                  marker_line_width=1)\n\nfig.update_layout(\n    title=\"R√©partition de l'√¢ge en fonction de la survie\",\n    template=\"plotly_white\",\n    xaxis_title=\"√Çge\",\n    yaxis_title=\"Fr√©quence (%)\"\n)\n\nfig.show()"
  },
  {
    "objectID": "blog/titanic/titanic.html#traitement-des-donn√©es",
    "href": "blog/titanic/titanic.html#traitement-des-donn√©es",
    "title": "Peut-on pr√©dire les survivants du Titanic ?",
    "section": "Traitement des donn√©es",
    "text": "Traitement des donn√©es\nMaintenant que nous avons une vue d‚Äôensemble de nos donn√©es, nous allons nous concentrer sur le traitement des anomalies identifi√©es pr√©c√©demment, en vue de pr√©parer efficacement la construction du mod√®le.\nNous commen√ßons par convertir la variable Sexe en une variable binaire. Ici, nous la rempla√ßons directement en cr√©ant une nouvelle version par-dessus. Cependant, il est recommand√© de cr√©er une variable distincte afin de pr√©server l‚Äôint√©grit√© des donn√©es d‚Äôorigine.\n---\ntitanic['Sex'] = (titanic['Sex'] == 'female').astype(int)\n---\n\n\nAvant conversion :\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: object\n\nApr√®s conversion :\n0    0\n1    1\n2    1\n3    1\n4    0\nName: Sex, dtype: int64\n\n\nNous passons maintenant au traitement des donn√©es manquantes. Il existe plusieurs approches pour compl√©ter les donn√©es manquantes. Par exemple, la suppresion des observations, l‚Äôimputation de ces donn√©es par une valeur tel que la moyenne, la m√©diane ‚Ä¶ On introduirait cependant un biais sur cette valeur. Il est √©galement possible d‚Äôimputer les donn√©es manquantes √† l‚Äôaide d‚Äôune m√©thode statistique (r√©gression, \\(k\\) plus proches voisins).\n\n\nCliquer pour voir le code\ntitanic.count().plot(kind='barh', title=\"Nombre d'occurrences par variable avant traitement\",\n                                                 xlabel=\"Nombre d'occurences\",\n                                                 ylabel=\"Variable\")\n\n\n\n\n\n\n\n\n\nDans notre jeu de donn√©es, il manque des donn√©es pour les variables Age, Embarked et Cabin.\nLa variable Cabin est tr√®s peu renseign√©e. Il est inutile d‚Äôimputer les valeurs manquantes par une autre valeur, nous choisissons donc de supprimer cette variable. Toutefois, nous d√©cidons de cr√©er une nouvelle variable indiquant si cette donn√©e a √©t√© renseign√©e ou non {0; 1}.\n---\ntitanic['CabinMissing'] = titanic.Cabin.isnull().astype(int) # Variable indicatrice\n\ntitanic = titanic.drop('Cabin', axis=1) # Suppresion de Cabin\n\ntitanic['CabinMissing'] # Visualisation de CabinMissing\n---\n\n\n0      1\n1      0\n2      1\n3      0\n4      1\n      ..\n886    1\n887    0\n888    1\n889    0\n890    1\nName: CabinMissing, Length: 891, dtype: int64\n\n\nLa variable Age est bien plus renseign√© que la variable Cabin, on d√©cide d‚Äôimputer ces valeurs manquantes par l‚Äô√¢ge m√©dian. √âvidemment, cela biaisera notre distribution.\nL‚Äô√¢ge m√©dian des passagers est de 28 ans. Nous imputerons les 177 valeurs manquantes par cet valeur.\n---\ntitanic.Age.median()\n---\n\n\n28.0\n\n\n---\nage_median = titanic.Age.median()\n\ntitanic['Age'] = titanic.Age.fillna(median_age)\n---\nCi-dessous, nous observons les distributions de l‚Äô√¢ge avant et apr√®s imputation. On remarque clairement que l‚Äôimputation a modifi√© la distribution des donn√©es.\n\n\nCliquer pour voir le code\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=[\"Avant imputation\", \"Apr√®s imputation\"], shared_yaxes=True)\n\nfig.add_trace(\n    go.Histogram(x=titanic[\"Age\"], nbinsx=20, name=\"Age\", marker_color=\"blue\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=titanic[\"Age_2\"], nbinsx=20, name=\"Age_2\", marker_color=\"darkcyan\"),\n    row=1, col=2\n)\n\nfig.update_traces(marker_line_color=\"black\",\n                  marker_line_width=1)\n\nfig.update_layout(\n    title=\"Distribution de l'√¢ge des passagers du Titanic\",\n    template=\"plotly_white\",\n    xaxis_title=\"√Çge\",\n    yaxis_title=\"Nombre de passagers\",\n    showlegend=False\n)\n\nfig.show()\n\n\n                                                \n\n\nNous rempla√ßons les deux valeurs manquantes de la variable Embarked, repr√©sentant le port de d√©part, par le port de d√©part majoritaire des passagers.\n---\nmaj_port = titanic[\"Embarked\"].mode()[0]\n\ntitanic[\"Embarked\"].fillna(maj_port, inplace=True)\n---\nEnfin, nous excluons les variables suivantes :\n\nPassengerId : Il s‚Äôagit d‚Äôun simple identifiant sans influence sur la survie.\nName : Cette variable contient des informations textuelles difficiles √† exploiter directement.\nTicket : Elle pr√©sente trop de valeurs uniques et apporte peu d‚Äôinformations pertinentes.\n\n---\ntitanic = titanic.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"])\n---\nLe jeu de donn√©es est d√©sormais complet, sans aucune valeur manquante.\n\n\nCliquer pour voir le code\ntitanic.count().plot(kind='barh', color='green', title=\"Nombre d'occurrences par variable apr√®s traitement\",\n                                                 xlabel=\"Nombre d'occurences\",\n                                                 ylabel=\"Variable\")"
  },
  {
    "objectID": "blog/titanic/titanic.html#variables-pertinentes",
    "href": "blog/titanic/titanic.html#variables-pertinentes",
    "title": "Peut-on pr√©dire les survivants du Titanic ?",
    "section": "Variables pertinentes",
    "text": "Variables pertinentes\nNous commen√ßons par mesurer la corr√©lation entre les variables explicatives et notre variable cible afin d‚Äôidentifier celles qui ont le plus d‚Äôinfluence sur la pr√©diction.\n---\ncor = titanic.corr(numeric_only=True)[\"Survived\"].sort_values(ascending=False)\ncor\n---\n\n\nSurvived        1.000000\nSex             0.543351\nFare            0.257307\nParch           0.081629\nSibSp          -0.035322\nAge            -0.064910\nCabinMissing   -0.316912\nPclass         -0.338481\nName: Survived, dtype: float64"
  },
  {
    "objectID": "blog/reg_logistique.html",
    "href": "blog/reg_logistique.html",
    "title": "R√©gression logistique en pratique",
    "section": "",
    "text": "Introduction\nCe billet de blog a pour objectif d‚Äô√©tudier le mod√®le de r√©gression logistique et de le comparer ainsi √† la m√©thode des \\(k\\) plus proches voisins. Ces deux m√©thodes sont des algorithmes d‚Äôapprentissage supervis√©. Le but de l‚Äôapprentissage supervis√© est de pr√©voir l‚Äô√©tiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (r√©gression) associ√©e √† une nouvelle entr√©e \\(X\\), o√π il est sous-entendu que (\\(X,Y\\)) est une nouvelle r√©alisation des donn√©es, ind√©pendante de l‚Äô√©chantillon observ√©.\nLa r√©gression logistique est une m√©thode de classification. Ce mod√®le de classification binaire permet d‚Äôexpliquer une variable (\\(Y\\)) par p variables explicatives (\\(X_1,...,X_j\\). La variable \\(Y\\) ne peut prendre que deux modalit√©s \\(\\left\\{0 ;1\\right\\}\\). Les variables \\(X_j\\) sont exclusivement continues ou binaires (on re-code les variables qualitatives avec des 0 et 1).\nL‚Äôalgorithme des \\(k\\) plus proches voisins fonctionne de la fa√ßon suivante pour la classification. On d√©termine les \\(k\\) plus proches \\(X_i\\) de l‚Äô√©chantillon par rapport √† \\(X\\) et on attribue la modalit√© dominante parmi les \\(k\\) modalit√©s observ√©es (on parle de vote majoritaire).\nNous allons utiliser ces deux m√©thodes de classification afin de pr√©dire, en se basant sur des mesures diagnostiques, si un patient est atteint du diab√®te ou non.\n\n\nImport et pr√©paration des donn√©es\nDans un premier temps, nous importons le jeu de donn√©es ‚Äúdiabetes.csv‚Äù. Nous allons mettre en pratique nos m√©thodes de r√©gression logistique et \\(k\\) plus proches voisins sur ce dernier. Ce jeu de donn√©es est issu de l‚ÄôInstitut national du diab√®te et des maladies digestives et r√©nales. Tous les patients ici sont des femmes d‚Äôau moins 21 ans d‚Äôorigine Pima. Il contient les mesures suivantes.\n\nDescription des variables\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\nPregnancies\nNombre de grossesses\nQuanti continue\n\n\nGlucose\nConcentration de glucose plasmatique\nQuanti discr√®te\n\n\nBloodPressure\nPression art√©rielle diastolique (mm Hg)\nQuanti discr√®te\n\n\nSkinThickness\n√âpaisseur du pli cutan√© du triceps (mm)\nQuanti discr√®te\n\n\nInsulin\nInsuline s√©rique √† 2 heures (mu U/ml)\nQuanti discr√®te\n\n\nBMI\nIndice de masse corporelle (poids en kg / (taille en m)¬≤)\nQuanti continue\n\n\nDiabetesPedigree\nFonction de pr√©disposition au diab√®te\nQuanti continue\n\n\nAge\n√Çge (ann√©es)\nQuanti discr√®te\n\n\nOutcome\nStatut diab√©tique (oui ou non)\nQuanti discr√®te\n\n\n\n\n# Import library ---------------\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(class)\n\n# Import data ---------------\ndiabetes &lt;- read.csv(\"data_blog/diabetes.csv\")\n\ndiabetes$Outcome &lt;- as.factor(diabetes$Outcome)\n\nNous avons en m√™me temps import√© 3 packages qui nous seront utiles dans la r√©alisation de notre travail. √Ä savoir, ggplot pour la r√©alisation de graphique, et class pour la classification de l‚Äôalgorithme des \\(k\\) plus proches voisins.\nVoici un petit aper√ßu de nos donn√©es.\n\nhead(diabetes)\n\n  Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n6           5     116            74             0       0 25.6\n  DiabetesPedigreeFunction Age Outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n6                    0.201  30       0\n\n\nNotre jeu de donn√©es ne pr√©sente pas de valeurs manquantes (NA), mais contient des observations pour lesquelles la valeur est √©gale √† 0. Nous consid√©rerons ces valeurs comme manquantes et les supprimons. Nous faisons cela uniquement sur 5 variables pour qui l‚Äôon juge que la valeur 0 est une vraie donn√©e manquante. Par exemple, nous ne le faisons pas pour la variable Pregnancies pour qui le 0 veut tout simplement dire que la personne n‚Äôa jamais √©t√© enceinte. Nous conservons alors 392 individus sur les 768 initiaux.\n\n# Suppression des 0\ndiabetes &lt;- diabetes[diabetes$SkinThickness!=0,]\ndiabetes &lt;- diabetes[diabetes$Insulin!=0,]\ndiabetes &lt;- diabetes[diabetes$Glucose!=0,]\ndiabetes &lt;- diabetes[diabetes$BloodPressure!=0,]\ndiabetes &lt;- diabetes[diabetes$BMI!=0,] \n\nCette suppression √† pour objectif de ne pas fausser les relations que nous cherchons √† mod√©liser. Par exemple, avoir une insuline √† 0 n‚Äôest premi√®rement pas coh√©rent et peut influencer de mani√®re disproportionn√©e le mod√®le, car les autres observations sont √©loign√©es. En somme, nous essayons de garantir que le mod√®le repr√©sente fid√®lement la relation entre les variables.\n\n\nStatistique descriptive\n√Ä pr√©sent, nous allons √©tudier nos donn√©es de mani√®re descriptive afin d‚Äôen obtenir un aper√ßu et ainsi de visualiser la r√©partition de la variable \\(Y\\) (Outcome). De plus, nous analyserons les corr√©lations entre les co-variables elles-m√™mes, ainsi qu‚Äôentre celles-ci et la variable \\(Y\\).\n\nsummary(diabetes)\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   : 56.0   Min.   : 24.00   Min.   : 7.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.:21.00  \n Median : 2.000   Median :119.0   Median : 70.00   Median :29.00  \n Mean   : 3.301   Mean   :122.6   Mean   : 70.66   Mean   :29.15  \n 3rd Qu.: 5.000   3rd Qu.:143.0   3rd Qu.: 78.00   3rd Qu.:37.00  \n Max.   :17.000   Max.   :198.0   Max.   :110.00   Max.   :63.00  \n    Insulin            BMI        DiabetesPedigreeFunction      Age       \n Min.   : 14.00   Min.   :18.20   Min.   :0.0850           Min.   :21.00  \n 1st Qu.: 76.75   1st Qu.:28.40   1st Qu.:0.2697           1st Qu.:23.00  \n Median :125.50   Median :33.20   Median :0.4495           Median :27.00  \n Mean   :156.06   Mean   :33.09   Mean   :0.5230           Mean   :30.86  \n 3rd Qu.:190.00   3rd Qu.:37.10   3rd Qu.:0.6870           3rd Qu.:36.00  \n Max.   :846.00   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n Outcome\n 0:262  \n 1:130  \n        \n        \n        \n        \n\n\n\n\n\n\n\n\n\n\n\nDans notre √©chantillon, 262 patients (67 %) ne sont pas atteints de diab√®te, tandis que 130 patients (33 %) en sont atteints. Les personnes non-diab√©tiques sont donc largement plus nombreuses.\nEnviron 14,3 % des patients n‚Äôont jamais eu de grossesse. Il y a 23,7 % des patients qui ont eu une unique grossesse, et nous constatons une diminution progressive du nombre de patients √† mesure que le nombre de grossesses augmente. La majorit√© des patients ont eu au moins une grossesse, ce qui indique que les grossesses sont relativement courantes dans cet √©chantillon.\n\n\n\n\n\n\n\n\n\nOn observe ci-dessous, l‚Äô√¢ge des patients. Le plus jeune patient a 21 ans, tandis que le plus √¢g√© a 81 ans. Ce dernier se distingue clairement en haut de la bo√Æte √† moustaches, o√π il se situe assez √©loign√© des autres patients. On note √©galement 5 autres valeurs aberrantes avec un √¢ge tr√®s √©lev√©. L‚Äô√¢ge m√©dian dans l‚Äô√©chantillon est de 29 ans. Il est proche du Q1, on peut donc penser √† une repr√©sentation assez √©lev√©e de patients plut√¥t jeunes.\n\n\n\n\n\n\n\n\n\n√Ä pr√©sent, nous observons les diff√©rentes corr√©lations entre nos variables quantitatives continues. L‚Äôobjectif du coefficient de corr√©lation est de muserer la force de relation lin√©aire entre deux variables. Il se calcule de la mani√®re suivante :\n\\[\nR(x, y) = \\frac{\\text{Cov}(x, y)}{\\sigma_x \\sigma_y}\n\\]\nLe coefficient de corr√©lation est compris entre -1 et 1 :\n\nSi \\(R(x,y)\\) est proche de 0 : La relation lin√©aire est nulle.\nSi \\(R(x,y)\\) est proche de -1 : La relation lin√©aire est forte mais n√©gative.\nSi \\(R(x,y)\\) est proche de 1 : La relation lin√©aire est forte.\n\n\ncor(diabetes[,c(-1,-9)])\n\n                           Glucose BloodPressure SkinThickness   Insulin\nGlucose                  1.0000000     0.2100266     0.1988558 0.5812230\nBloodPressure            0.2100266     1.0000000     0.2325712 0.0985115\nSkinThickness            0.1988558     0.2325712     1.0000000 0.1821991\nInsulin                  0.5812230     0.0985115     0.1821991 1.0000000\nBMI                      0.2095159     0.3044034     0.6643549 0.2263965\nDiabetesPedigreeFunction 0.1401802    -0.0159711     0.1604985 0.1359058\nAge                      0.3436415     0.3000389     0.1677611 0.2170820\n                               BMI DiabetesPedigreeFunction        Age\nGlucose                  0.2095159               0.14018018 0.34364150\nBloodPressure            0.3044034              -0.01597110 0.30003895\nSkinThickness            0.6643549               0.16049853 0.16776114\nInsulin                  0.2263965               0.13590578 0.21708199\nBMI                      1.0000000               0.15877104 0.06981380\nDiabetesPedigreeFunction 0.1587710               1.00000000 0.08502911\nAge                      0.0698138               0.08502911 1.00000000\n\n\nLes variables sont toutes moyennement corr√©l√©es positivement.\nNous repr√©sentons ci-dessous les relations entre les co-variables et notre variable \\(Y\\) (Outcome).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn observe une corr√©lation entre nos variables quantitatives continues et la variable Outcome (\\(Y\\)). On le voit par exemple √† travers le lien entre la variable Glucose et la variable \\(Y\\). Les patients atteints de diab√®te pr√©sentent un taux de glucose significativement plus √©lev√© que ceux qui ne sont pas atteints par la maladie.\n\n\nMod√®le de regression logistique\nNous pouvons mettre en place notre mod√®le de r√©gression logistique. Nous commen√ßons par le mod√®le complet. C‚Äôest-√†-dire que nous s√©lectionnons toutes les co-variables pour expliquer \\(Y\\). On rappel qu‚Äôon cherche √† pr√©dire les valeurs de la variable Outcom (\\(Y\\)) √† l‚Äôaide des variables explicatives pr√©sentent dans notre jeu de donn√©es.\nOn utilise pour cela la fonction glm() (Fitting Generalized Linear Models).\n\nmodel_complet &lt;- glm(Outcome~., family = \"binomial\", data = diabetes)\nsummary(model_complet)\n\n\nCall:\nglm(formula = Outcome ~ ., family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7823  -0.6603  -0.3642   0.6409   2.5612  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -1.004e+01  1.218e+00  -8.246  &lt; 2e-16 ***\nPregnancies               8.216e-02  5.543e-02   1.482  0.13825    \nGlucose                   3.827e-02  5.768e-03   6.635 3.24e-11 ***\nBloodPressure            -1.420e-03  1.183e-02  -0.120  0.90446    \nSkinThickness             1.122e-02  1.708e-02   0.657  0.51128    \nInsulin                  -8.253e-04  1.306e-03  -0.632  0.52757    \nBMI                       7.054e-02  2.734e-02   2.580  0.00989 ** \nDiabetesPedigreeFunction  1.141e+00  4.274e-01   2.669  0.00760 ** \nAge                       3.395e-02  1.838e-02   1.847  0.06474 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.02  on 383  degrees of freedom\nAIC: 362.02\n\nNumber of Fisher Scoring iterations: 5\n\n\nLe param√®tre family = binomial indique que nous utilisons une r√©gression logistique et que notre variable est binaire.\nNous nous concentrons sur la significativit√© statistique de chaque coefficient. En effet, la \\(p-valeur\\) nous indique la probabilit√© que l‚Äôeffet de la variable explicative soit d√ª au hasard. Plus sa valeur est faible, plus l‚Äôeffet de la variable explicative sur notre variable \\(Y\\) (Outcome) est statistiquement significatif. Au contraire, si elle est sup√©rieure √† 0.05, cela sugg√®re que l‚Äôeffet de la variable explicative sur notre variable \\(Y\\) n‚Äôest pas significatif et pourrait √™tre d√ª au hasard.\nIci, les variables explicatives avec des p-valeurs inf√©rieur √† 5 % sont :\n\nGlucose (\\(p-valeur\\) = \\(3,24 * 10^{-11}\\)) : L‚Äôeffet de la variable glucose sur la probabilit√© de la variable diab√®te est significatif.\nBMI (\\(p-valeur\\) = \\(0.00989\\)) : L‚Äôeffet de la variable BMI sur la probabilit√© de la variable diab√®te est significatif.\nDiabetesPedigreeFunction (\\(p-valeur\\) = \\(0.00760\\)) : L‚Äôeffet de la variable DiabetesPedigreeFunction sur la probabilit√© de la variable diab√®te est significatif.\n\nLes variables restantes ont des \\(p-valeur\\) &gt; \\(5 \\%\\), on en conclut donc qu‚Äôelles ne sont pas li√©es au risque de diab√®te.\n\n\nCalcul des odds-ratios et de leurs intervalles de confiance\nL‚Äôodds-ratio est une mesure statistique exprimant le degr√© de d√©pendance entre des variables al√©atoires qualitatives. Il permet de mesurer l‚Äôeffet d‚Äôun facteur en comparant les chances qu‚Äôun √©v√®nement se produise dans un groupe par rapport √† un autre groupe.\nIci, l‚Äôodds-ratio va repr√©senter l‚Äôimpact de nos variables explicatives sur les chances que notre variable \\(Y\\) (Outcome) prenne la valeur diab√©tique (1) ou non-diab√©tique.\nNous calculons ci-dessous les odds-ratios de toutes les variables, ainsi que leurs intervalles de confiance qui nous permetteront d‚Äô√™tre sur √† 95 % que l‚Äôodds-ratio se trouve entre les bornes inf√©rieurs et sup√©rieurs.\nAfin d‚Äôobtenir les odds-ratios √† partir des coefficients de r√©gression logistique, nous avons pris l‚Äôexponentielle des coefficients.\n\n# Permet la realisation de tableau\nlibrary(knitr)\n\nkable(exp(model_complet$coefficients[-1]), col.names = c(\"Variable\", \"Odd-ratios\"))\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.0856289\n\n\nGlucose\n1.0390112\n\n\nBloodPressure\n0.9985807\n\n\nSkinThickness\n1.0112846\n\n\nInsulin\n0.9991750\n\n\nBMI\n1.0730849\n\n\nDiabetesPedigreeFunction\n3.1296107\n\n\nAge\n1.0345346\n\n\n\n\nkable(exp(confint(model_complet)[-1, ]), col.names = c(\"Variable\", \"IC_inf\", \"IC_sup\"))\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9743237\n1.211631\n\n\nGlucose\n1.0277173\n1.051303\n\n\nBloodPressure\n0.9757909\n1.022307\n\n\nSkinThickness\n0.9778466\n1.045780\n\n\nInsulin\n0.9966180\n1.001767\n\n\nBMI\n1.0178269\n1.133537\n\n\nDiabetesPedigreeFunction\n1.3783799\n7.368273\n\n\nAge\n0.9985446\n1.073523\n\n\n\n\n\nNous nous int√©ressons uniquement aux variables ayant un effet significatif sur la probabilit√© de la variable Outcome. Ce sont les variables Glucose, BMI et DiabetesPedigreeFunction.\nOn note ici qu‚Äôune augmentation de 20 de BMI correspond √† \\(1.0730849^{20} = 4.09\\) plus de risque d‚Äôavoir du diab√®te que de ne pas en avoir. Une augmentation de 30 de glucose correspond √† \\(1.0390112^{30} = 3.15\\) plus de risque d‚Äôavoir du diab√®te que de ne pas en avoir. Enfin, une augmentation de 2 de DiabetesPedigreeFunction correspond √† \\(3.1296107^{2} = 9.79\\) plus de risque d‚Äôavoir du diab√®te que de ne pas en avoir.\nNous allons maintenant exhiber des profils d‚Äôindividus particuli√®rement √† risque d‚Äô√™tre diab√©tiques √† l‚Äôaide des coefficients de notre mod√®le de r√©gression logistique.\nPar exemple, un individu particuli√®rement √† risque d‚Äôavoir du diab√®te sera un individu ayant un taux de glucose √©gale √† 160, 40 de BMI et 1.5 de DiabetesPedigreeFunction. Son risque de diab√®te est alors √©gal √† environ 99.9 %.\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             60*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.9994916 \n\n\nCependant, si ce m√™me individu avait eu un BMI de 20 au lieu de 40, son risque de diab√®te aurait grandement diminu√© (71.4 %).\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             30*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.7137806 \n\n\n\n\nM√©thode de s√©lection de variable\nToutes les variables ne contribuent pas n√©cessairement √† expliquer notre variable Y, nous allons voir comment s√©lectionner certaines variables afin de simplifier notre mod√®le, mais aussi pour am√©liorer la classification.\nIci, nous utiliserons l‚ÄôAIC (Crit√®re d‚Äôinformation d‚ÄôAkaike) qui est une mesure de la qualit√© d‚Äôun mod√®le statistique. L‚ÄôAIC va nous permettre de comparer les diff√©rents mod√®les en utilisant un crit√®re de vraisemblance. Il repr√©sente un compromis entre le biais (qui diminue avec le nombre de param√®tres) et la parcimonie (n√©cessit√© de d√©crire les donn√©es avec le plus petit nombre de param√®tres possible).\n\\[AIC = -2 \\cdot \\log(L) + 2 \\cdot k\\]\n\n\\(L\\) est la vraisemblance maximis√©e\n\\(k\\) est le nombre de param√®tres dans le mod√®le\n\\(-2 \\cdot \\log(L)\\) est la d√©viance du mod√®le. Elle p√©nalise par 2 fois le nombre de param√®tres\n\nLa fonction step() de R, nous permet d‚Äôeffectuer une s√©lection descendante. On commence par le mod√®le complet incluant toutes les variables explicatives, puis on retire progressivement les variables qui contribuent le moins √† l‚Äôajustement du mod√®le, jusqu‚Äô√† obtenir un mod√®le optimal.\n\nmodel_final &lt;- step(model_complet)\n\nStart:  AIC=362.02\nOutcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + \n    Insulin + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- BloodPressure             1   344.04 360.04\n- Insulin                   1   344.42 360.42\n- SkinThickness             1   344.45 360.45\n&lt;none&gt;                          344.02 362.02\n- Pregnancies               1   346.24 362.24\n- Age                       1   347.55 363.55\n- BMI                       1   350.89 366.89\n- DiabetesPedigreeFunction  1   351.58 367.58\n- Glucose                   1   396.95 412.95\n\nStep:  AIC=360.04\nOutcome ~ Pregnancies + Glucose + SkinThickness + Insulin + BMI + \n    DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- Insulin                   1   344.42 358.42\n- SkinThickness             1   344.46 358.46\n&lt;none&gt;                          344.04 360.04\n- Pregnancies               1   346.24 360.24\n- Age                       1   347.60 361.60\n- BMI                       1   351.28 365.28\n- DiabetesPedigreeFunction  1   351.67 365.67\n- Glucose                   1   397.31 411.31\n\nStep:  AIC=358.42\nOutcome ~ Pregnancies + Glucose + SkinThickness + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- SkinThickness             1   344.89 356.89\n&lt;none&gt;                          344.42 358.42\n- Pregnancies               1   346.74 358.74\n- Age                       1   347.87 359.87\n- BMI                       1   351.32 363.32\n- DiabetesPedigreeFunction  1   351.90 363.90\n- Glucose                   1   411.11 423.11\n\nStep:  AIC=356.89\nOutcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n&lt;none&gt;                          344.89 356.89\n- Pregnancies               1   347.23 357.23\n- Age                       1   348.72 358.72\n- DiabetesPedigreeFunction  1   352.72 362.72\n- BMI                       1   360.44 370.44\n- Glucose                   1   411.85 421.85\n\n\nLe meilleur mod√®le est celui poss√©dant l‚ÄôAIC le plus faible.\nIci, le meilleur mod√®le inclut les variables Age, Pregnancies, BMI, DiabetesPedigreeFunction et Age.\nCi-dessous les r√©sultats de notre mod√®le.\n\nsummary(model_final)\n\n\nCall:\nglm(formula = Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age, family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8827  -0.6535  -0.3694   0.6521   2.5814  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -9.992080   1.086866  -9.193  &lt; 2e-16 ***\nPregnancies               0.083953   0.055031   1.526 0.127117    \nGlucose                   0.036458   0.004978   7.324 2.41e-13 ***\nBMI                       0.078139   0.020605   3.792 0.000149 ***\nDiabetesPedigreeFunction  1.150913   0.424242   2.713 0.006670 ** \nAge                       0.034360   0.017810   1.929 0.053692 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.89  on 386  degrees of freedom\nAIC: 356.89\n\nNumber of Fisher Scoring iterations: 5\n\n\nMis √† part la variable Pregnancies, tous ont des \\(p-valeur\\) significative. Elles ont toutes un impact positif sur la probabilit√© du patient √† avoir le diab√®te.\nMaintenant, nous calculons les odds-ratio de ce mod√®le.\n\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.087578\n\n\nGlucose\n1.037130\n\n\nBMI\n1.081273\n\n\nDiabetesPedigreeFunction\n3.161077\n\n\nAge\n1.034957\n\n\n\n\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9769517\n1.212971\n\n\nGlucose\n1.0274079\n1.047716\n\n\nBMI\n1.0395095\n1.127321\n\n\nDiabetesPedigreeFunction\n1.4016639\n7.398164\n\n\nAge\n0.9999761\n1.072664\n\n\n\n\n\n\n\nClassification avec le mod√®le de r√©gression logistique\nNous allons maintenant classifier nos donn√©es √† l‚Äôaide de notre mod√®le de r√©gression logistique.\nDans un premier temps, nous allons s√©parer notre jeu de donn√©es en deux sous-√©chantillons.\n\nEchantillon d‚Äôapprentissage : Ce dernier contient 80 % de notre jeu de donn√©es. Notre mod√®le va apprendre √† pr√©dire sur cet √©chantillon.\nEchantillon de test : Cet √©chantillon contient les 20 % restants. Il va nous servir √† tester notre mod√®le et √† comparer les r√©sultats avec les vraies valeurs de cet √©chantillon.\n\n\n# On fixe la graine (resultat reproductible)\nset.seed(75016)\n\n# Nombres d'observations dans notre jeu de donnees\nn &lt;- nrow(diabetes)\n\n# Nombres d'observations a selectionne dans l'echant d'apprentissage\nN &lt;- floor(n*0.8)\n\n# Selection des individus aleatoirement\nidx &lt;- sample(n, N, replace = F)\n\n# Echantillon d'apprentissage (80%)\ndataL &lt;- diabetes[idx,]\n\n# Echantillon de test (20%)\ndataV &lt;- diabetes[-c(idx),]\n\nOn entra√Æne notre mod√®le final (choisi pr√©c√©demment √† l‚Äôaide de l‚ÄôAIC) sur l‚Äô√©chantillon d‚Äôapprentissage et on utilise la fonction predict qui permet de pr√©dire pour tout individu de l‚Äô√©chantillon de test, sa probabilit√© d‚Äô√™tre diab√©tique.\n\n# Entrainement du meilleur modele sur echantillon d'apprentissage (DataL)\nmodel_final_train &lt;- glm(Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + Age , data = dataL, family = \"binomial\")\n\n# Prediction de la probabilite d'etre diabetique sur echantillon de test (DataV)\nprediction_modele &lt;- predict(model_final_train,dataV, type = \"response\")\n\nNous calculons ensuite le taux de mauvaise classification moyen. Ce dernier est tout simplement une comparaison des classifications √† leurs vraies √©tiquettes. On calcule ensuite le pourcentage de donn√©es mal classifi√©es.\nDans notre cas, nous allons cr√©er une r√®gle de classification. Si la pr√©diction est inf√©rieure √† 0.5 alors le patient prendre la valeur 0 (non-diab√©tique) dans le cas contraire, il prendra la valeur 1 (diab√©tique).\n\n# Regle de classification\nprediction_regle &lt;- ifelse(prediction_modele &gt;= 0.5, 1,0)\n\nerreur &lt;- mean(prediction_regle != dataV$Outcome)\npaste0(\"Le taux d'erreur moyen est de \", round(erreur,3)*100,\" %\")\n\n[1] \"Le taux d'erreur moyen est de 25.3 %\"\n\n\n\n\nComparaison de la r√©gression logistique avec l‚Äôalgorithme des k-plus proches voisins\nPr√©c√©demment, nous avons r√©alis√© une classification sur ce m√™me jeu de donn√©es en utilisant l‚Äôalgorithme des \\(k\\) plus proches voisins. La valeur de \\(k\\) optimal √©tait \\(k\\) = 19, c‚Äôest-√†-dire la valeur de \\(k\\) avec laquelle le taux de mauvaise classification √©tait le plus faible.\nNous mettons donc en place notre algorithme des \\(k\\) plus proches voisins avec \\(k\\) = 19, et on r√©alise nos pr√©dictions √† l‚Äôaide de la fonction knn() du package class.\n\nprediction_knn &lt;- knn(train = dataL[,-9], test = dataV[,-9], cl = dataL[,9], k = 19, prob = F)\n# resultat des k plus proches voisins\nprediction_knn\n\n [1] 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0\n[77] 0 0 0\nLevels: 0 1\n\n\nOn calcule ensuite notre taux d‚Äôerreur moyen.\n\nerreur_knn &lt;- mean(prediction_knn != dataV[,9])\npaste0(\"Le taux d'erreur moyen est de \", round(erreur_knn,3)*100, \" %\")\n\n[1] \"Le taux d'erreur moyen est de 20.3 %\"\n\n\nMaintenant, que nous avons r√©alis√© nos pr√©dictions avec nos deux m√©thodes, nous sommes en mesure de comparer les deux taux d‚Äôerreur moyens. Le taux d‚Äôerreur le plus bas est √©videmment pr√©f√©rable. Ici, les taux d‚Äôerreur moyens sont proches bien que celui de la r√©gression logistique est plus √©lev√©e (25.3 %) que celui de l‚Äôalgorithme des \\(k\\) plus proches voisins (20.3 %).\n\n\nConclusion\nEnfin, nous allons faire varier le seuil utilis√© dans le crit√®re de classification et calculer la sensibilit√© et la sp√©cificit√© pour chacune des valeurs du seuil possible.\n\n# Evaluation de 100 seuils differents\nl &lt;- 100\n\n# Predicition\npreds &lt;- predict(model_final_train, dataV, type = \"response\")\n\n# Sequence de seuils\nc &lt;- seq(1.001 * min(preds), 0.999 * max(preds), length.out = l)\n\n# Initialisation des vecteurs sensibilite et specificite\nSe &lt;- rep(NA,l)\nSp &lt;- rep(NA,l)\n\nfor (j in 1:l) {\n  mod.final.classif &lt;- (preds &gt;= c[j])  # Classe (TRUE ou FALSE) de la pr√©diction\n  pt &lt;- table(mod.final.classif, dataV$Outcome)  # Comparaison des classes pr√©dites\n  \n  if (nrow(pt) &gt;= 2 && ncol(pt) &gt;= 2) {\n\n    Se[j] &lt;- prop.table(pt, margin=2)[2, 2]  # Sensibilit√©\n    Sp[j] &lt;- prop.table(pt, margin=2)[1, 1]  # Sp√©cificit√©\n  } else {\n\n    Se[j] &lt;- NA\n    Sp[j] &lt;- NA\n  }\n}\n\npar(mfrow=c(1,2))\nplot(c,Se,main=\"Sensibilit√©\",type='s', col = \"orange\")\nplot(c,Sp,main=\"Specificit√©\",type='s', col = \"purple\")\n\n\n\n\n\n\n\n\nSur les deux graphiques ci-dessus, la courbe de sensibilit√© montre comment le taux de d√©tection des diab√©tiques varie en fonction des diff√©rents seuils de classification. Tandis que la courbe de sp√©cificit√© montre comment le taux de d√©tection des non-diab√©tiques varie √©galement en fonction des diff√©rents seuils de classification.\nOn cherche √† maximiser la sensibilit√© et la sp√©cificit√©. Cependant une augmentation de la sensibilit√© peut entra√Æner une diminution de la sp√©cificit√© et vice versa.\nEn somme, ces deux mesures permettent de d√©terminer √† quel point le mod√®le est efficace dans la classification des cas positifs et n√©gatifs, et d‚Äôajuster les seuils de d√©cision en fonction des besoins sp√©cifiques.\nPuis, nous pouvons calculer la courbe ROC. Elle sert √† √©valuer la performance d‚Äôun mod√®le de classification binaire et en particulier les mod√®les qui pr√©disent une probabilit√©. Elle repr√©sente la sensibilit√© en fonction de 1 ‚Äì sp√©cificit√© pour toutes les valeurs seuils possibles du marqueur √©tudi√©.\n\npar(mfrow=c(1,1))\nplot(1-Sp,Se,type='s',main=\"Courbe ROC\")\nabline(0,1,col='blue')\n\n\n\n\n\n\n\n\nNotre classification est plut√¥t bonne au vu de la courbe ROC. La courbe est assez proche du coin sup√©rieur gauche du graphique, ce qui indique un taux √©lev√© de vrais positifs (sensibilit√©). Notre mod√®le arrive assez bien √† identifier les personnes diab√©tiques en minimisant les erreurs de classification des cas n√©gatifs. De plus, la courbe est largement sup√©rieure √† la courbe 0,1."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "√Ä propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des donn√©es | Sept 2022 - Juin 2025\nCours suivis : Alg√®bre, Statistique, Probabilit√©s, Programmation, Base de donn√©es, √âconomie"
  },
  {
    "objectID": "about.html#formation",
    "href": "about.html#formation",
    "title": "√Ä propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des donn√©es | Sept 2022 - Juin 2025\nCours suivis : Alg√®bre, Statistique, Probabilit√©s, Programmation, Base de donn√©es, √âconomie"
  },
  {
    "objectID": "about.html#exp√©rience",
    "href": "about.html#exp√©rience",
    "title": "√Ä propos",
    "section": "Exp√©rience",
    "text": "Exp√©rience\n\n\n\n\n\n\nInstitut national de la statistique et des √©tudes √©conomiques (INSEE) | Montrouge, France\n\n\n\nProgrammeur Statistique | 2023 - 2025 Au sein de la Direction des statistiques d√©mographiques et sociales, j‚Äôai men√© des travaux d‚Äôappariement de donn√©es administratives. L‚Äôobjectif de ces travaux √©tait de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j‚Äôai compar√© diff√©rents algorithmes d‚Äôappariement √† l‚Äôaide d‚Äôanalyses statistiques.\nMes missions ont inclus la manipulation de donn√©es brutes, le nettoyage des donn√©es, l‚Äôappariement de grands volumes de donn√©es, ainsi que leur analyse statistique."
  },
  {
    "objectID": "about.html#comp√©tences",
    "href": "about.html#comp√©tences",
    "title": "√Ä propos",
    "section": "Comp√©tences",
    "text": "Comp√©tences\n\n\n\n\n\n\nProgrammation\n\n\n\nPython | R | SQL | SAS | GIT\n\n\n\n\n\n\n\n\nLangue\n\n\n\nAnglais (B2) | Espagnol (B2) | Arabe (C2)"
  }
]