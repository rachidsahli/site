[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rachid Sahli",
    "section": "",
    "text": "Je suis étudiant en Science des Données à l’IUT Paris Rives de Seine et alternant à l’INSEE. Au sein de la Direction des Statistiques Démographiques et Sociales, je travaille sur des problématiques liées à la mesure de la couverture des bases de sondage.\nJe travaille principalement avec R et Python et j’ai créé un site de ressources appelé PratiqueR. Vous pouvez également me retrouver sur YouTube, où je partage des vidéos sur R et son environnement.\nEn-dehors de mes études, je consacre mon temps libre à la construction de robots et je partage mes idées sur mon blog, Jiqiren.\nPassionné par les statistiques, la robotique 🦿🤖, le vélo 🚲, le cinéma 🎞️, et la programmation 👾, j’aime explorer et partager mes découvertes dans ces domaines.\nJe suis très curieux et toujours ouvert à de nouvelles idées. N’hésitez pas à me contacter pour discuter d’un projet ou explorer des opportunités de collaboration."
  },
  {
    "objectID": "projets/fontaines_eau.html",
    "href": "projets/fontaines_eau.html",
    "title": "Où boire de l’eau à Paris ?",
    "section": "",
    "text": "Ce mini-projet utilise le langage R pour créer une carte interactive des points d’eau publics dans la ville de Paris. Il inclut également une application Shiny, permettant une exploration dynamique et interactive des données."
  },
  {
    "objectID": "projets/fontaines_eau.html#aperçu-de-lapplication",
    "href": "projets/fontaines_eau.html#aperçu-de-lapplication",
    "title": "Où boire de l’eau à Paris ?",
    "section": "1 Aperçu de l’application",
    "text": "1 Aperçu de l’application\n\n\n\n\n\nL’objectif principal de ce projet est de proposer une visualisation géographique claire et accessible des points d’eau répartis dans Paris, grâce à une interface conviviale développée avec Shiny. Les données exploitées proviennent de la Ville de Paris et sont accessibles sur le site data.gouv. Cette application, à la fois fluide et interactive, permet aux utilisateurs d’explorer facilement les différents points d’eau présents dans chaque arrondissement de la capitale."
  },
  {
    "objectID": "projets/fontaines_eau.html#accédez-à-lapplication",
    "href": "projets/fontaines_eau.html#accédez-à-lapplication",
    "title": "Où boire de l’eau à Paris ?",
    "section": "2 Accédez à l’application",
    "text": "2 Accédez à l’application\nAccéder à l’application ici\nRepo github"
  },
  {
    "objectID": "projets/projets_but/louvre/louvre.html",
    "href": "projets/projets_but/louvre/louvre.html",
    "title": "Nocturne au musée du Louvre",
    "section": "",
    "text": "Tous les vendredis, le musée du Louvre offre un moment de magie à ses visiteurs au milieu de ses collections. Au programme de ces nocturnes hebdomadaires, de nombreuses activités pour petits et grands. Le 24 novembre 2023, mes camarades de l’IUT et moi avons pu participer à l’une de ces soirées. Nous avons décidé de présenter certains tableaux du célèbre musée au filtre de l’IA. Au moment où cette technologie prend une place active dans notre société, avec par exemple ChatGPT, DALL-E ou encore Mistral AI, il est intéressant de pouvoir approfondir l’utilisation de ces outils permettant de générer des éléments en lien, ici, avec l’art, par exemple. De plus, ce sujet fascinant qu’est l’intelligence artificielle est plus ou moins en lien avec notre formation en science des données. Explorer ce domaine où se mêlent mathématiques, informatique et données était particulièrement captivant.\n\nQu’avons nous fait ?\nL’objectif final était de pouvoir le 23 novembre 2024, présenter une œuvre en rapport avec l’IA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons décidé de choisir un sujet d’intelligence artificielle qui pouvait coïncider avec une œuvre du musée. Après de longues recherches passionnantes, nous avons choisi de présenter les GAN (Generative adversarial networks). C’est une classe d’algorithmes d’apprentissage non supervisé. Ils permettent de générer des images avec un fort degré de réalisme. Nous avons trouvé la technologie et les méthodes très intéressantes, d’autant plus qu’elles aboutissent à des applications concrètes, telles que la génération d’images artistiques. Cependant, ces algorithmes sont utilisés dans bien d’autres domaines tels que la médecine ou encore la finance. Mais concrètement, comment ça marche ?\n\n\nQu’est-ce qu’un GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL’artiste (Générateur) : L’objectif du joueur est de créer des images qui se rapprochent le plus de la réalité. Pour cela, il apprend de ses erreurs et réessaie en continu d’obtenir l’œuvre la plus proche du réel.\nLe juge (Discriminateur) : L’objectif de ce joueur est de vérifier si les œuvres réalisées par l’artiste peuvent paraître réelles ou si elles sont encore trop fausses. Pour cela, il compare les œuvres de l’artiste à des œuvres réelles faites par des peintres.\n\nTant que l’image n’est pas accepté par le juge, l’artiste continue à produire des œuvres. Voilà, le fonctionnement d’un GAN ou deux réseaux de neurones se font concurrence. De cette manière, il est possible de créer des images, des vidéos ou d’autres contenues de très bonnes qualités.\n\n\n\n\n\n\n\nSchéma GAN\n\n\n\n\nPrésentation au public\nNous avons décidé de présenter au public le travail d’Obvious. Ce collectif de chercheurs, d’artistes travaille avec des modèles d’apprentissage profond pour explorer le potentiel créatif de l’intelligence artificielle. Ils ont justement utilisé des GAN pour générer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d’Edmond de Belamy. Ce tableau est une impression sur toile qui est rentrée dans l’histoire de l’art moderne. Cela, car c’est la première œuvre d’art produite par un logiciel d’intelligence artificielle à être présentée dans une salle des ventes. Pour couronner le tout, il a été vendu 432 500 dollars chez Christie’s le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est très difficile à première vue de déterminer qu’une machine a pu en être l’auteur.   Obvious à utiliser un GAN, en l’entraînant sur 15 000 portraits classiques réalisés entre le 14e et 20e siècle. L’algorithme devait donc produire un tableau en sortie qui serait très ressemblant aux portraits classiques. Nous avons décidé de comparer le portrait d’Edmond de Belamy à une œuvre du Louvre se trouvant dans la salle 846 de l’aile Richelieu du musée. C’est une peinture datant du 17e siècle réalisée par Jean Bray, peintre néerlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\nPortrait d’Edmond de Belamy, Collectif Obvious\n\n\n\n\n\n\n\n\n\n\n\nPortrait d’homme, 1658\n\n\n\n\n\nLes visiteurs étaient agréablement surpris par le réalisme du portrait d’Edmond de Belamy, mais aussi par le prix de vente de l’œuvre. Ils pensaient pouvoir reconnaître la réalisation d’une IA."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "",
    "text": "L’objectif est d’analyser et de modéliser statistiquement une série temporelle liée à la production d’électricité par combustion aux États-Unis entre 2001 et 2022. Nous étudions l’évolution de cette série au fil du temps afin d’identifier ses principales tendances et composantes. Cette série est issue du site web de l’Administration américaine de l’information sur l’énergie. Nous réalisons ensuite une application Shiny afin de visualiser les résultats interactivement.\nLe dépot github du projet contenant le code est disponible ici."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#tendance-de-la-série",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#tendance-de-la-série",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "Tendance de la série",
    "text": "Tendance de la série\nAfin de modéliser la série, nous commençons par analyser la tendance. Nous utilisons un filtre de moyennes mobiles simples et centrées, ainsi qu’une régression des moyennes annuelles, dans le but de visualiser l’évolution générale de la série.\nLes moyennes mobiles permettent de lisser la série et d’atténuer les fluctuations aléatoires. La moyenne mobile simple est calculée sur une fenêtre fixe, tandis que la moyenne mobile centrée est déterminée de manière symétrique autour de chaque point. Cela permet d’identifier la tendance sous-jacente de la série entre 2001 et 2022, qui s’est avérée être décroissante. On se sert également de la moyenne mobile pour éléminer la composante saisonnière de période \\(p\\) de notre série et réduire au maximum l’amplitude des fluctuations irrégulières.\nLa moyenne mobile qui s’ajuste le mieux à la série est celle d’ordre 12.\nDe plus, la courbe de régression des moyennes annuelles permet de modéliser la tendance à long terme de la série en s’appuyant sur les moyennes calculées pour chaque année. On trace sur notre nuage de point une courbe linéaire qui nous indique une tendance décroissante de la production d’éléctricité par combustion."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#décomposition-de-la-série",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#décomposition-de-la-série",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "Décomposition de la série",
    "text": "Décomposition de la série\nÀ présent, nous procédons à la décomposition de notre série à l’aide d’un modèle additif. Il s’écrit de la manière suivante :\n\\[ y_i = f_i + s_i + e_i \\quad \\text{pour } i = 1, \\dots, n \\quad \\text{avec }  \\sum_{j=1}^{p} s_j = 0 \\quad \\text{et} \\quad \\sum_{j=1}^{n} e_j = 0 \\]\nDans ce modèle, l’amplitude de la composante saisonnière et du bruit reste constante au cours du temps. Cela se traduit graphiquement par des fluctuations autour de la tendance d’amplitude constante. L’utilisation de la méthode de la bande, qui consiste à tracer la courbe reliant les minima sur une période ainsi que celle reliant les maxima, a montré que ces deux courbes sont parallèles, indiquant ainsi que le modèle est additif.\nOn rappelle qu’une série chronologique résulte de trois composantes fondamentales : la tendance (\\(f_i\\)), la composante saisonnière (\\(s_i\\)) ou saisonnalité, et la composante résiduelle (\\(e_i\\)).\nNous analysons d’abord l’effet des saisons en calculant les coefficients de variation saisonnière (CVS). Ces variations, qui reviennent chaque année à la même période, sont influencées par des phénomènes naturels ou économiques (comme la météo ou les fêtes) mais n’ont pas d’impact durable sur la tendance.\nEnfin, nous nous intéressons à la composante résiduelle, qui regroupe toutes les variations non expliquées par la tendance et la saisonnalité. Ces fluctuations peuvent être dues à des événements imprévus (crises économiques, accidents, conditions exceptionnelles) ou à des variations aléatoires.\n\n\n\nDécomposition de la série"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#prévision-de-la-série",
    "href": "projets/projets_but/serie_temp_charbon/serie_temp_charbon.html#prévision-de-la-série",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "Prévision de la série",
    "text": "Prévision de la série\nL’objectif de cette étape est de réaliser deux prévisions (2022 et 2023) de la production d’électricité par combustion en utilisant trois méthodes différentes : Trend + Saison, ARMA et Holt-Winters.\nLa méthode Trend + Saison, identifie deux composantes principales dans notre série qui sont la tendance et la saisonnalité. La méthode ARMA repose sur l’idée que la production d’électricité à un instant donné dépend des valeurs passées et des erreurs passées. Pour garantir des prévisions optimales, nous avons sélectionné les paramètres du modèle ARMA en fonction des performances statistiques obtenues. Enfin, le modèle Holt-Winters est une extension de lissage exponentiel qui prend en compte trois éléments, la tendance, la saisonnalité et la moyenne ajustée de la série.\nLa prévision sur les données de 2022 a permis de comparer les valeurs estimées avec les valeurs réelles de la série, afin d’évaluer la précision des trois méthodes utilisées. Pour cela, nous avons calculé l’erreur quadratique moyenne (RMSE) pour chacune des méthodes, ce qui nous a permis d’identifier celle offrant les prévisions les plus fiables.\nParmi les trois approches testées, la méthode Trend + Saison s’est révélée être la plus performante, car elle a obtenu l’erreur quadratique moyenne la plus faible. Cela s’explique par sa capacité à capturer à la fois la tendance de long terme et les variations saisonnières, qui jouent un rôle clé dans l’évolution de la production d’électricité. En revanche, les modèles ARMA et Holt-Winters ont montré des écarts plus importants avec les valeurs réelles, probablement en raison de leurs hypothèses sous-jacentes qui s’adaptent moins bien aux dynamiques spécifiques de notre série temporelle."
  },
  {
    "objectID": "blog/carte_volontaires/carte_volontaires.html",
    "href": "blog/carte_volontaires/carte_volontaires.html",
    "title": "Carte d’aide pour les Volontaires de Paris 2024",
    "section": "",
    "text": "Lors des Jeux Olympiques de 2024 à Paris, plus de 45 000 volontaires ont été les véritables hommes et femmes de l’ombre. On les a vus partout à Paris grâce à leur tenue bleue. Ils ont contribué au succès des Jeux Olympiques. Venant des quatre coins de la France, ils devaient pouvoir se repérer. La carte d’aide pour les volontaires est conçue à cet effet et leur a été très utile durant leur séjour à Paris.\n\n\n\n\n\n\n Cette carte a été créée à l’aide du package leaflet, en utilisant les données disponibles en open data fournies par les acteurs suivants :\n\nFontaines à eau : opendata de la Ville de Paris\nToilettes publiques : opendata de la Ville de Paris\nDistributeurs automatiques de billets : Opendatasoft hub\nParkings vélo : opendata de Paris 2024\nSite de compétitions des Jeux Olympiques\nParalympiques : opendata de Paris 2024\n\nAu moment de la réalisation de la carte, la dernière mise à jour des données datait du 27 juillet 2024."
  },
  {
    "objectID": "projets.html",
    "href": "projets.html",
    "title": "Projets",
    "section": "",
    "text": "Projets BUT SD\n\n\n\nScience des données\n\n\nStatistique\n\n\n\nVoici une liste de mes projets réalisées durant mes 3 années à l’Institut Universitaire de Technologie de Paris.\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "blog/knn/knn.html",
    "href": "blog/knn/knn.html",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "",
    "text": "Introduction\nL’algorithme des \\(k\\) plus proches voisins est une méthode d’apprentissage supervisé. Il peut être utilisé pour la classification lorsque la variable à expliquer (\\(Y\\)) est qualitative, mais aussi pour effectuer une régression lorsque \\(Y \\in \\mathbb{R}\\).\n\nEn apprentissage supervisé, une variable \\(Y\\) est étudiée à partir de variables explicatives \\(X\\) à des fins de description ou de prédiction. En ce qui concerne la prédiction, l’objectif est de prévoir l’étiquette (classification) ou la valeur (régression) de \\(Y\\) associée à une nouvelle entrée \\(x\\). En apprentissage non supervisé, le problème est beaucoup moins bien posé. Il s’agit de découvrir des structures intéressantes dans des données non étiquetées, notamment à travers l’analyse exploratoire multidimensionnelle et la classification non supervisé\nIci, nous sommes face à un problème d’apprentissage supervisé : nous disposons d’un jeu de données constitué de \\(N\\) lignes représentant chacune un “individu”. Pour chaque individu, on dispose de \\(n\\) caractéristiques (les entrées) et d’une donnée représentant l’étiquette (ou la classe) à laquelle ce dernier appartient. Chaque ligne est donc constituée de \\(n+1\\) données. Notre objectif est de construire un modèle prédictif prenant en entrée \\(n\\) valeurs correspondant aux caractéristiques d’un “individu” et donnant en sortie la classe à laquelle il appartient.\n\n\nMéthode des \\(k\\) plus proches voisins\nPour estimer la sortie (étiquette ou valeur) associée à \\(n\\) entrées \\((x_1, ..., x_n)\\), la méthode des \\(k\\) plus proches voisins consiste à déterminer les \\(k\\) lignes du jeu de données dont les \\(n\\) entrées sont les plus proches des valeurs \\((x_1, ..., x_n)\\) à travers le calcul d’une distance.\n\n\n\n\n\n\n\n\n\nSource : Cornell Computer Science\n\n\n\n\nEnsuite, l’algorithme regarde les \\(k\\) voisins les plus proches et détermine leur sortie. En classification, il attribue à l’individu la classe la plus fréquente parmi ces \\(k\\) voisins (on parle de vote majoritaire). En régression, il calcule simplement la moyenne des valeurs de sortie.\n\n\n\nIl existe différents types de distances pouvant être utilisés pour l’algorithme des \\(k\\) plus proches voisins.\n\nNous utiliserons la distance Euclidienne. C’est tout simplement la racine carrée de la somme des carrés des différences entre chaque coordonnée des deux points. Elle est donnée par la formule ci-dessous et représente la distance la plus courte entre deux points. Elle est également connue sous le nom de norme L2 d’un vecteur.\n\\[d(x, y) = \\sqrt{ \\sum_{i=1}^{n} (x_i - y_i)^2 }\\]\nNous appliquerons la méthode des \\(k\\) plus proches voisins à un cas de classification et à un cas de régression, en nous appuyant sur deux jeux de données distincts.\n\n\nClassification\nNous commençons par importer le package class. Ce dernier ne contient que des fonctions pour l’algorithme des \\(k\\) plus proches voisins et nous sera utile pour le cas de classification.\n---\nlibrary(class)\n---\nLe jeu de données Iris, également connu sous le nom d’Iris de Fisher, contient 150 observations de trois espèces d’iris : setosa, virginica et versicolor. Pour chaque fleur, quatre caractéristiques mesurées en centimètres sont renseignées : la longueur et la largeur des sépales, ainsi que la longueur et la largeur des pétales.\nCe jeu de données est initialement intégré à R. Nous l’importons à l’aide de la commande suivante :\n---\ndata(iris)\n---\nNous disposons donc d’un jeu de données avec 4 variables explicatives, qui sont les caractéristiques de chaque fleur, et une variable à prédire, qui est l’espèce. On observe ci-dessous que la variable Species comporte trois modalités. Afin d’éviter un problème de classification multiclasse, nous choisissons d’exclure la modalité Setosa. L’objectif est de construire un modèle qui se résume à une règle de classification binaire.\n---\nstr(iris)\n---\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nOn supprime les 50 observations associées à l’espèce Setosa, puis la modalité correspondante de la variable.\n---\niris &lt;- iris[!iris$Species == \"setosa\",]\n\niris$Species &lt;- droplevels(iris$Species)\n---\nNotre jeu de données compte désormais 50 observations.\n---\ndim(iris)\n---\n\n\n[1] 100   5\n\n\nIris est un jeu de données souvent utilisé à des fins pédagogiques, car il est déjà propre, équilibré et bien structuré. Il ne comporte pas de valeurs manquantes, les variables sont déjà au format numérique, leurs échelles sont relativement comparables, et la répartition des classes est équilibrée. Nous ne réaliserons donc pas d’analyses descriptives approfondies, mis à part le résumé statistique de notre data frame présenté ci-dessous.\n---\nsummary(iris)\n---\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.900   Min.   :2.000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:5.800   1st Qu.:2.700   1st Qu.:4.375   1st Qu.:1.300  \n Median :6.300   Median :2.900   Median :4.900   Median :1.600  \n Mean   :6.262   Mean   :2.872   Mean   :4.906   Mean   :1.676  \n 3rd Qu.:6.700   3rd Qu.:3.025   3rd Qu.:5.525   3rd Qu.:2.000  \n Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n       Species  \n versicolor:50  \n virginica :50  \n                \n                \n                \n                \n\n\n\n\n\n\n\n\n\n\n\nAfin de pouvoir évaluer notre modèle, nous divisons notre jeu de données en deux sous-ensembles : Un jeu de données d’apprentissage réprésentant 80 % du jeu de données initial. L’algorithme s’entraînera à partir de ces données. Puis un jeu de données de test correspondant aux 20 % restants. Il servira à évaluer les performances du modèle de classification sur des données jamais vues pendant l’apprentissage.\n\n\n\n---\nN &lt;- 80\nidx1 &lt;- sample(1:50, N/2, replace = F)\nidx1 &lt;- sample(1:50, N/2, replace = F) # Tirage aleatoire de 40 indices entre 1 et 50 \n                                       # (classe : versicolor)\nidx0 &lt;- sample(51:100, N/2, replace = F) # Tirage aleatoire de 40 indices entre 51 et 100 \n                                         # (classe : virginica)\ndataL &lt;- iris[c(idx1,idx0),] # Apprentissage (80 %)\ndataV &lt;- iris[-c(idx1,idx0),] # Test (20 %)\n---\nNous pouvons maintenant créer notre fonction de prédiction et l’appliquer à nos données à l’aide de la fonction knn(). Cette fonction prend cinq paramètres principaux :\n\ntrain : le jeu de données d’apprentissage,\ntest : le jeu de données sur lequel on souhaite faire des prédictions,\ncl : le vecteur des classes associées aux données d’apprentissage \\((Y)\\),\n\\(k\\) : le nombre de voisins à considérer pour la classification,\nprob : un paramètre optionnel qui, s’il est défini à TRUE, renvoie également la probabilité associée à la prédiction.\n\n---\nprediction = knn(train = dataL[,-5], test = dataV[,-5],\n                 cl = dataL[,5], k = 3)\n---\nIci, nous avons choisi de considérer les 3 plus proches voisins pour prédire la classe de la prochaine observation. On voit sur l’image ci-dessous que la valeur de \\(k\\) influence fortement la performance du modèle et la qualité des prédictions.\n Nous verrons par la suite comment déterminer la valeur optimale de \\(k\\) afin de minimiser le taux de mauvaise classification.\nNous calculons l’erreur de notre classification, c’est-à-dire le taux de mauvaise classification sur l’échantillon test. Il s’agit simplement de comparer les prédictions aux vraies valeurs, puis de calculer la proportion d’observations mal classées.\n---\nmean(prediction != dataV[,5])\n---\n\n\n[1] 0.15\n\n\nLe taux d’erreur de notre classification est de 15 %. Cependant, ce taux peut varier à chaque répétition de l’expérience, car la répartition des données entre l’échantillon d’apprentissage et celui de test change à chaque tirage aléatoire. Nous décidons donc de répéter l’expérience 100 fois, en utilisant des échantillons différents à chaque itération. À l’issue de ces répétitions, nous calculerons la moyenne des taux de mauvaise classification obtenus afin d’obtenir une estimation plus fiable de la performance du modèle.\n---\nrep &lt;- 100 # Nb de répétitions\nerror &lt;- rep(NA, rep)\n\nfor (i in 1:rep){\n  N &lt;- 80\n  idx1 &lt;- sample(1:50, N/2, replace = F)\n  idx0 &lt;- sample(51:100, N/2, replace = F)\n  dataL &lt;- iris[c(idx1,idx0),]\n  dataV &lt;- iris[-c(idx1,idx0),]\n  \n  prediction_100 &lt;- knn(train = dataL[,-5], test = dataV[,-5],\n                 cl = dataL[,5], k = 3)\n  \n  error[i] &lt;- c(mean(prediction_100 != dataV[,5]))\n}\n\nmean(error)\n---\n\n\n[1] 0.053\n\n\nLe taux d’erreur de notre classification est de 5,3 % lorsque nous considérons les 3 plus proches voisins. Il existe de nombreux types de visualisations sur R, tant pour les prédictions que pour les erreurs, que nous ne détaillerons pas ici.\nNous souhaitons maintenant déterminer le nombre de voisins qui permet la meilleure classification de la nouvelle observation. Pour ce faire, nous allons chercher à identifier la valeur optimale de \\(k\\), ce qui permettra de réduire le taux d’erreur du modèle précédent. Afin de gagner du temps, nous calculerons le taux d’erreur moyen pour chaque valeur de \\(k\\) à partir de 100 échantillons différents.\n---\nrep &lt;- 100 # Nb de répétitions\nkval &lt;- seq(1, 79, by = 2)\nerror &lt;- matrix(NA, rep, length(kval))\n\nfor (i in 1:rep){\n  N &lt;- 80\n  idx1 &lt;- sample(1:50, N/2, replace = F)\n  idx0 &lt;- sample(51:100, N/2, replace = F)\n  dataL &lt;- iris[c(idx1,idx0),]\n  dataV &lt;- iris[-c(idx1,idx0),]\n  for (j in 1:(length(kval)))\n  {\n    prediction_2 &lt;- knn(train = dataL[,-5], test = dataV[,-5], \n                    cl = dataL[,5], k = kval[j])\n    error[i,j] &lt;- mean(dataV[,5] != prediction_2)\n  }\n}\n---\nOn visualise les résultats sur le graphique ci-dessous.\n\n\n\n\n\n\n\n\n\nIci, il est nécessaire de considérer 11 voisins pour obtenir le taux de classification le plus faible. Nous réentraînerons donc le modèle en utilisant ce paramètre modifié.\n---\nrep &lt;- 100 # Nb de répétitions\nerror &lt;- rep(NA, rep)\n\nfor (i in 1:rep){\n  N &lt;- 80\n  idx1 &lt;- sample(1:50, N/2, replace = F)\n  idx0 &lt;- sample(51:100, N/2, replace = F)\n  dataL &lt;- iris[c(idx1,idx0),]\n  dataV &lt;- iris[-c(idx1,idx0),]\n  \n  prediction_11 &lt;- knn(train = dataL[,-5], test = dataV[,-5],\n                 cl = dataL[,5], k = 11)\n  \n  error[i] &lt;- c(mean(prediction_11 != dataV[,5]))\n}\n\nmean(error)\n---\n\n\n[1] 0.039\n\n\nNous obtenons un taux d’erreur plus faible (3,9 %) par rapport au modèle précédent, ce qui confirme notre petite recherche du \\(k\\) optimal.\n\n\nRégression"
  },
  {
    "objectID": "projets/but_sd.html",
    "href": "projets/but_sd.html",
    "title": "Projets BUT SD",
    "section": "",
    "text": "Aucun article correspondant"
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html",
    "href": "projets/projets_but/mirgation/migration.html",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "",
    "text": "Ce projet vise à migrer des données d’un environnement SQL vers un environnement NoSQL. Concrètement, il s’agit de transférer les informations stockées dans une base de données relationnelle traditionnelle, où les données sont organisées en tables avec des relations fixes, vers une base de données NoSQL, qui offre une structure plus flexible adaptée aux données non structurées ou semi-structurées.\nNous travaillons avec les données d’une entreprise de voitures qui rencontre des problèmes avec sa base de données actuelle : les requêtes sont lentes et des défaillances serveur entraînent des pertes de données. Pour résoudre ces problèmes, nous avons décidé de passer à un environnement NoSQL. Cette technologie permet de stocker des données sous une forme non structurée, offrant ainsi plus de flexibilité et de performance. Cette migration vise à améliorer la performance des requêtes et à préparer l’infrastructure pour une croissance future.\nLe dépôt GitHub contenant le rapport complet du projet, ainsi que les requêtes SQL et NoSQL associées, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html#base-de-données-relationnelle",
    "href": "projets/projets_but/mirgation/migration.html#base-de-données-relationnelle",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Base de données relationnelle",
    "text": "Base de données relationnelle\nLa base de données relationnelle initiale contient des informations sur les véhicules, les clients, les commandes, les employés… La représentation des données est claire et bien organisée. Chaque table dispose de relations avec d’autres tables, ce qui permet de structurer efficacement les informations et de faciliter les requêtes complexes.\n\n\n\nSchéma relationnel de la bdd initial\n\n\nDans un premier temps, nous avons créé des requêtes SQL sur cette base de données. Ces requêtes serviront de tests pour évaluer le succès de la migration. Nous comparerons les résultats obtenus dans la base de données relationnelle avec ceux obtenus dans la base NoSQL pour vérifier l’intégrité et la performance de la migration.\n\n\n\nRequêtes SQL\n\n\nLa base de données est au format SQLite. Nous avons importé le module sqlite3 en Python pour établir la connexion et interagir avec la base. Ensuite, nous avons utilisé la bibliothèque Pandas et notamment sa fonction read_sql_query(), pour exécuter et lire les résultats des requêtes SQL."
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html#algorithme-de-migration",
    "href": "projets/projets_but/mirgation/migration.html#algorithme-de-migration",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Algorithme de migration",
    "text": "Algorithme de migration\nIl existe plusieurs types de bases de données NoSQL (Clé-valeur, Document, Colonne et Graphe), chacun adapté à ses propres cas d’usage et ayant ses propres avantages et inconvénients, notamment en termes de scalabilité et de flexibilité. Le choix dépend donc de plusieurs facteurs clés, comme la structure des données, les exigences de performance…\nEn ce qui nous concerne, nous pouvons réaliser une migration vers un environnement NoSQL, car l’entreprise dispose d’une grande quantité de données structurées en constante croissance. De plus, il est possible d’améliorer significativement les performances d’accès aux données en optimisant le traitement de données plus importantes et en réduisant le temps de latence. On souhaite donc une solution qui offre plus de flexibilité et évolutivité, tout en préservant l’intégrité des données de notre base initial.\nAprès mûre réflexion, le format document est celui s’adaptant le mieux à notre objectif. En effet, il permet de structurer naturellement les entités de manière hiérarchique. Par exemple, un client peut être représenté par un document contenant ses commandes, chaque commande incluant les produits associés. De plus, il offre une grande flexibilité, permettant de traiter différents types de données sans modifications complexes du schéma. Enfin, il permet une scalabilité horizontale grâce à la partition de document, c-à-d si les données augmentent, on peut facilement ajouter de nouveaux serveurs pour stocker et gérer plus de documents, sans tout restructurer.\n\n\n\nExemple d’une modélisation au format Document\n\n\nCe modèle présente quelques inconvénients, notamment des performances limitées pour les requêtes complexes ou les jointures entre documents. De plus, les mises à jour simultanées de documents imbriqués peuvent être plus difficiles à gérer.\nNous avons décidé de structurer nos données autour de quatres collections : customers, payments, orders et employees."
  },
  {
    "objectID": "projets/projets_but/mirgation/migration.html#script-de-migration",
    "href": "projets/projets_but/mirgation/migration.html#script-de-migration",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Script de migration",
    "text": "Script de migration\nAvant de développer un script de migration, nous avons d’abord établi un pseudo-algorithme dans l’objecitf de structurer et organiser la logique de notre programme.\nEnsuite, pour la migration de nos données, nous avons utilisé SQLite comme source et MongoDB comme destination, en exploitant les bibliothèques Python sqlite3, pymongo et pandas. Le processus inclut l’extraction des données de SQLite, leur transformation au format document compatible avec MongoDB, et leur insertion dans les collections appropriées."
  },
  {
    "objectID": "projets/projets_but/essai_clinique/essai_clinique.html",
    "href": "projets/projets_but/essai_clinique/essai_clinique.html",
    "title": "Etude statistique dans un essai clinique",
    "section": "",
    "text": "Ce projet consiste en la réalisation d’une étude statistique dans le cadre d’un essai clinique. Nous travaillons sur un jeu de données simulées pour effectuer l’analyse statistique d’une étude de phase 3. L’objectif est de fournir au laboratoire une Autorisation de Mise sur le Marché (AMM) avec une indication dans la prise en charge de la drépanocytose.\nLa drépanocytose est une maladie génétique qui affecte les globules rouges et peut entraîner des complications graves. Elle se manifeste notamment par une anémie, des crises douloureuses et un risque accru d’infections.\n\n\n\nSchéma de la drépanocytose\n\n\nLors d’une étude de phase 3, les chercheurs comparent un nouveau traitement prometteur au traitement standard, qui est le traitement reconnu et généralement administré pour une affection ou une maladie. Dans notre étude statistique, le nouveau traitement s’appelle le Voxelotor et nous comparons ces effets à un placebo (traitement qui n’a aucune action spécifique sur le trouble qu’il vise à soulager). Pour cela, nous réalisons des tests statistiques sur des hypothèses définit en amont.\nLe dépôt GitHub contenant le rapport complet du projet, ainsi que le code R, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/essai_clinique/essai_clinique.html#plan-danalyse-statistique",
    "href": "projets/projets_but/essai_clinique/essai_clinique.html#plan-danalyse-statistique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Plan d’analyse statistique",
    "text": "Plan d’analyse statistique\nLe plan d’analyse statistique (SAP, disponible ici) décrit de manière détaillée la stratégie d’analyse des données de l’étude. Il garantit la rigueur, la transparence et la reproductibilité des analyses statistiques. Ce plan est généralement rédigé avant la fin de l’essai, afin d’éviter tout biais pouvant découler des résultats observés.\nPour évaluer l’effet du voxelotor sur l’amélioration de l’hémoglobine des patients par rapport au placebo, les analyses définies dans le SAP sont toutes réalisées. L’analyse principale porte sur la réponse en hémoglobine à la semaine 72, qui correspond au maximum du suivi des patients. L’analyse secondaire quant à elle examine le changement de l’hémoglobine entre le début de l’étude et la semaine 72 sur plusieurs marqueurs cliniques.\n\nAnalyse principal\nLa réponse en hémoglobine est définie comme une augmentation de plus de 1g/dL par rapport à la valeur de référence mesurée avant le début du traitement. Cela signifie qu’un patient est considéré comme ayant répondu favorablement au traitement si son taux d’hémoglobine a augmenté d’au moins 1g/dL par rapport à la valeur initiale, mesurée avant l’administration du voxelotor. Si aucune mesure d’hémoglobine n’est disponible à la semaine 72, le patient est imputé comme non-répondeur. Par exemple, si le patient ne se présente pas à la consultation, si les données sont perdues ou invalides, ce patient sera considéré comme n’ayant pas montré de réponse au traitement.\nNous utilisons un test statistique du Chi-2 avec correction de Yates pour comparer les taux de réponse en hémoglobine entre le groupe traité par VOX_1500 et le groupe par le placebo, afin de déterminer s’il existe une différence significative.\n\n\n\nRépartition des traitements dans notre population\n\n\nLes résultats du test indiquent qu’il n’y a pas de preuve statistiquement significative d’une différence entre les taux de réponse en hémoglobine des deux groupes.\nCependant, certains facteurs pourraient être amenés à faire varier l’efficacité du VOX_1500. C’est pourquoi, nous réalisons le même test statistique avec les mêmes hypothèses sur des sous-populations définis à partir des facteurs suivants : l’âge des patients, l’utilisation préalable d’hydroxyurée (HU) et l’historique des crises vaso-occlusives (VOC).\nLes résultats de cette étude, montrent que le VOX_1500 semble plus efficace chez certains groupes de patients. Premièrement, on observe une différence significative en faveur du traitement chez les non-utilisateurs d’HU. Cela indique une meilleure réponse en hémoglobine par rapport au placebo. En revanche, chez les utilisateurs d’HU, aucune différence notable n’a été trouvée entre les deux traitements.\nConcernant les crises vaso-occlusives, les patients ayant eu une seule crise répondent mieux au VOX_15000, alors que ceux ayant eu plusieurs crises ne montrent aucun signe positif supplémentaire par rapport au placebo.\nEnfin, l’analyse par tranche d’âge révèle des résultats similaires entre les groupes VOX_1500 et placebo, tant chez les adolescents que chez les adultes.\nEn somme, ces observations suggèrent que l’efficacité du VOX_1500 pourrait dépendre de certains facteurs cliniques, mais des études supplémentaires seront nécessaires pour valider ces conclusions.\n\n\nAnalyse secondaire\nDans cette secone partie, nous essayons de déterminer s’il existe une différence dans le changement moyen des taux d’hémoglobine entre les deux groupes (VOX_1500 et placebo). Pour répondre à cette question, nous réalisons une analyse statistique à l’aide de l’ANOVA. Cette dernière est une méthode statistique permettant de comparer les moyennes de plusieurs groupes afin de déterminer si au moins un groupe diffère significativement des autres. Plus concrètement, elle permet d’évaluer si les variations observées dans les données sont dues à des différences entre les groupes ou à des variations aléatoires au sein des groupes eux-mêmes. Elle nous est très utile ici, car elle permet d’ajuster les résultats en tenant compte des valeurs d’hémoglobine de départ (baseline) et de l’utilisation préalable d’hydroxyurée (HU) comme facteur supplémentaire.\nLes résultats révèlent que le VOX_1500 entraîne une augmentation significative du taux d’hémoglobine par rapport au placebo.\n\n\n\nReprésentation graphique du changement ajusté d’hémoglobine entre le départ et la semaine 72\n\n\nLes patients traités par VOX_1500 présentent une augmentation moyenne de 0,51 g/dL d’hémoglobine, contre 0,23 g/dL pour le groupe placebo. Cette différence est statistiquement significative au risque 5 %. Nous observons, un très faible impact des taux d’hémoglobine au départ sur l’évolution des niveaux observés. Cependant, l’utilisation préalable d’hydroxyurée influence fortement les résultats, les patients sous ce traitement montrant des évolutions différentes de ceux n’en prenant pas. Le traitement peut donc offrir un bénéfice notable en termes d’augmentation de l’hémoglobine, en particulier chez les patients n’utilisant pas d’hydroxyurée.\n\n\nMarqueurs d’hémolyse\nNous analysons ensuite l’évolution des marqueurs d’hémolyse en utilisant la même méthode. Ces indicateurs biologiques permettent d’évaluer la destruction des globules rouges dans le sang. Deux marqueurs clésretiennent notre attention :\n\nBilirubine non conjuguée : elle augmente lorsque les globules rouges sont détruits, reflétant une hémolyse active.\nPourcentage de réticulocytes : ces globules rouges immatures sont produits en réponse à la destruction des globules matures. Un taux élevé traduit une compensation de l’organisme face à l’hémolyse.\n\nL’analyse de ces paramètres permet de mieux comprendre l’impact du traitement sur la santé des globules rouges.\n\n\n\nVariations dans les marqueurs d’hémolyse entre la baseline et la semaine 72\n\n\nL’analyse des marqueurs d’hémolyse révèle que le traitement VOX_1500 a un effet significatif sur la réduction de la bilirubine non conjuguée et des réticulocytes, deux indicateurs clés de la destruction des globules rouges.\n\n\n\nReprésentation graphique du changement dans les marqueurs d’hémolyse\n\n\nLe traitement VOX_1500 entraîne une diminution moyenne de -8,09 g/dL contre -4,56 g/dL pour le placebo. Cette différence statistiquement significative confirme l’effet notable du traitement. De plus, le VOX_1500 montre également un impact significatif, sans que les valeurs initiales ou l’utilisation d’Hydroxyurée n’influencent les résultats.\nOn en conclut que le traitement VOX_1500 a un effet plus marqué que le placebo sur la réduction de la bilirubine non conjuguée et des réticulocytes, comme le montre la différence significative dans les changements moyens ajustés pour ces deux marqueurs d’hémolyse. Le traitement pourrait ainsi être plus efficace pour réduire les caractéristiques d’hémolyse par rapport au placebo."
  },
  {
    "objectID": "projets/projets_but/essai_clinique/essai_clinique.html#nouvel-essai-clinique",
    "href": "projets/projets_but/essai_clinique/essai_clinique.html#nouvel-essai-clinique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Nouvel essai clinique",
    "text": "Nouvel essai clinique\nAu vu des facteurs influents vus dans l’analyse principal. Il se peut que le laboratoire envisage de lancer un nouvel essai clinique afin d’évaluer l’efficacité du VOX_1500 chez les adolescents (patient de 12 à 18 ans). En utilisant le même critère principal que précédemment et en se basant sur les résultats obtenus pour le voxelotor dans cette population, tout en supposant un taux de réponse de 10% pour le groupe placebo, nous procédons au calcul du nombre de patients nécessaires pour démontrer l’efficacité du traitement. Ce calcul est effectué en visant une puissance statistique de 90% et un risque alpha de 5%, avec un ratio de traitement 1:1.\n\n\n\nProgramme R du calcul du nombre de patients\n\n\nLa taille de l’échantillon nécessaire pour ce nouvel essai clinique est de 48 patients au total, répartis également entre les deux groupes, soit 24 patients par groupe (placebo et le VOX_1500). Cette estimation garantit une puissance de 90% pour détecter une différence significative entre les groupes 29 avec un risque alpha de 5%."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Trier par\n       Ordre par défaut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus récent\n        \n         \n          Auteur·rice\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nL’algorithme des \\(k\\) plus proches voisins\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication sur R de l’algorithme des \\(k\\) plus proches voisins.\n\n\n\n\n\n29 sept. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarte d’aide pour les Volontaires de Paris 2024\n\n\n\n\n\n\nR\n\n\nOpendata\n\n\n\nPlus de 45 000 volontaires ont contribué au bon déroulement des Jeux Olympiques. Venus des quatre coins de la France, cette carte interactive leur permet de se préparer efficacement et d’accomplir au mieux leur mission.\n\n\n\n\n\n9 sept. 2024\n\n\n\n\n\n\nAucun article correspondant"
  }
]