[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rachid Sahli",
    "section": "",
    "text": "Je suis √©tudiant √† l‚ÄôIUT Paris Rives de Seine en Science des Donn√©es et alternant √† l‚ÄôINSEE. Au sein de la Direction des Statistiques D√©mographiques et Sociales, mes missions consistent √† couvrir diff√©rentes bases de sondage et √† apparier de gros volumes de donn√©es.\nJe travaille principalement avec R et Python, et j‚Äôai cr√©√© un site de ressources appel√© PratiqueR. Vous pouvez √©galement me retrouver sur YouTube, o√π je partage des vid√©os sur R et son environnement.\nEn dehors de mes √©tudes, je consacre mon temps libre √† la construction de robots et je partage mes id√©es sur mon blog, Jiqiren.\nPassionn√© par les statistiques, la robotique ü¶øü§ñ, le v√©lo üö≤, le cin√©ma üéûÔ∏è, et la programmation üëæ, j‚Äôaime explorer et partager mes d√©couvertes dans ces domaines.\nJe suis tr√®s curieux et toujours ouvert √† de nouvelles id√©es. N‚Äôh√©sitez pas √† me contacter pour discuter d‚Äôun projet ou explorer des opportunit√©s de collaboration.\n\n\nFormation\n\n\n\n\n\n\nIUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, 75016\n\n\n\nBUT Science des donn√©es, parcours exploration et mod√©lisation statistique | Sept 2022 - Juin 2025\nCours suivis : Statistique inf√©rentielle, param√©trique et non-param√©trique, Mod√©lisation statistique, Alg√®bre lin√©aire, Analyse, Probabilit√©s, Machine learning, Data mining, Programmation, Base de donn√©es\n\n\n\n\nExp√©rience\n\n\n\n\n\n\nInstitut national de la statistique et des √©tudes √©conomiques (INSEE) | Montrouge, 92120\n\n\n\nStatisticien | Sept 2023 - Sept 2025 Au sein de la Direction des statistiques d√©mographiques et sociales, j‚Äôai men√© des travaux d‚Äôappariement de donn√©es administratives. L‚Äôobjectif de ces travaux √©tait de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j‚Äôai compar√© diff√©rents algorithmes d‚Äôappariement √† l‚Äôaide d‚Äôanalyses statistiques. De plus, j‚Äôai mis en place des mod√®les de classification exploitant les donn√©es administratives afin de r√©aliser des pr√©dictions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "√Ä propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des donn√©es | Sept 2022 - Juin 2025\nCours suivis : Alg√®bre, Statistique, Probabilit√©s, Programmation, Base de donn√©es, √âconomie"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Trier par\n       Ordre par d√©faut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus r√©cent\n        \n         \n          Auteur¬∑rice\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR√©gression logistique en pratique\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication d‚Äôun mod√®le de r√©gression logistique pour pr√©dire si un patient est atteint de diab√®te. Le mod√®le est √©galement compar√© √† la m√©thode des \\(k\\) plus proches voisins.\n\n\n\n\n\n10 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôalgorithme des \\(k\\) plus proches voisins\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication sur R de l‚Äôalgorithme des \\(k\\) plus proches voisins.\n\n\n\n\n\n29 sept. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarte d‚Äôaide pour les Volontaires de Paris 2024\n\n\n\n\n\n\nR\n\n\nOpendata\n\n\n\nCarte interactive des volontaires de Paris 2024.\n\n\n\n\n\n9 sept. 2024\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "blog/knn.html",
    "href": "blog/knn.html",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "",
    "text": "Le but de l‚Äôapprentissage supervis√© est de pr√©voir l‚Äô√©tiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (r√©gression) associ√©e √† une nouvelle entr√©e \\(X\\), o√π il est sous-entendu que (\\(X,Y\\)) est une nouvelle r√©alisation des donn√©es, ind√©pendante de l‚Äô√©chantillon observ√©.\nL‚Äôalgorithme des \\(k\\) plus proches voisins est une m√©thode d‚Äôapprentissage supervis√©. On peut l‚Äôutiliser pour classifier quand \\(Y_i\\) est une variable qualitative, les \\(Y_i\\) sont appel√©s √©tiquettes. On peut √©galement l‚Äôutiliser pour pr√©dire si \\(Y_i \\in \\mathbb{R}\\). C‚Äôest donc une r√©gression et les \\(Y_i\\) sont appel√©s variables √† expliquer.\nRemarque : On parle d‚Äôapprentissage supervis√© car pour chaque \\(X_i\\) de l‚Äô√©chantillon d‚Äôapprentissage on dispose de \\(Y_i\\), l‚Äô√©tiquette. Au contraire, on parlera d‚Äôapprentissage non-supervis√© lorsque l‚Äô√©chantillon est simplement constitu√© des \\(X_i\\)."
  },
  {
    "objectID": "projets.html",
    "href": "projets.html",
    "title": "Projets",
    "section": "",
    "text": "Trier par\n       Ordre par d√©faut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus r√©cent\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProjets BUT SD\n\n\n1 min.\n\n\n\nScience des donn√©es\n\n\nStatistique\n\n\n\nVoici une liste de mes projets r√©alis√©es durant mes 3 ann√©es √† l‚ÄôInstitut Universitaire de Technologie de Paris.\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "projets/projets_but/louvre.html",
    "href": "projets/projets_but/louvre.html",
    "title": "Nocturne au mus√©e du Louvre",
    "section": "",
    "text": "Tous les vendredis, le mus√©e du Louvre offre un moment de magie √† ses visiteurs au milieu de ses collections. Au programme de ces nocturnes hebdomadaires, de nombreuses activit√©s pour petits et grands. Le 24 novembre 2023, mes camarades de l‚ÄôIUT et moi avons pu participer √† l‚Äôune de ces soir√©es. Nous avons d√©cid√© de pr√©senter certains tableaux du c√©l√®bre mus√©e au filtre de l‚ÄôIA. Au moment o√π cette technologie prend une place active dans notre soci√©t√©, avec par exemple ChatGPT, DALL-E ou encore Mistral AI, il est int√©ressant de pouvoir approfondir l‚Äôutilisation de ces outils permettant de g√©n√©rer des √©l√©ments en lien, ici, avec l‚Äôart, par exemple. De plus, ce sujet fascinant qu‚Äôest l‚Äôintelligence artificielle est plus ou moins en lien avec notre formation en science des donn√©es. Explorer ce domaine o√π se m√™lent math√©matiques, informatique et donn√©es √©tait particuli√®rement captivant.\n\nQu‚Äôavons nous fait ?\nL‚Äôobjectif final √©tait de pouvoir le 23 novembre 2024, pr√©senter une ≈ìuvre en rapport avec l‚ÄôIA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons d√©cid√© de choisir un sujet d‚Äôintelligence artificielle qui pouvait co√Øncider avec une ≈ìuvre du mus√©e. Apr√®s de longues recherches passionnantes, nous avons choisi de pr√©senter les GAN (Generative adversarial networks). C‚Äôest une classe d‚Äôalgorithmes d‚Äôapprentissage non supervis√©. Ils permettent de g√©n√©rer des images avec un fort degr√© de r√©alisme. Nous avons trouv√© la technologie et les m√©thodes tr√®s int√©ressantes, d‚Äôautant plus qu‚Äôelles aboutissent √† des applications concr√®tes, telles que la g√©n√©ration d‚Äôimages artistiques. Cependant, ces algorithmes sont utilis√©s dans bien d‚Äôautres domaines tels que la m√©decine ou encore la finance. Mais concr√®tement, comment √ßa marche ?\n\n\nQu‚Äôest-ce qu‚Äôun GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL‚Äôartiste (G√©n√©rateur) : L‚Äôobjectif du joueur est de cr√©er des images qui se rapprochent le plus de la r√©alit√©. Pour cela, il apprend de ses erreurs et r√©essaie en continu d‚Äôobtenir l‚Äô≈ìuvre la plus proche du r√©el.\nLe juge (Discriminateur) : L‚Äôobjectif de ce joueur est de v√©rifier si les ≈ìuvres r√©alis√©es par l‚Äôartiste peuvent para√Ætre r√©elles ou si elles sont encore trop fausses. Pour cela, il compare les ≈ìuvres de l‚Äôartiste √† des ≈ìuvres r√©elles faites par des peintres.\n\nTant que l‚Äôimage n‚Äôest pas accept√© par le juge, l‚Äôartiste continue √† produire des ≈ìuvres. Voil√†, le fonctionnement d‚Äôun GAN ou deux r√©seaux de neurones se font concurrence. De cette mani√®re, il est possible de cr√©er des images, des vid√©os ou d‚Äôautres contenues de tr√®s bonnes qualit√©s.\n\n\n\n\n\n\n\nSch√©ma GAN\n\n\n\n\nPr√©sentation au public\nNous avons d√©cid√© de pr√©senter au public le travail d‚ÄôObvious. Ce collectif de chercheurs, d‚Äôartistes travaille avec des mod√®les d‚Äôapprentissage profond pour explorer le potentiel cr√©atif de l‚Äôintelligence artificielle. Ils ont justement utilis√© des GAN pour g√©n√©rer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d‚ÄôEdmond de Belamy. Ce tableau est une impression sur toile qui est rentr√©e dans l‚Äôhistoire de l‚Äôart moderne. Cela, car c‚Äôest la premi√®re ≈ìuvre d‚Äôart produite par un logiciel d‚Äôintelligence artificielle √† √™tre pr√©sent√©e dans une salle des ventes. Pour couronner le tout, il a √©t√© vendu 432 500 dollars chez Christie‚Äôs le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est tr√®s difficile √† premi√®re vue de d√©terminer qu‚Äôune machine a pu en √™tre l‚Äôauteur.   Obvious √† utiliser un GAN, en l‚Äôentra√Ænant sur 15 000 portraits classiques r√©alis√©s entre le 14e et 20e si√®cle. L‚Äôalgorithme devait donc produire un tableau en sortie qui serait tr√®s ressemblant aux portraits classiques. Nous avons d√©cid√© de comparer le portrait d‚ÄôEdmond de Belamy √† une ≈ìuvre du Louvre se trouvant dans la salle 846 de l‚Äôaile Richelieu du mus√©e. C‚Äôest une peinture datant du 17e si√®cle r√©alis√©e par Jean Bray, peintre n√©erlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\nPortrait d‚ÄôEdmond de Belamy, Collectif Obvious\n\n\n\n\n\n\n\n\n\n\n\nPortrait d‚Äôhomme, 1658\n\n\n\n\n\nLes visiteurs √©taient agr√©ablement surpris par le r√©alisme du portrait d‚ÄôEdmond de Belamy, mais aussi par le prix de vente de l‚Äô≈ìuvre. Ils pensaient pouvoir reconna√Ætre la r√©alisation d‚Äôune IA."
  },
  {
    "objectID": "projets/projets_but/louvre.html#quavons-nous-fait",
    "href": "projets/projets_but/louvre.html#quavons-nous-fait",
    "title": "Nocturne au mus√©e du Louvre",
    "section": "Qu‚Äôavons nous fait ?",
    "text": "Qu‚Äôavons nous fait ?\nL‚Äôobjectif final √©tait de pouvoir le 23 novembre 2024, pr√©senter une ≈ìuvre en rapport avec l‚ÄôIA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons d√©cid√© de choisir un sujet d‚Äôintelligence artificielle qui pouvait co√Øncider avec une ≈ìuvre du mus√©e. Apr√®s de longues recherches passionnantes, nous avons choisi de pr√©senter les GAN (Generative adversarial networks). C‚Äôest une classe d‚Äôalgorithmes d‚Äôapprentissage non supervis√©. Ils permettent de g√©n√©rer des images avec un fort degr√© de r√©alisme. Nous avons trouv√© la technologie et les m√©thodes tr√®s int√©ressantes, d‚Äôautant plus qu‚Äôelles aboutissent √† des applications concr√®tes, telles que la g√©n√©ration d‚Äôimages artistiques. Cependant, ces algorithmes sont utilis√©s dans bien d‚Äôautres domaines tels que la m√©decine ou encore la finance. Mais concr√®tement, comment √ßa marche ?"
  },
  {
    "objectID": "projets/projets_but/louvre.html#quest-ce-quun-gan",
    "href": "projets/projets_but/louvre.html#quest-ce-quun-gan",
    "title": "Nocturne au mus√©e du Louvre",
    "section": "Qu‚Äôest-ce qu‚Äôun GAN ?",
    "text": "Qu‚Äôest-ce qu‚Äôun GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL‚Äôartiste (G√©n√©rateur) : L‚Äôobjectif du joueur est de cr√©er des images qui se rapprochent le plus de la r√©alit√©. Pour cela, il apprend de ses erreurs et r√©essaie en continu d‚Äôobtenir l‚Äô≈ìuvre la plus proche du r√©el.\nLe juge (Discriminateur) : L‚Äôobjectif de ce joueur est de v√©rifier si les ≈ìuvres r√©alis√©es par l‚Äôartiste peuvent para√Ætre r√©elles ou si elles sont encore trop fausses. Pour cela, il compare les ≈ìuvres de l‚Äôartiste √† des ≈ìuvres r√©elles faites par des peintres.\n\nTant que l‚Äôimage n‚Äôest pas accept√© par le juge, l‚Äôartiste continue √† produire des ≈ìuvres. Voil√†, le fonctionnement d‚Äôun GAN ou deux r√©seaux de neurones se font concurrence. De cette mani√®re, il est possible de cr√©er des images, des vid√©os ou d‚Äôautres contenues de tr√®s bonnes qualit√©s.\n\n\n\n\n\n\n\nSch√©ma GAN"
  },
  {
    "objectID": "projets/projets_but/louvre.html#pr√©sentation-au-public",
    "href": "projets/projets_but/louvre.html#pr√©sentation-au-public",
    "title": "Nocturne au mus√©e du Louvre",
    "section": "Pr√©sentation au public",
    "text": "Pr√©sentation au public\nNous avons d√©cid√© de pr√©senter au public le travail d‚ÄôObvious. Ce collectif de chercheurs, d‚Äôartistes travaille avec des mod√®les d‚Äôapprentissage profond pour explorer le potentiel cr√©atif de l‚Äôintelligence artificielle. Ils ont justement utilis√© des GAN pour g√©n√©rer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d‚ÄôEdmond de Belamy. Ce tableau est une impression sur toile qui est rentr√©e dans l‚Äôhistoire de l‚Äôart moderne. Cela, car c‚Äôest la premi√®re ≈ìuvre d‚Äôart produite par un logiciel d‚Äôintelligence artificielle √† √™tre pr√©sent√©e dans une salle des ventes. Pour couronner le tout, il a √©t√© vendu 432 500 dollars chez Christie‚Äôs le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est tr√®s difficile √† premi√®re vue de d√©terminer qu‚Äôune machine a pu en √™tre l‚Äôauteur.   Obvious √† utiliser un GAN, en l‚Äôentra√Ænant sur 15 000 portraits classiques r√©alis√©s entre le 14e et 20e si√®cle. L‚Äôalgorithme devait donc produire un tableau en sortie qui serait tr√®s ressemblant aux portraits classiques. Nous avons d√©cid√© de comparer le portrait d‚ÄôEdmond de Belamy √† une ≈ìuvre du Louvre se trouvant dans la salle 846 de l‚Äôaile Richelieu du mus√©e. C‚Äôest une peinture datant du 17e si√®cle r√©alis√©e par Jean Bray, peintre n√©erlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPortrait d‚Äôhomme, 1658\n\n\n\n\n\nLes visiteurs √©taient agr√©ablement surpris par le r√©alisme du portrait d‚ÄôEdmond de Belamy, mais aussi par le prix de vente de l‚Äô≈ìuvre. Ils pensaient pouvoir reconna√Ætre la r√©alisation d‚Äôune IA."
  },
  {
    "objectID": "projets/but_sd.html",
    "href": "projets/but_sd.html",
    "title": "Projets BUT SD",
    "section": "",
    "text": "Etude statistique dans un essai clinique\n\n\n1 min.\n\n\n\nR\n\n\nStatistique\n\n\n\n\n\n\n\n1 d√©c. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMigration de donn√©es : De SQL √† NoSQL\n\n\n4 min.\n\n\n\nSQL\n\n\n\nCe projet vise √† migrer des donn√©es d‚Äôun environnement SQL vers un environnement NoSQL pour une entreprise automobile.\n\n\n\n28 d√©c. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNocturne au mus√©e du Louvre\n\n\n4 min.\n\n\n\nIA\n\n\n\nPr√©sentation d‚Äôune ≈ìuvre g√©n√©r√©e par une IA aux visiteurs du Mus√©e du Louvre.\n\n\n\n24 nov. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS√©rie Temporelle : Production de charbon aux √âtats-Unis\n\n\n1 min.\n\n\n\nR\n\n\nRShiny\n\n\nStatistique\n\n\n\nLa pr√©vision de la production de charbon aux √âtats-Unis est r√©alis√©e √† partir de l‚Äôanalyse de la saisonnalit√© des s√©ries temporelles et de la mod√©lisation via trois m√©thodes‚Ä¶\n\n\n\n19 janv. 2024\n\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "about.html#formation",
    "href": "about.html#formation",
    "title": "√Ä propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des donn√©es | Sept 2022 - Juin 2025\nCours suivis : Alg√®bre, Statistique, Probabilit√©s, Programmation, Base de donn√©es, √âconomie"
  },
  {
    "objectID": "about.html#exp√©rience",
    "href": "about.html#exp√©rience",
    "title": "√Ä propos",
    "section": "Exp√©rience",
    "text": "Exp√©rience\n\n\n\n\n\n\nInstitut national de la statistique et des √©tudes √©conomiques (INSEE) | Montrouge, France\n\n\n\nProgrammeur Statistique | 2023 - 2025 Au sein de la Direction des statistiques d√©mographiques et sociales, j‚Äôai men√© des travaux d‚Äôappariement de donn√©es administratives. L‚Äôobjectif de ces travaux √©tait de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j‚Äôai compar√© diff√©rents algorithmes d‚Äôappariement √† l‚Äôaide d‚Äôanalyses statistiques.\nMes missions ont inclus la manipulation de donn√©es brutes, le nettoyage des donn√©es, l‚Äôappariement de grands volumes de donn√©es, ainsi que leur analyse statistique."
  },
  {
    "objectID": "about.html#comp√©tences",
    "href": "about.html#comp√©tences",
    "title": "√Ä propos",
    "section": "Comp√©tences",
    "text": "Comp√©tences\n\n\n\n\n\n\nProgrammation\n\n\n\nPython | R | SQL | SAS | GIT\n\n\n\n\n\n\n\n\nLangue\n\n\n\nAnglais (B2) | Espagnol (B2) | Arabe (C2)"
  },
  {
    "objectID": "projets/pratiquer.html",
    "href": "projets/pratiquer.html",
    "title": "PratiqueR",
    "section": "",
    "text": "J‚Äôai d√©velopper PratiqueR pour les raisons suivantes :"
  },
  {
    "objectID": "projets/projets_but/integration.html",
    "href": "projets/projets_but/integration.html",
    "title": "Int√©gration de donn√©es dans un data warehouse",
    "section": "",
    "text": "Ce projet a √©t√© r√©alis√© dans le cadre du cours d‚Äôint√©gration de donn√©es en BUT SD (2 √®me ann√©e).\nL‚Äôobjectif de ce module √©tait de nous initier au stockage de donn√©es en entreprise afin de permettre une approche d√©cisionnelle. Nous avons vu des notions telles que le data warehouse, l‚ÄôETL, les syst√®mes d‚Äôinformation d√©cisionnels, SQL‚Ä¶"
  },
  {
    "objectID": "projets/projets_but/integration.html#introduction",
    "href": "projets/projets_but/integration.html#introduction",
    "title": "Int√©gration de donn√©es dans un data warehouse",
    "section": "Introduction",
    "text": "Introduction\nUne entreprise fran√ßaise de e-commerce nous a sollicit√©s pour analyser le comportement des utilisateurs sur sa plateforme. Dans un premier temps, nous avons √©quip√© l‚Äôentreprise d‚Äôun syst√®me de stockage de donn√©es. Ensuite, nous avons mis en place un tableau de bord permettant de r√©pondre √† la probl√©matique pos√©e."
  },
  {
    "objectID": "projets/projets_but/integration.html#mod√®le-relationnel",
    "href": "projets/projets_but/integration.html#mod√®le-relationnel",
    "title": "Int√©gration de donn√©es dans un data warehouse",
    "section": "Mod√®le relationnel",
    "text": "Mod√®le relationnel\nApr√®s une √©tude approfondie de l‚Äôenvironnement et des besoins de l‚Äôentreprise, nous avons √©tabli un mod√®le relationnel pertinent, adapt√© √† l‚Äôanalyse du comportement des utilisateurs sur la plateforme.\nNous avons choisi d‚Äôutiliser un mod√®le relationnel en flocon."
  },
  {
    "objectID": "projets/pratiquer.html#acc√©dez-au-site",
    "href": "projets/pratiquer.html#acc√©dez-au-site",
    "title": "PratiqueR",
    "section": "Acc√©dez au site",
    "text": "Acc√©dez au site\nD√©couvrez PratiqueR ici"
  },
  {
    "objectID": "blog/knn.html#pr√©diction",
    "href": "blog/knn.html#pr√©diction",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "3.1 Pr√©diction",
    "text": "3.1 Pr√©diction\nNous allons maintenant manipuler le jeu de donn√©es Abalone, disponible ici. Ce dernier contient des informations sur les ormeaux. Ce sont des mollusques marins qui poss√®dent une seule coquille et qui habitent principalement dans les eaux froides des c√¥tes. La valeur commerciale des ormeaux est √©troitement li√©e √† leur √¢ge, qui est le principal crit√®re utilis√© pour estimer leur prix. D√©terminez l‚Äô√¢ge des ormeaux se fait √† partir de leurs anneaux (rings). C‚Äôest une t√¢che g√©n√©ralement r√©alis√©e en laboratoire qui prend beaucoup de temps. Ainsi, notre objectif est de pr√©dire leur √¢ge (ici la taille de leurs anneaux) √† l‚Äôaide des variables physiologiques dont nous disposons.\nAfin de pouvoir r√©aliser notre pr√©diction nous importons les packages suivants.\n\n# Import library ---------------\nlibrary(kknn)\nlibrary(Metrics)\nlibrary(ggplot2)\n\n\nLe package kknn est une impl√©mentation de l‚Äôalgorithme des k plus proches voisins. Il permet notamment de pond√©rer les plus proches voisins lors de la pr√©diction.\nLe package Metrics est con√ßu pour √©valuer les performances des mod√®les pr√©dictifs en calculant diverses mesures d‚Äôerreur et de pr√©cision. Il est particuli√®rement utile pour les t√¢ches de r√©gression et de classification, car il propose un ensemble de fonctions pour mesurer l‚Äôexactitude des pr√©dictions. Nous l‚Äôutiliserons pour calculer l‚Äôerreur quadratique moyenne.\nLe package ggplot2 permet d‚Äôobtenir des visualisations graphiques plus pouss√©es.\n\nEnsuite, nous importons notre jeu de donn√©es en nommant les colonnes.\n\n# Import data ---------------\n# Nom des colonnes\ncolnames = c(\"Sex\",\"Length\",\"Diameter\",\"Height\",\"Whole_weight\",\"Shucked_weight\",\n             \"Viscera_weight\",\"Shell_weight\",\"Rings\")\n\n# Import de la table\nabalone &lt;- read.table(\"data_blog/abalone.data\", header = TRUE, sep = \",\", col.names = colnames)\n\nIl contient 4 176 observations et 9 variables. Nous supprimons la variable Sex et les observations pour qui la taille (Height) est √©gale √† 0.\n\n# Longeur du jeu de donnees\ndim(abalone)\n\n[1] 4176    9\n\n# Suppression de la variable Sex\nabalone &lt;- abalone[,-1]\n\n# Suppression des valeurs nul\nabalone &lt;- subset(abalone, Height!=0)\n\n# Aucune valeurs manquantes\nsapply(abalone, function(x) sum(is.na(x)))\n\n        Length       Diameter         Height   Whole_weight Shucked_weight \n             0              0              0              0              0 \nViscera_weight   Shell_weight          Rings \n             0              0              0 \n\n\nCi-dessous, un petit aper√ßu des donn√©es.\n\nhead(abalone, 4)\n\n  Length Diameter Height Whole_weight Shucked_weight Viscera_weight\n1   0.35    0.265  0.090       0.2255         0.0995         0.0485\n2   0.53    0.420  0.135       0.6770         0.2565         0.1415\n3   0.44    0.365  0.125       0.5160         0.2155         0.1140\n4   0.33    0.255  0.080       0.2050         0.0895         0.0395\n  Shell_weight Rings\n1        0.070     7\n2        0.210     9\n3        0.155    10\n4        0.055     7\n\n\n\n3.1.1 Analyse descriptive\nOn proc√®de √† une succinte analyse descritpive de notre jeu de donn√©es.\n\nsummary(abalone)\n\n     Length          Diameter         Height        Whole_weight   \n Min.   :0.0750   Min.   :0.055   Min.   :0.0100   Min.   :0.0020  \n 1st Qu.:0.4500   1st Qu.:0.350   1st Qu.:0.1150   1st Qu.:0.4421  \n Median :0.5450   Median :0.425   Median :0.1400   Median :0.8000  \n Mean   :0.5241   Mean   :0.408   Mean   :0.1396   Mean   :0.8291  \n 3rd Qu.:0.6150   3rd Qu.:0.480   3rd Qu.:0.1650   3rd Qu.:1.1538  \n Max.   :0.8150   Max.   :0.650   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1861   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3595   Mean   :0.1807   Mean   :0.2389   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3289   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n\n\nOn observe ci-dessous la distribution de la variable Rings ainsi que la relation entre la longueur et la taille des ormeaux.\n\nggplot(abalone, aes(x = Rings)) +\n  geom_histogram(fill = \"blue\") +\n  labs(title = \"Distribution de Rings\", y = \"Fr√©quence\", x = \"Rings\") + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(abalone, aes(x = Length, y = Height)) +\n  geom_point(col = \"blue\", pch = 1) +\n  labs(title = \"Relation entre Height et Length\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.1.2 Pr√©dicition de la variable Rings\nComme dans la partie pr√©c√©dente, nous commen√ßons par cr√©er deux sous-√©chantillons distincts (√©chantillon d‚Äôapprentissage et echantillon de test) √† partir du jeu de donn√©es complet.\n\nN = round((80/100)*nrow(abalone)) # Calcul du nombre d'observations a s√©lectionner (80 %) \nidx1 &lt;- sample(1:nrow(abalone), size = N, replace = FALSE) # Tirage aleatoire des indices qu'on va s√©lectionner\ndataL &lt;- abalone[idx1,] # Construction du dataset d'apprentissage\ndataV &lt;- abalone[-idx1,] # Construction du dataset de test ou de validite\n\nA pr√©sent, on utilise la fonction kknn() pour mettre en oeuvre notre algorithme de pr√©diction en fixant \\(k = 3\\).\n\npred &lt;- kknn(Rings ~., dataL, dataV, k = 3, kernel = 'rectangular')\n\nCi-dessous, nous observons nos pr√©dictions en fonction de la variable Rings.\n\nplot(dataV$Rings,pred$fitted.values, xlab = \"Rings\", ylab = \"Prediction\", col = \"blue\")\nabline(0,1, col = \"red\")\n\n\n\n\n\n\n\n\nContrairement √† la classification, nous utiliserons l‚Äôerreur quadratique moyenne pour mesurer la performance de notre mod√®le sur l‚Äô√©chantillon de test.\n\nmse &lt;- mse(pred$fitted.values, dataV$Rings)\npaste0(\"Erreur quadratique moyenne = \",mse)\n\n[1] \"Erreur quadratique moyenne = 5.73666001330672\"\n\n\nEnfin, nous allons identifier la valeur de \\(k\\) pour laquelle l‚Äôerreur quadratique moyenne est la plus faible. On pourra alors d√©terminer le niveau optimal de \\(k\\) afin d‚Äôam√©liorer la pr√©cision du mod√®le. La boucle suivante permet de calculer l‚Äôerreur quadratique moyenne pour chaque valeur de \\(k\\) sur notre √©chantillon.\n\nkvec &lt;- 1:100\nerror &lt;- rep(NA, length(kvec))\n\nfor(i in 1:length(kvec)){\n  pred &lt;- kknn(Rings ~., dataL, dataV, k = i, kernel = 'rectangular')\n  error[i] &lt;- mse(dataV$Rings, pred$fitted.values)\n}\n\nOn visualise les r√©sultats sur le graphique ci-dessous.\n\nplot(kvec, error, type = \"b\", col = \"orange\")\nmin_error_niveau &lt;- which.min(error)\nabline(v = kvec[min_error_niveau], col = \"red\", lty = 2)\nlegend(\"topright\", legend = paste(\"Min error at k =\", kvec[min_error_niveau]), col = \"red\", lty = 2)"
  },
  {
    "objectID": "blog/knn.html#analyse-descriptive",
    "href": "blog/knn.html#analyse-descriptive",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "Analyse descriptive",
    "text": "Analyse descriptive\nOn proc√®de √† une succinte analyse descritpive de notre jeu de donn√©es.\n\nsummary(abalone)\n\n     Length          Diameter         Height        Whole_weight   \n Min.   :0.0750   Min.   :0.055   Min.   :0.0100   Min.   :0.0020  \n 1st Qu.:0.4500   1st Qu.:0.350   1st Qu.:0.1150   1st Qu.:0.4421  \n Median :0.5450   Median :0.425   Median :0.1400   Median :0.8000  \n Mean   :0.5241   Mean   :0.408   Mean   :0.1396   Mean   :0.8291  \n 3rd Qu.:0.6150   3rd Qu.:0.480   3rd Qu.:0.1650   3rd Qu.:1.1538  \n Max.   :0.8150   Max.   :0.650   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1861   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3595   Mean   :0.1807   Mean   :0.2389   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3289   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n\n\nOn observe ci-dessous la distribution de la variable Rings ainsi que la relation entre la longueur et la taille des ormeaux.\n\nggplot(abalone, aes(x = Rings)) +\n  geom_histogram(fill = \"blue\") +\n  labs(title = \"Distribution de Rings\", y = \"Fr√©quence\", x = \"Rings\") + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(abalone, aes(x = Length, y = Height)) +\n  geom_point(col = \"blue\", pch = 1) +\n  labs(title = \"Relation entre Height et Length\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/knn.html#pr√©dicition-de-la-variable-rings",
    "href": "blog/knn.html#pr√©dicition-de-la-variable-rings",
    "title": "L‚Äôalgorithme des \\(k\\) plus proches voisins",
    "section": "Pr√©dicition de la variable Rings",
    "text": "Pr√©dicition de la variable Rings\nComme dans la partie pr√©c√©dente, nous commen√ßons par cr√©er deux sous-√©chantillons distincts (√©chantillon d‚Äôapprentissage et echantillon de test) √† partir du jeu de donn√©es complet.\n\nN = round((80/100)*nrow(abalone)) # Calcul du nombre d'observations a s√©lectionner (80 %) \nidx1 &lt;- sample(1:nrow(abalone), size = N, replace = FALSE) # Tirage aleatoire des indices qu'on va s√©lectionner\ndataL &lt;- abalone[idx1,] # Construction du dataset d'apprentissage\ndataV &lt;- abalone[-idx1,] # Construction du dataset de test ou de validite\n\nA pr√©sent, on utilise la fonction kknn() pour mettre en oeuvre notre algorithme de pr√©diction en fixant \\(k = 3\\).\n\npred &lt;- kknn(Rings ~., dataL, dataV, k = 3, kernel = 'rectangular')\n\nCi-dessous, nous observons nos pr√©dictions en fonction de la variable Rings.\n\nplot(dataV$Rings,pred$fitted.values, xlab = \"Rings\", ylab = \"Prediction\", col = \"blue\")\nabline(0,1, col = \"red\")\n\n\n\n\n\n\n\n\nContrairement √† la classification, nous utiliserons l‚Äôerreur quadratique moyenne pour mesurer la performance de notre mod√®le sur l‚Äô√©chantillon de test.\n\nmse &lt;- mse(pred$fitted.values, dataV$Rings)\npaste0(\"Erreur quadratique moyenne = \",mse)\n\n[1] \"Erreur quadratique moyenne = 6.92375249500998\"\n\n\nEnfin, nous allons identifier la valeur de \\(k\\) pour laquelle l‚Äôerreur quadratique moyenne est la plus faible. On pourra alors d√©terminer le niveau optimal de \\(k\\) afin d‚Äôam√©liorer la pr√©cision du mod√®le. La boucle suivante permet de calculer l‚Äôerreur quadratique moyenne pour chaque valeur de \\(k\\) sur notre √©chantillon.\n\nkvec &lt;- 1:100\nerror &lt;- rep(NA, length(kvec))\n\nfor(i in 1:length(kvec)){\n  pred &lt;- kknn(Rings ~., dataL, dataV, k = i, kernel = 'rectangular')\n  error[i] &lt;- mse(dataV$Rings, pred$fitted.values)\n}\n\nOn visualise les r√©sultats sur le graphique ci-dessous.\n\nplot(kvec, error, type = \"b\", col = \"orange\")\nmin_error_niveau &lt;- which.min(error)\nabline(v = kvec[min_error_niveau], col = \"red\", lty = 2)\nlegend(\"topright\", legend = paste(\"Erreur min √† k =\", kvec[min_error_niveau]), col = \"red\", lty = 2)"
  },
  {
    "objectID": "projets/pratiquer.html#apprendre-r-pas-√†-pas",
    "href": "projets/pratiquer.html#apprendre-r-pas-√†-pas",
    "title": "PratiqueR",
    "section": "",
    "text": "J‚Äôai cr√©er PratiqueR pour les raisons suivantes :\n- Expliquer les bases du langage R de mani√®re claire et progressive.\n- Illustrer des cas d‚Äôutilisation courants √† l‚Äôaide d‚Äôexemples concrets et applicables.\n- Proposer des exercices pratiques pour renforcer vos comp√©tences et faciliter la prise en main."
  },
  {
    "objectID": "projets/pratiquer.html#aper√ßu-du-site",
    "href": "projets/pratiquer.html#aper√ßu-du-site",
    "title": "PratiqueR",
    "section": "Aper√ßu du site",
    "text": "Aper√ßu du site"
  },
  {
    "objectID": "blog/carte_volontaires.html",
    "href": "blog/carte_volontaires.html",
    "title": "Carte d‚Äôaide pour les Volontaires de Paris 2024",
    "section": "",
    "text": "Lors des Jeux Olympiques de 2024 √† Paris, plus de 45 000 volontaires ont √©t√© les v√©ritables hommes et femmes de l‚Äôombre. On les a vus partout √† Paris gr√¢ce √† leur tenue bleue. Ils ont contribu√© au succ√®s des Jeux Olympiques. Venant des quatre coins de la France, ils devaient pouvoir se rep√©rer. La carte d‚Äôaide pour les volontaires est con√ßue √† cet effet et leur a √©t√© tr√®s utile durant leur s√©jour √† Paris.\n\n\n\n\n\n\n Cette carte a √©t√© cr√©√©e √† l‚Äôaide du package leaflet, en utilisant les donn√©es disponibles en open data fournies par les acteurs suivants :\n\nFontaines √† eau : opendata de la Ville de Paris\nToilettes publiques : opendata de la Ville de Paris\nDistributeurs automatiques de billets : Opendatasoft hub\nParkings v√©lo : opendata de Paris 2024\nSite de comp√©titions des Jeux Olympiques\nParalympiques : opendata de Paris 2024\n\n(Deni√®re mis √† jour des donn√©es le 27 juillet 2024)"
  },
  {
    "objectID": "blog/reg_logistique.html",
    "href": "blog/reg_logistique.html",
    "title": "R√©gression logistique en pratique",
    "section": "",
    "text": "Introduction\nCe billet de blog a pour objectif d‚Äô√©tudier le mod√®le de r√©gression logistique et de le comparer ainsi √† la m√©thode des \\(k\\) plus proches voisins. Ces deux m√©thodes sont des algorithmes d‚Äôapprentissage supervis√©. Le but de l‚Äôapprentissage supervis√© est de pr√©voir l‚Äô√©tiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (r√©gression) associ√©e √† une nouvelle entr√©e \\(X\\), o√π il est sous-entendu que (\\(X,Y\\)) est une nouvelle r√©alisation des donn√©es, ind√©pendante de l‚Äô√©chantillon observ√©.\nLa r√©gression logistique est une m√©thode de classification. Ce mod√®le de classification binaire permet d‚Äôexpliquer une variable (\\(Y\\)) par p variables explicatives (\\(X_1,...,X_j\\). La variable \\(Y\\) ne peut prendre que deux modalit√©s \\(\\left\\{0 ;1\\right\\}\\). Les variables \\(X_j\\) sont exclusivement continues ou binaires (on re-code les variables qualitatives avec des 0 et 1).\nL‚Äôalgorithme des \\(k\\) plus proches voisins fonctionne de la fa√ßon suivante pour la classification. On d√©termine les \\(k\\) plus proches \\(X_i\\) de l‚Äô√©chantillon par rapport √† \\(X\\) et on attribue la modalit√© dominante parmi les \\(k\\) modalit√©s observ√©es (on parle de vote majoritaire).\nNous allons utiliser ces deux m√©thodes de classification afin de pr√©dire, en se basant sur des mesures diagnostiques, si un patient est atteint du diab√®te ou non.\n\n\nImport et pr√©paration des donn√©es\nDans un premier temps, nous importons le jeu de donn√©es ‚Äúdiabetes.csv‚Äù. Nous allons mettre en pratique nos m√©thodes de r√©gression logistique et \\(k\\) plus proches voisins sur ce dernier. Ce jeu de donn√©es est issu de l‚ÄôInstitut national du diab√®te et des maladies digestives et r√©nales. Tous les patients ici sont des femmes d‚Äôau moins 21 ans d‚Äôorigine Pima. Il contient les mesures suivantes.\n\nDescription des variables\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\nPregnancies\nNombre de grossesses\nQuanti continue\n\n\nGlucose\nConcentration de glucose plasmatique\nQuanti discr√®te\n\n\nBloodPressure\nPression art√©rielle diastolique (mm Hg)\nQuanti discr√®te\n\n\nSkinThickness\n√âpaisseur du pli cutan√© du triceps (mm)\nQuanti discr√®te\n\n\nInsulin\nInsuline s√©rique √† 2 heures (mu U/ml)\nQuanti discr√®te\n\n\nBMI\nIndice de masse corporelle (poids en kg / (taille en m)¬≤)\nQuanti continue\n\n\nDiabetesPedigree\nFonction de pr√©disposition au diab√®te\nQuanti continue\n\n\nAge\n√Çge (ann√©es)\nQuanti discr√®te\n\n\nOutcome\nStatut diab√©tique (oui ou non)\nQuanti discr√®te\n\n\n\n\n# Import library ---------------\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(class)\n\n# Import data ---------------\ndiabetes &lt;- read.csv(\"data_blog/diabetes.csv\")\n\ndiabetes$Outcome &lt;- as.factor(diabetes$Outcome)\n\nNous avons en m√™me temps import√© 3 packages qui nous seront utiles dans la r√©alisation de notre travail. √Ä savoir, ggplot pour la r√©alisation de graphique, et class pour la classification de l‚Äôalgorithme des \\(k\\) plus proches voisins.\nVoici un petit aper√ßu de nos donn√©es.\n\nhead(diabetes)\n\n  Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n6           5     116            74             0       0 25.6\n  DiabetesPedigreeFunction Age Outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n6                    0.201  30       0\n\n\nNotre jeu de donn√©es ne pr√©sente pas de valeurs manquantes (NA), mais contient des observations pour lesquelles la valeur est √©gale √† 0. Nous consid√©rerons ces valeurs comme manquantes et les supprimons. Nous faisons cela uniquement sur 5 variables pour qui l‚Äôon juge que la valeur 0 est une vraie donn√©e manquante. Par exemple, nous ne le faisons pas pour la variable Pregnancies pour qui le 0 veut tout simplement dire que la personne n‚Äôa jamais √©t√© enceinte. Nous conservons alors 392 individus sur les 768 initiaux.\n\n# Suppression des 0\ndiabetes &lt;- diabetes[diabetes$SkinThickness!=0,]\ndiabetes &lt;- diabetes[diabetes$Insulin!=0,]\ndiabetes &lt;- diabetes[diabetes$Glucose!=0,]\ndiabetes &lt;- diabetes[diabetes$BloodPressure!=0,]\ndiabetes &lt;- diabetes[diabetes$BMI!=0,] \n\nCette suppression √† pour objectif de ne pas fausser les relations que nous cherchons √† mod√©liser. Par exemple, avoir une insuline √† 0 n‚Äôest premi√®rement pas coh√©rent et peut influencer de mani√®re disproportionn√©e le mod√®le, car les autres observations sont √©loign√©es. En somme, nous essayons de garantir que le mod√®le repr√©sente fid√®lement la relation entre les variables.\n\n\nStatistique descriptive\n√Ä pr√©sent, nous allons √©tudier nos donn√©es de mani√®re descriptive afin d‚Äôen obtenir un aper√ßu et ainsi de visualiser la r√©partition de la variable \\(Y\\) (Outcome). De plus, nous analyserons les corr√©lations entre les co-variables elles-m√™mes, ainsi qu‚Äôentre celles-ci et la variable \\(Y\\).\n\nsummary(diabetes)\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   : 56.0   Min.   : 24.00   Min.   : 7.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.:21.00  \n Median : 2.000   Median :119.0   Median : 70.00   Median :29.00  \n Mean   : 3.301   Mean   :122.6   Mean   : 70.66   Mean   :29.15  \n 3rd Qu.: 5.000   3rd Qu.:143.0   3rd Qu.: 78.00   3rd Qu.:37.00  \n Max.   :17.000   Max.   :198.0   Max.   :110.00   Max.   :63.00  \n    Insulin            BMI        DiabetesPedigreeFunction      Age       \n Min.   : 14.00   Min.   :18.20   Min.   :0.0850           Min.   :21.00  \n 1st Qu.: 76.75   1st Qu.:28.40   1st Qu.:0.2697           1st Qu.:23.00  \n Median :125.50   Median :33.20   Median :0.4495           Median :27.00  \n Mean   :156.06   Mean   :33.09   Mean   :0.5230           Mean   :30.86  \n 3rd Qu.:190.00   3rd Qu.:37.10   3rd Qu.:0.6870           3rd Qu.:36.00  \n Max.   :846.00   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n Outcome\n 0:262  \n 1:130  \n        \n        \n        \n        \n\n\n\n\n\n\n\n\n\n\n\nDans notre √©chantillon, 262 patients (67 %) ne sont pas atteints de diab√®te, tandis que 130 patients (33 %) en sont atteints. Les personnes non-diab√©tiques sont donc largement plus nombreuses.\nEnviron 14,3 % des patients n‚Äôont jamais eu de grossesse. Il y a 23,7 % des patients qui ont eu une unique grossesse, et nous constatons une diminution progressive du nombre de patients √† mesure que le nombre de grossesses augmente. La majorit√© des patients ont eu au moins une grossesse, ce qui indique que les grossesses sont relativement courantes dans cet √©chantillon.\n\n\n\n\n\n\n\n\n\nOn observe ci-dessous, l‚Äô√¢ge des patients. Le plus jeune patient a 21 ans, tandis que le plus √¢g√© a 81 ans. Ce dernier se distingue clairement en haut de la bo√Æte √† moustaches, o√π il se situe assez √©loign√© des autres patients. On note √©galement 5 autres valeurs aberrantes avec un √¢ge tr√®s √©lev√©. L‚Äô√¢ge m√©dian dans l‚Äô√©chantillon est de 29 ans. Il est proche du Q1, on peut donc penser √† une repr√©sentation assez √©lev√©e de patients plut√¥t jeunes.\n\n\n\n\n\n\n\n\n\n√Ä pr√©sent, nous observons les diff√©rentes corr√©lations entre nos variables quantitatives continues. L‚Äôobjectif du coefficient de corr√©lation est de muserer la force de relation lin√©aire entre deux variables. Il se calcule de la mani√®re suivante :\n\\[\nR(x, y) = \\frac{\\text{Cov}(x, y)}{\\sigma_x \\sigma_y}\n\\]\nLe coefficient de corr√©lation est compris entre -1 et 1 :\n\nSi \\(R(x,y)\\) est proche de 0 : La relation lin√©aire est nulle.\nSi \\(R(x,y)\\) est proche de -1 : La relation lin√©aire est forte mais n√©gative.\nSi \\(R(x,y)\\) est proche de 1 : La relation lin√©aire est forte.\n\n\ncor(diabetes[,c(-1,-9)])\n\n                           Glucose BloodPressure SkinThickness   Insulin\nGlucose                  1.0000000     0.2100266     0.1988558 0.5812230\nBloodPressure            0.2100266     1.0000000     0.2325712 0.0985115\nSkinThickness            0.1988558     0.2325712     1.0000000 0.1821991\nInsulin                  0.5812230     0.0985115     0.1821991 1.0000000\nBMI                      0.2095159     0.3044034     0.6643549 0.2263965\nDiabetesPedigreeFunction 0.1401802    -0.0159711     0.1604985 0.1359058\nAge                      0.3436415     0.3000389     0.1677611 0.2170820\n                               BMI DiabetesPedigreeFunction        Age\nGlucose                  0.2095159               0.14018018 0.34364150\nBloodPressure            0.3044034              -0.01597110 0.30003895\nSkinThickness            0.6643549               0.16049853 0.16776114\nInsulin                  0.2263965               0.13590578 0.21708199\nBMI                      1.0000000               0.15877104 0.06981380\nDiabetesPedigreeFunction 0.1587710               1.00000000 0.08502911\nAge                      0.0698138               0.08502911 1.00000000\n\n\nLes variables sont toutes moyennement corr√©l√©es positivement.\nNous repr√©sentons ci-dessous les relations entre les co-variables et notre variable \\(Y\\) (Outcome).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn observe une corr√©lation entre nos variables quantitatives continues et la variable Outcome (\\(Y\\)). On le voit par exemple √† travers le lien entre la variable Glucose et la variable \\(Y\\). Les patients atteints de diab√®te pr√©sentent un taux de glucose significativement plus √©lev√© que ceux qui ne sont pas atteints par la maladie.\n\n\nMod√®le de regression logistique\nNous pouvons mettre en place notre mod√®le de r√©gression logistique. Nous commen√ßons par le mod√®le complet. C‚Äôest-√†-dire que nous s√©lectionnons toutes les co-variables pour expliquer \\(Y\\). On rappel qu‚Äôon cherche √† pr√©dire les valeurs de la variable Outcom (\\(Y\\)) √† l‚Äôaide des variables explicatives pr√©sentent dans notre jeu de donn√©es.\nOn utilise pour cela la fonction glm() (Fitting Generalized Linear Models).\n\nmodel_complet &lt;- glm(Outcome~., family = \"binomial\", data = diabetes)\nsummary(model_complet)\n\n\nCall:\nglm(formula = Outcome ~ ., family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7823  -0.6603  -0.3642   0.6409   2.5612  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -1.004e+01  1.218e+00  -8.246  &lt; 2e-16 ***\nPregnancies               8.216e-02  5.543e-02   1.482  0.13825    \nGlucose                   3.827e-02  5.768e-03   6.635 3.24e-11 ***\nBloodPressure            -1.420e-03  1.183e-02  -0.120  0.90446    \nSkinThickness             1.122e-02  1.708e-02   0.657  0.51128    \nInsulin                  -8.253e-04  1.306e-03  -0.632  0.52757    \nBMI                       7.054e-02  2.734e-02   2.580  0.00989 ** \nDiabetesPedigreeFunction  1.141e+00  4.274e-01   2.669  0.00760 ** \nAge                       3.395e-02  1.838e-02   1.847  0.06474 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.02  on 383  degrees of freedom\nAIC: 362.02\n\nNumber of Fisher Scoring iterations: 5\n\n\nLe param√®tre family = binomial indique que nous utilisons une r√©gression logistique et que notre variable est binaire.\nNous nous concentrons sur la significativit√© statistique de chaque coefficient. En effet, la \\(p-valeur\\) nous indique la probabilit√© que l‚Äôeffet de la variable explicative soit d√ª au hasard. Plus sa valeur est faible, plus l‚Äôeffet de la variable explicative sur notre variable \\(Y\\) (Outcome) est statistiquement significatif. Au contraire, si elle est sup√©rieure √† 0.05, cela sugg√®re que l‚Äôeffet de la variable explicative sur notre variable \\(Y\\) n‚Äôest pas significatif et pourrait √™tre d√ª au hasard.\nIci, les variables explicatives avec des p-valeurs inf√©rieur √† 5 % sont :\n\nGlucose (\\(p-valeur\\) = \\(3,24 * 10^{-11}\\)) : L‚Äôeffet de la variable glucose sur la probabilit√© de la variable diab√®te est significatif.\nBMI (\\(p-valeur\\) = \\(0.00989\\)) : L‚Äôeffet de la variable BMI sur la probabilit√© de la variable diab√®te est significatif.\nDiabetesPedigreeFunction (\\(p-valeur\\) = \\(0.00760\\)) : L‚Äôeffet de la variable DiabetesPedigreeFunction sur la probabilit√© de la variable diab√®te est significatif.\n\nLes variables restantes ont des \\(p-valeur\\) &gt; \\(5 \\%\\), on en conclut donc qu‚Äôelles ne sont pas li√©es au risque de diab√®te.\n\n\nCalcul des odds-ratios et de leurs intervalles de confiance\nL‚Äôodds-ratio est une mesure statistique exprimant le degr√© de d√©pendance entre des variables al√©atoires qualitatives. Il permet de mesurer l‚Äôeffet d‚Äôun facteur en comparant les chances qu‚Äôun √©v√®nement se produise dans un groupe par rapport √† un autre groupe.\nIci, l‚Äôodds-ratio va repr√©senter l‚Äôimpact de nos variables explicatives sur les chances que notre variable \\(Y\\) (Outcome) prenne la valeur diab√©tique (1) ou non-diab√©tique.\nNous calculons ci-dessous les odds-ratios de toutes les variables, ainsi que leurs intervalles de confiance qui nous permetteront d‚Äô√™tre sur √† 95 % que l‚Äôodds-ratio se trouve entre les bornes inf√©rieurs et sup√©rieurs.\nAfin d‚Äôobtenir les odds-ratios √† partir des coefficients de r√©gression logistique, nous avons pris l‚Äôexponentielle des coefficients.\n\n# Permet la realisation de tableau\nlibrary(knitr)\n\nkable(exp(model_complet$coefficients[-1]), col.names = c(\"Variable\", \"Odd-ratios\"))\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.0856289\n\n\nGlucose\n1.0390112\n\n\nBloodPressure\n0.9985807\n\n\nSkinThickness\n1.0112846\n\n\nInsulin\n0.9991750\n\n\nBMI\n1.0730849\n\n\nDiabetesPedigreeFunction\n3.1296107\n\n\nAge\n1.0345346\n\n\n\n\nkable(exp(confint(model_complet)[-1, ]), col.names = c(\"Variable\", \"IC_inf\", \"IC_sup\"))\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9743237\n1.211631\n\n\nGlucose\n1.0277173\n1.051303\n\n\nBloodPressure\n0.9757909\n1.022307\n\n\nSkinThickness\n0.9778466\n1.045780\n\n\nInsulin\n0.9966180\n1.001767\n\n\nBMI\n1.0178269\n1.133537\n\n\nDiabetesPedigreeFunction\n1.3783799\n7.368273\n\n\nAge\n0.9985446\n1.073523\n\n\n\n\n\nNous nous int√©ressons uniquement aux variables ayant un effet significatif sur la probabilit√© de la variable Outcome. Ce sont les variables Glucose, BMI et DiabetesPedigreeFunction.\nOn note ici qu‚Äôune augmentation de 20 de BMI correspond √† \\(1.0730849^{20} = 4.09\\) plus de risque d‚Äôavoir du diab√®te que de ne pas en avoir. Une augmentation de 30 de glucose correspond √† \\(1.0390112^{30} = 3.15\\) plus de risque d‚Äôavoir du diab√®te que de ne pas en avoir. Enfin, une augmentation de 2 de DiabetesPedigreeFunction correspond √† \\(3.1296107^{2} = 9.79\\) plus de risque d‚Äôavoir du diab√®te que de ne pas en avoir.\nNous allons maintenant exhiber des profils d‚Äôindividus particuli√®rement √† risque d‚Äô√™tre diab√©tiques √† l‚Äôaide des coefficients de notre mod√®le de r√©gression logistique.\nPar exemple, un individu particuli√®rement √† risque d‚Äôavoir du diab√®te sera un individu ayant un taux de glucose √©gale √† 160, 40 de BMI et 1.5 de DiabetesPedigreeFunction. Son risque de diab√®te est alors √©gal √† environ 99.9 %.\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             60*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.9994916 \n\n\nCependant, si ce m√™me individu avait eu un BMI de 20 au lieu de 40, son risque de diab√®te aurait grandement diminu√© (71.4 %).\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             30*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.7137806 \n\n\n\n\nM√©thode de s√©lection de variable\nToutes les variables ne contribuent pas n√©cessairement √† expliquer notre variable Y, nous allons voir comment s√©lectionner certaines variables afin de simplifier notre mod√®le, mais aussi pour am√©liorer la classification.\nIci, nous utiliserons l‚ÄôAIC (Crit√®re d‚Äôinformation d‚ÄôAkaike) qui est une mesure de la qualit√© d‚Äôun mod√®le statistique. L‚ÄôAIC va nous permettre de comparer les diff√©rents mod√®les en utilisant un crit√®re de vraisemblance. Il repr√©sente un compromis entre le biais (qui diminue avec le nombre de param√®tres) et la parcimonie (n√©cessit√© de d√©crire les donn√©es avec le plus petit nombre de param√®tres possible).\n\\[AIC = -2 \\cdot \\log(L) + 2 \\cdot k\\]\n\n\\(L\\) est la vraisemblance maximis√©e\n\\(k\\) est le nombre de param√®tres dans le mod√®le\n\\(-2 \\cdot \\log(L)\\) est la d√©viance du mod√®le. Elle p√©nalise par 2 fois le nombre de param√®tres\n\nLa fonction step() de R, nous permet d‚Äôeffectuer une s√©lection descendante. On commence par le mod√®le complet incluant toutes les variables explicatives, puis on retire progressivement les variables qui contribuent le moins √† l‚Äôajustement du mod√®le, jusqu‚Äô√† obtenir un mod√®le optimal.\n\nmodel_final &lt;- step(model_complet)\n\nStart:  AIC=362.02\nOutcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + \n    Insulin + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- BloodPressure             1   344.04 360.04\n- Insulin                   1   344.42 360.42\n- SkinThickness             1   344.45 360.45\n&lt;none&gt;                          344.02 362.02\n- Pregnancies               1   346.24 362.24\n- Age                       1   347.55 363.55\n- BMI                       1   350.89 366.89\n- DiabetesPedigreeFunction  1   351.58 367.58\n- Glucose                   1   396.95 412.95\n\nStep:  AIC=360.04\nOutcome ~ Pregnancies + Glucose + SkinThickness + Insulin + BMI + \n    DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- Insulin                   1   344.42 358.42\n- SkinThickness             1   344.46 358.46\n&lt;none&gt;                          344.04 360.04\n- Pregnancies               1   346.24 360.24\n- Age                       1   347.60 361.60\n- BMI                       1   351.28 365.28\n- DiabetesPedigreeFunction  1   351.67 365.67\n- Glucose                   1   397.31 411.31\n\nStep:  AIC=358.42\nOutcome ~ Pregnancies + Glucose + SkinThickness + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- SkinThickness             1   344.89 356.89\n&lt;none&gt;                          344.42 358.42\n- Pregnancies               1   346.74 358.74\n- Age                       1   347.87 359.87\n- BMI                       1   351.32 363.32\n- DiabetesPedigreeFunction  1   351.90 363.90\n- Glucose                   1   411.11 423.11\n\nStep:  AIC=356.89\nOutcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n&lt;none&gt;                          344.89 356.89\n- Pregnancies               1   347.23 357.23\n- Age                       1   348.72 358.72\n- DiabetesPedigreeFunction  1   352.72 362.72\n- BMI                       1   360.44 370.44\n- Glucose                   1   411.85 421.85\n\n\nLe meilleur mod√®le est celui poss√©dant l‚ÄôAIC le plus faible.\nIci, le meilleur mod√®le inclut les variables Age, Pregnancies, BMI, DiabetesPedigreeFunction et Age.\nCi-dessous les r√©sultats de notre mod√®le.\n\nsummary(model_final)\n\n\nCall:\nglm(formula = Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age, family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8827  -0.6535  -0.3694   0.6521   2.5814  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -9.992080   1.086866  -9.193  &lt; 2e-16 ***\nPregnancies               0.083953   0.055031   1.526 0.127117    \nGlucose                   0.036458   0.004978   7.324 2.41e-13 ***\nBMI                       0.078139   0.020605   3.792 0.000149 ***\nDiabetesPedigreeFunction  1.150913   0.424242   2.713 0.006670 ** \nAge                       0.034360   0.017810   1.929 0.053692 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.89  on 386  degrees of freedom\nAIC: 356.89\n\nNumber of Fisher Scoring iterations: 5\n\n\nMis √† part la variable Pregnancies, tous ont des \\(p-valeur\\) significative. Elles ont toutes un impact positif sur la probabilit√© du patient √† avoir le diab√®te.\nMaintenant, nous calculons les odds-ratio de ce mod√®le.\n\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.087578\n\n\nGlucose\n1.037130\n\n\nBMI\n1.081273\n\n\nDiabetesPedigreeFunction\n3.161077\n\n\nAge\n1.034957\n\n\n\n\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9769517\n1.212971\n\n\nGlucose\n1.0274079\n1.047716\n\n\nBMI\n1.0395095\n1.127321\n\n\nDiabetesPedigreeFunction\n1.4016639\n7.398164\n\n\nAge\n0.9999761\n1.072664\n\n\n\n\n\n\n\nClassification avec le mod√®le de r√©gression logistique\nNous allons maintenant classifier nos donn√©es √† l‚Äôaide de notre mod√®le de r√©gression logistique.\nDans un premier temps, nous allons s√©parer notre jeu de donn√©es en deux sous-√©chantillons.\n\nEchantillon d‚Äôapprentissage : Ce dernier contient 80 % de notre jeu de donn√©es. Notre mod√®le va apprendre √† pr√©dire sur cet √©chantillon.\nEchantillon de test : Cet √©chantillon contient les 20 % restants. Il va nous servir √† tester notre mod√®le et √† comparer les r√©sultats avec les vraies valeurs de cet √©chantillon.\n\n\n# On fixe la graine (resultat reproductible)\nset.seed(75016)\n\n# Nombres d'observations dans notre jeu de donnees\nn &lt;- nrow(diabetes)\n\n# Nombres d'observations a selectionne dans l'echant d'apprentissage\nN &lt;- floor(n*0.8)\n\n# Selection des individus aleatoirement\nidx &lt;- sample(n, N, replace = F)\n\n# Echantillon d'apprentissage (80%)\ndataL &lt;- diabetes[idx,]\n\n# Echantillon de test (20%)\ndataV &lt;- diabetes[-c(idx),]\n\nOn entra√Æne notre mod√®le final (choisi pr√©c√©demment √† l‚Äôaide de l‚ÄôAIC) sur l‚Äô√©chantillon d‚Äôapprentissage et on utilise la fonction predict qui permet de pr√©dire pour tout individu de l‚Äô√©chantillon de test, sa probabilit√© d‚Äô√™tre diab√©tique.\n\n# Entrainement du meilleur modele sur echantillon d'apprentissage (DataL)\nmodel_final_train &lt;- glm(Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + Age , data = dataL, family = \"binomial\")\n\n# Prediction de la probabilite d'etre diabetique sur echantillon de test (DataV)\nprediction_modele &lt;- predict(model_final_train,dataV, type = \"response\")\n\nNous calculons ensuite le taux de mauvaise classification moyen. Ce dernier est tout simplement une comparaison des classifications √† leurs vraies √©tiquettes. On calcule ensuite le pourcentage de donn√©es mal classifi√©es.\nDans notre cas, nous allons cr√©er une r√®gle de classification. Si la pr√©diction est inf√©rieure √† 0.5 alors le patient prendre la valeur 0 (non-diab√©tique) dans le cas contraire, il prendra la valeur 1 (diab√©tique).\n\n# Regle de classification\nprediction_regle &lt;- ifelse(prediction_modele &gt;= 0.5, 1,0)\n\nerreur &lt;- mean(prediction_regle != dataV$Outcome)\npaste0(\"Le taux d'erreur moyen est de \", round(erreur,3)*100,\" %\")\n\n[1] \"Le taux d'erreur moyen est de 25.3 %\"\n\n\n\n\nComparaison de la r√©gression logistique avec l‚Äôalgorithme des k-plus proches voisins\nPr√©c√©demment, nous avons r√©alis√© une classification sur ce m√™me jeu de donn√©es en utilisant l‚Äôalgorithme des \\(k\\) plus proches voisins. La valeur de \\(k\\) optimal √©tait \\(k\\) = 19, c‚Äôest-√†-dire la valeur de \\(k\\) avec laquelle le taux de mauvaise classification √©tait le plus faible.\nNous mettons donc en place notre algorithme des \\(k\\) plus proches voisins avec \\(k\\) = 19, et on r√©alise nos pr√©dictions √† l‚Äôaide de la fonction knn() du package class.\n\nprediction_knn &lt;- knn(train = dataL[,-9], test = dataV[,-9], cl = dataL[,9], k = 19, prob = F)\n# resultat des k plus proches voisins\nprediction_knn\n\n [1] 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0\n[77] 0 0 0\nLevels: 0 1\n\n\nOn calcule ensuite notre taux d‚Äôerreur moyen.\n\nerreur_knn &lt;- mean(prediction_knn != dataV[,9])\npaste0(\"Le taux d'erreur moyen est de \", round(erreur_knn,3)*100, \" %\")\n\n[1] \"Le taux d'erreur moyen est de 20.3 %\"\n\n\nMaintenant, que nous avons r√©alis√© nos pr√©dictions avec nos deux m√©thodes, nous sommes en mesure de comparer les deux taux d‚Äôerreur moyens. Le taux d‚Äôerreur le plus bas est √©videmment pr√©f√©rable. Ici, les taux d‚Äôerreur moyens sont proches bien que celui de la r√©gression logistique est plus √©lev√©e (25.3 %) que celui de l‚Äôalgorithme des \\(k\\) plus proches voisins (20.3 %).\n\n\nConclusion\nEnfin, nous allons faire varier le seuil utilis√© dans le crit√®re de classification et calculer la sensibilit√© et la sp√©cificit√© pour chacune des valeurs du seuil possible.\n\n# Evaluation de 100 seuils differents\nl &lt;- 100\n\n# Predicition\npreds &lt;- predict(model_final_train, dataV, type = \"response\")\n\n# Sequence de seuils\nc &lt;- seq(1.001 * min(preds), 0.999 * max(preds), length.out = l)\n\n# Initialisation des vecteurs sensibilite et specificite\nSe &lt;- rep(NA,l)\nSp &lt;- rep(NA,l)\n\nfor (j in 1:l) {\n  mod.final.classif &lt;- (preds &gt;= c[j])  # Classe (TRUE ou FALSE) de la pr√©diction\n  pt &lt;- table(mod.final.classif, dataV$Outcome)  # Comparaison des classes pr√©dites\n  \n  if (nrow(pt) &gt;= 2 && ncol(pt) &gt;= 2) {\n\n    Se[j] &lt;- prop.table(pt, margin=2)[2, 2]  # Sensibilit√©\n    Sp[j] &lt;- prop.table(pt, margin=2)[1, 1]  # Sp√©cificit√©\n  } else {\n\n    Se[j] &lt;- NA\n    Sp[j] &lt;- NA\n  }\n}\n\npar(mfrow=c(1,2))\nplot(c,Se,main=\"Sensibilit√©\",type='s', col = \"orange\")\nplot(c,Sp,main=\"Specificit√©\",type='s', col = \"purple\")\n\n\n\n\n\n\n\n\nSur les deux graphiques ci-dessus, la courbe de sensibilit√© montre comment le taux de d√©tection des diab√©tiques varie en fonction des diff√©rents seuils de classification. Tandis que la courbe de sp√©cificit√© montre comment le taux de d√©tection des non-diab√©tiques varie √©galement en fonction des diff√©rents seuils de classification.\nOn cherche √† maximiser la sensibilit√© et la sp√©cificit√©. Cependant une augmentation de la sensibilit√© peut entra√Æner une diminution de la sp√©cificit√© et vice versa.\nEn somme, ces deux mesures permettent de d√©terminer √† quel point le mod√®le est efficace dans la classification des cas positifs et n√©gatifs, et d‚Äôajuster les seuils de d√©cision en fonction des besoins sp√©cifiques.\nPuis, nous pouvons calculer la courbe ROC. Elle sert √† √©valuer la performance d‚Äôun mod√®le de classification binaire et en particulier les mod√®les qui pr√©disent une probabilit√©. Elle repr√©sente la sensibilit√© en fonction de 1 ‚Äì sp√©cificit√© pour toutes les valeurs seuils possibles du marqueur √©tudi√©.\n\npar(mfrow=c(1,1))\nplot(1-Sp,Se,type='s',main=\"Courbe ROC\")\nabline(0,1,col='blue')\n\n\n\n\n\n\n\n\nNotre classification est plut√¥t bonne au vu de la courbe ROC. La courbe est assez proche du coin sup√©rieur gauche du graphique, ce qui indique un taux √©lev√© de vrais positifs (sensibilit√©). Notre mod√®le arrive assez bien √† identifier les personnes diab√©tiques en minimisant les erreurs de classification des cas n√©gatifs. De plus, la courbe est largement sup√©rieure √† la courbe 0,1."
  },
  {
    "objectID": "projets/fontaines_eau.html",
    "href": "projets/fontaines_eau.html",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "",
    "text": "Ce mini-projet utilise le langage R pour cr√©er une carte interactive des points d‚Äôeau publics dans la ville de Paris. Il inclut √©galement une application Shiny, permettant une exploration dynamique et interactive des donn√©es."
  },
  {
    "objectID": "projets/fontaines_eau.html#aper√ßu-de-lapplication",
    "href": "projets/fontaines_eau.html#aper√ßu-de-lapplication",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "1 Aper√ßu de l‚Äôapplication",
    "text": "1 Aper√ßu de l‚Äôapplication\n\n\n\n\n\nL‚Äôobjectif principal de ce projet est de proposer une visualisation g√©ographique claire et accessible des points d‚Äôeau r√©partis dans Paris, gr√¢ce √† une interface conviviale d√©velopp√©e avec Shiny. Les donn√©es exploit√©es proviennent de la Ville de Paris et sont accessibles sur le site data.gouv. Cette application, √† la fois fluide et interactive, permet aux utilisateurs d‚Äôexplorer facilement les diff√©rents points d‚Äôeau pr√©sents dans chaque arrondissement de la capitale."
  },
  {
    "objectID": "projets/fontaines_eau.html#acc√©dez-√†-lapplication",
    "href": "projets/fontaines_eau.html#acc√©dez-√†-lapplication",
    "title": "O√π boire de l‚Äôeau √† Paris ?",
    "section": "2 Acc√©dez √† l‚Äôapplication",
    "text": "2 Acc√©dez √† l‚Äôapplication\nAcc√©der √† l‚Äôapplication ici\nRepo github"
  },
  {
    "objectID": "projets/projets_but/migration.html",
    "href": "projets/projets_but/migration.html",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "",
    "text": "Ce projet vise √† migrer des donn√©es d‚Äôun environnement SQL vers un environnement NoSQL. Concr√®tement, il s‚Äôagit de transf√©rer les informations stock√©es dans une base de donn√©es relationnelle traditionnelle, o√π les donn√©es sont organis√©es en tables avec des relations fixes, vers une base de donn√©es NoSQL, qui offre une structure plus flexible adapt√©e aux donn√©es non structur√©es ou semi-structur√©es.\nNous travaillons avec les donn√©es d‚Äôune entreprise de voitures qui rencontre des probl√®mes avec sa base de donn√©es actuelle : les requ√™tes sont lentes et des d√©faillances serveur entra√Ænent des pertes de donn√©es. Pour r√©soudre ces probl√®mes, nous avons d√©cid√© de passer √† un environnement NoSQL. Cette technologie permet de stocker des donn√©es sous une forme non structur√©e, offrant ainsi plus de flexibilit√© et de performance. Cette migration vise √† am√©liorer la performance des requ√™tes et √† pr√©parer l‚Äôinfrastructure pour une croissance future.\nLe d√©p√¥t GitHub contenant le rapport complet du projet, ainsi que les requ√™tes SQL et NoSQL associ√©es, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/migration.html#base-de-donn√©es-relationnelle",
    "href": "projets/projets_but/migration.html#base-de-donn√©es-relationnelle",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Base de donn√©es relationnelle",
    "text": "Base de donn√©es relationnelle\nLa base de donn√©es relationnelle initiale contient des informations sur les v√©hicules, les clients, les commandes, les employ√©s‚Ä¶ La repr√©sentation des donn√©es est claire et bien organis√©e. Chaque table dispose de relations avec d‚Äôautres tables, ce qui permet de structurer efficacement les informations et de faciliter les requ√™tes complexes.\n\n\n\nSch√©ma relationnel de la bdd initial\n\n\nDans un premier temps, nous avons cr√©√© des requ√™tes SQL sur cette base de donn√©es. Ces requ√™tes serviront de tests pour √©valuer le succ√®s de la migration. Nous comparerons les r√©sultats obtenus dans la base de donn√©es relationnelle avec ceux obtenus dans la base NoSQL pour v√©rifier l‚Äôint√©grit√© et la performance de la migration.\n\n\n\nRequ√™tes SQL\n\n\nLa base de donn√©es est au format SQLite. Nous avons import√© le module sqlite3 en Python pour √©tablir la connexion et interagir avec la base. Ensuite, nous avons utilis√© la biblioth√®que Pandas et notamment sa fonction read_sql_query(), pour ex√©cuter et lire les r√©sultats des requ√™tes SQL."
  },
  {
    "objectID": "projets/projets_but/migration.html#algorithme-de-migration",
    "href": "projets/projets_but/migration.html#algorithme-de-migration",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Algorithme de migration",
    "text": "Algorithme de migration\nIl existe plusieurs types de bases de donn√©es NoSQL (Cl√©-valeur, Document, Colonne et Graphe), chacun adapt√© √† ses propres cas d‚Äôusage et ayant ses propres avantages et inconv√©nients, notamment en termes de scalabilit√© et de flexibilit√©. Le choix d√©pend donc de plusieurs facteurs cl√©s, comme la structure des donn√©es, les exigences de performance‚Ä¶\nEn ce qui nous concerne, nous pouvons r√©aliser une migration vers un environnement NoSQL, car l‚Äôentreprise dispose d‚Äôune grande quantit√© de donn√©es structur√©es en constante croissance. De plus, il est possible d‚Äôam√©liorer significativement les performances d‚Äôacc√®s aux donn√©es en optimisant le traitement de donn√©es plus importantes et en r√©duisant le temps de latence. On souhaite donc une solution qui offre plus de flexibilit√© et √©volutivit√©, tout en pr√©servant l‚Äôint√©grit√© des donn√©es de notre base initial.\nApr√®s m√ªre r√©flexion, le format document est celui s‚Äôadaptant le mieux √† notre objectif. En effet, il permet de structurer naturellement les entit√©s de mani√®re hi√©rarchique. Par exemple, un client peut √™tre repr√©sent√© par un document contenant ses commandes, chaque commande incluant les produits associ√©s. De plus, il offre une grande flexibilit√©, permettant de traiter diff√©rents types de donn√©es sans modifications complexes du sch√©ma. Enfin, il permet une scalabilit√© horizontale gr√¢ce √† la partition de document, c-√†-d si les donn√©es augmentent, on peut facilement ajouter de nouveaux serveurs pour stocker et g√©rer plus de documents, sans tout restructurer.\n\n\n\nExemple d‚Äôune mod√©lisation au format Document\n\n\nCe mod√®le pr√©sente quelques inconv√©nients, notamment des performances limit√©es pour les requ√™tes complexes ou les jointures entre documents. De plus, les mises √† jour simultan√©es de documents imbriqu√©s peuvent √™tre plus difficiles √† g√©rer.\nNous avons d√©cid√© de structurer nos donn√©es autour de quatres collections : customers, payments, orders et employees."
  },
  {
    "objectID": "projets/projets_but/migration.html#script-de-migration",
    "href": "projets/projets_but/migration.html#script-de-migration",
    "title": "Migration de donn√©es : De SQL √† NoSQL",
    "section": "Script de migration",
    "text": "Script de migration\nAvant de d√©velopper un script de migration, nous avons d‚Äôabord √©tabli un pseudo-algorithme dans l‚Äôobjecitf de structurer et organiser la logique de notre programme.\nEnsuite, pour la migration de nos donn√©es, nous avons utilis√© SQLite comme source et MongoDB comme destination, en exploitant les biblioth√®ques Python sqlite3, pymongo et pandas. Le processus inclut l‚Äôextraction des donn√©es de SQLite, leur transformation au format document compatible avec MongoDB, et leur insertion dans les collections appropri√©es."
  },
  {
    "objectID": "index.html#formation",
    "href": "index.html#formation",
    "title": "Rachid SAHLI",
    "section": "Formation",
    "text": "Formation\n\n\n\n\n\n\nIUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des donn√©es | Sept 2022 - Juin 2025\nCours suivis : Alg√®bre, Statistique, Probabilit√©s, Programmation, Base de donn√©es, √âconomie"
  },
  {
    "objectID": "index.html#exp√©rience",
    "href": "index.html#exp√©rience",
    "title": "Rachid SAHLI",
    "section": "Exp√©rience",
    "text": "Exp√©rience\n\n\n\n\n\n\nInstitut national de la statistique et des √©tudes √©conomiques (INSEE) | Montrouge, France\n\n\n\nProgrammeur Statistique | 2023 - 2025 Au sein de la Direction des statistiques d√©mographiques et sociales, j‚Äôai men√© des travaux d‚Äôappariement de donn√©es administratives. L‚Äôobjectif de ces travaux √©tait de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j‚Äôai compar√© diff√©rents algorithmes d‚Äôappariement √† l‚Äôaide d‚Äôanalyses statistiques.\nMes missions ont inclus la manipulation de donn√©es brutes, le nettoyage des donn√©es, l‚Äôappariement de grands volumes de donn√©es, ainsi que leur analyse statistique."
  },
  {
    "objectID": "index.html#iut-de-paris---rives-de-seine-universit√©-paris-cit√©-paris-france",
    "href": "index.html#iut-de-paris---rives-de-seine-universit√©-paris-cit√©-paris-france",
    "title": "Rachid SAHLI",
    "section": "IUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, France",
    "text": "IUT de Paris - Rives de Seine (Universit√© Paris Cit√©) | Paris, France\nBachelor universitaire de technologie en Science des donn√©es | Sept 2022 - Juin 2025\nCours suivis : Alg√®bre, Statistique, Probabilit√©s, Programmation, Base de donn√©es, √âconomie"
  },
  {
    "objectID": "index.html#institut-national-de-la-statistique-et-des-√©tudes-√©conomiques-insee-montrouge-france",
    "href": "index.html#institut-national-de-la-statistique-et-des-√©tudes-√©conomiques-insee-montrouge-france",
    "title": "Rachid SAHLI",
    "section": "Institut national de la statistique et des √©tudes √©conomiques (INSEE) | Montrouge, France",
    "text": "Institut national de la statistique et des √©tudes √©conomiques (INSEE) | Montrouge, France\nStatisticien | Sept 2023 - Sept 2025 Au sein de la Direction des statistiques d√©mographiques et sociales, j‚Äôai men√© des travaux d‚Äôappariement de donn√©es administratives. L‚Äôobjectif de ces travaux √©tait de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j‚Äôai compar√© diff√©rents algorithmes d‚Äôappariement √† l‚Äôaide d‚Äôanalyses statistiques.\nMes missions ont inclus la manipulation de donn√©es brutes, le nettoyage des donn√©es, l‚Äôappariement de grands volumes de donn√©es, ainsi que leur analyse statistique."
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html",
    "href": "projets/projets_but/essai_clinique.html",
    "title": "Etude statistique dans un essai clinique",
    "section": "",
    "text": "Introduction\nCe projet consiste en la r√©alisation d‚Äôune √©tude statistique dans le cadre d‚Äôun essai clinique. Nous travaillons sur un jeu de donn√©es simul√©es pour effectuer l‚Äôanalyse statistique d‚Äôune √©tude de phase 3. L‚Äôobjectif est de fournir au laboratoire une Autorisation de Mise sur le March√© (AMM) avec une indication dans la prise en charge de la dr√©panocytose.\nLa dr√©panocytose est une maladie g√©n√©tique qui affecte les globules rouges et peut entra√Æner des complications graves. Elle se manifeste notamment par une an√©mie, des crises douloureuses et un risque accru d‚Äôinfections.\n\n\n\nSch√©ma de la dr√©panocytose\n\n\nLors d‚Äôune √©tude de phase 3, les chercheurs comparent un nouveau traitement prometteur au traitement standard, qui est le traitement reconnu et g√©n√©ralement administr√© pour une affection ou une maladie. Dans notre √©tude, ce traitement s‚Äôappelle le Voxelotor.\nNotre objectif est d‚Äô√©valuer l‚Äôeffet du voxelotor en mesurant l‚Äôam√©lioration du taux d‚Äôh√©moglobine chez les patients, compar√© √† un placebo. Durant notre √©tude statistique, nous testerons diverses hypoth√®ses et utiliserons des m√©thodes statistiques pour √©valuer l‚Äôefficacit√© de ce traitement.\nLe rapport peut √™tre consult√© ici. Vous trouverez √©galement le code R ici.\n\n\nM√©thodologie\n\n\nR√©sultat\n\n\nConclusion"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html",
    "href": "projets/projets_but/serie_temp_charbon.html",
    "title": "S√©rie Temporelle : Production de charbon aux √âtats-Unis",
    "section": "",
    "text": "Introduction\n\n\nM√©thodologie\n\n\nR√©sultat"
  }
]