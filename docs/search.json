[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Trier par\n       Ordre par défaut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus récent\n        \n         \n          Auteur·rice\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeut-on prédire les survivants du Titanic ?\n\n\n\n\n\n\nPython\n\n\nMachine learning\n\n\nScikit-Learn\n\n\n\nBien que la chance ait joué un rôle dans la survie des passagers, certaines catégories de personnes semblent avoir eu un avantage. Est-il possible de prédire ces chances de survie à l’avance ?\n\n\n\n\n\n1 mars 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRégression logistique en pratique\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication d’un modèle de régression logistique pour prédire si un patient est atteint de diabète. Le modèle est également comparé à la méthode des \\(k\\) plus proches voisins.\n\n\n\n\n\n10 oct. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nL’algorithme des \\(k\\) plus proches voisins\n\n\n\n\n\n\nR\n\n\nMachine learning\n\n\n\nApplication sur R de l’algorithme des \\(k\\) plus proches voisins.\n\n\n\n\n\n29 sept. 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarte d’aide pour les Volontaires de Paris 2024\n\n\n\n\n\n\nR\n\n\nOpendata\n\n\n\nPlus de 45 000 volontaires ont contribué au bon déroulement des Jeux Olympiques. Venus des quatre coins de la France, cette carte interactive leur permet de se préparer efficacement et d’accomplir au mieux leur mission.\n\n\n\n\n\n9 sept. 2024\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "blog/carte_volontaires.html",
    "href": "blog/carte_volontaires.html",
    "title": "Carte d’aide pour les Volontaires de Paris 2024",
    "section": "",
    "text": "Lors des Jeux Olympiques de 2024 à Paris, plus de 45 000 volontaires ont été les véritables hommes et femmes de l’ombre. On les a vus partout à Paris grâce à leur tenue bleue. Ils ont contribué au succès des Jeux Olympiques. Venant des quatre coins de la France, ils devaient pouvoir se repérer. La carte d’aide pour les volontaires est conçue à cet effet et leur a été très utile durant leur séjour à Paris.\n\n\n\n\n\n\n Cette carte a été créée à l’aide du package leaflet, en utilisant les données disponibles en open data fournies par les acteurs suivants :\n\nFontaines à eau : opendata de la Ville de Paris\nToilettes publiques : opendata de la Ville de Paris\nDistributeurs automatiques de billets : Opendatasoft hub\nParkings vélo : opendata de Paris 2024\nSite de compétitions des Jeux Olympiques\nParalympiques : opendata de Paris 2024\n\n(Denière mis à jour des données le 27 juillet 2024)"
  },
  {
    "objectID": "blog/knn.html",
    "href": "blog/knn.html",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "",
    "text": "Le but de l’apprentissage supervisé est de prévoir l’étiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (régression) associée à une nouvelle entrée \\(X\\), où il est sous-entendu que (\\(X,Y\\)) est une nouvelle réalisation des données, indépendante de l’échantillon observé.\nL’algorithme des \\(k\\) plus proches voisins est une méthode d’apprentissage supervisé. On peut l’utiliser pour classifier quand \\(Y_i\\) est une variable qualitative, les \\(Y_i\\) sont appelés étiquettes. On peut également l’utiliser pour prédire si \\(Y_i \\in \\mathbb{R}\\). C’est donc une régression et les \\(Y_i\\) sont appelés variables à expliquer.\nRemarque : On parle d’apprentissage supervisé car pour chaque \\(X_i\\) de l’échantillon d’apprentissage on dispose de \\(Y_i\\), l’étiquette. Au contraire, on parlera d’apprentissage non-supervisé lorsque l’échantillon est simplement constitué des \\(X_i\\)."
  },
  {
    "objectID": "blog/knn.html#analyse-descriptive",
    "href": "blog/knn.html#analyse-descriptive",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "Analyse descriptive",
    "text": "Analyse descriptive\nOn procède à une succinte analyse descritpive de notre jeu de données.\n\nsummary(abalone)\n\n     Length          Diameter         Height        Whole_weight   \n Min.   :0.0750   Min.   :0.055   Min.   :0.0100   Min.   :0.0020  \n 1st Qu.:0.4500   1st Qu.:0.350   1st Qu.:0.1150   1st Qu.:0.4421  \n Median :0.5450   Median :0.425   Median :0.1400   Median :0.8000  \n Mean   :0.5241   Mean   :0.408   Mean   :0.1396   Mean   :0.8291  \n 3rd Qu.:0.6150   3rd Qu.:0.480   3rd Qu.:0.1650   3rd Qu.:1.1538  \n Max.   :0.8150   Max.   :0.650   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1861   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3595   Mean   :0.1807   Mean   :0.2389   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3289   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n\n\nOn observe ci-dessous la distribution de la variable Rings ainsi que la relation entre la longueur et la taille des ormeaux.\n\nggplot(abalone, aes(x = Rings)) +\n  geom_histogram(fill = \"blue\") +\n  labs(title = \"Distribution de Rings\", y = \"Fréquence\", x = \"Rings\") + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(abalone, aes(x = Length, y = Height)) +\n  geom_point(col = \"blue\", pch = 1) +\n  labs(title = \"Relation entre Height et Length\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/knn.html#prédicition-de-la-variable-rings",
    "href": "blog/knn.html#prédicition-de-la-variable-rings",
    "title": "L’algorithme des \\(k\\) plus proches voisins",
    "section": "Prédicition de la variable Rings",
    "text": "Prédicition de la variable Rings\nComme dans la partie précédente, nous commençons par créer deux sous-échantillons distincts (échantillon d’apprentissage et echantillon de test) à partir du jeu de données complet.\n\nN = round((80/100)*nrow(abalone)) # Calcul du nombre d'observations a sélectionner (80 %) \nidx1 &lt;- sample(1:nrow(abalone), size = N, replace = FALSE) # Tirage aleatoire des indices qu'on va sélectionner\ndataL &lt;- abalone[idx1,] # Construction du dataset d'apprentissage\ndataV &lt;- abalone[-idx1,] # Construction du dataset de test ou de validite\n\nA présent, on utilise la fonction kknn() pour mettre en oeuvre notre algorithme de prédiction en fixant \\(k = 3\\).\n\npred &lt;- kknn(Rings ~., dataL, dataV, k = 3, kernel = 'rectangular')\n\nCi-dessous, nous observons nos prédictions en fonction de la variable Rings.\n\nplot(dataV$Rings,pred$fitted.values, xlab = \"Rings\", ylab = \"Prediction\", col = \"blue\")\nabline(0,1, col = \"red\")\n\n\n\n\n\n\n\n\nContrairement à la classification, nous utiliserons l’erreur quadratique moyenne pour mesurer la performance de notre modèle sur l’échantillon de test.\n\nmse &lt;- mse(pred$fitted.values, dataV$Rings)\npaste0(\"Erreur quadratique moyenne = \",mse)\n\n[1] \"Erreur quadratique moyenne = 5.67212242182302\"\n\n\nEnfin, nous allons identifier la valeur de \\(k\\) pour laquelle l’erreur quadratique moyenne est la plus faible. On pourra alors déterminer le niveau optimal de \\(k\\) afin d’améliorer la précision du modèle. La boucle suivante permet de calculer l’erreur quadratique moyenne pour chaque valeur de \\(k\\) sur notre échantillon.\n\nkvec &lt;- 1:100\nerror &lt;- rep(NA, length(kvec))\n\nfor(i in 1:length(kvec)){\n  pred &lt;- kknn(Rings ~., dataL, dataV, k = i, kernel = 'rectangular')\n  error[i] &lt;- mse(dataV$Rings, pred$fitted.values)\n}\n\nOn visualise les résultats sur le graphique ci-dessous.\n\nplot(kvec, error, type = \"b\", col = \"orange\")\nmin_error_niveau &lt;- which.min(error)\nabline(v = kvec[min_error_niveau], col = \"red\", lty = 2)\nlegend(\"topright\", legend = paste(\"Erreur min à k =\", kvec[min_error_niveau]), col = \"red\", lty = 2)"
  },
  {
    "objectID": "projets.html",
    "href": "projets.html",
    "title": "Projets",
    "section": "",
    "text": "Trier par\n       Ordre par défaut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus récent\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nProjets BUT SD\n\n\n\nScience des données\n\n\nStatistique\n\n\n\nVoici une liste de mes projets réalisées durant mes 3 années à l’Institut Universitaire de Technologie de Paris.\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html",
    "href": "projets/projets_but/serie_temp_charbon.html",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "",
    "text": "L’objectif est d’analyser et de modéliser statistiquement une série temporelle liée à la production d’électricité par combustion aux États-Unis entre 2001 et 2022. Nous étudions l’évolution de cette série au fil du temps afin d’identifier ses principales tendances et composantes. Cette série est issue du site web de l’Administration américaine de l’information sur l’énergie. Nous réalisons ensuite une application Shiny afin de visualiser les résultats interactivement.\nLe dépot github du projet contenant le code est disponible ici."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html#tendance-de-la-série",
    "href": "projets/projets_but/serie_temp_charbon.html#tendance-de-la-série",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "Tendance de la série",
    "text": "Tendance de la série\nAfin de modéliser la série, nous commençons par analyser la tendance. Nous utilisons un filtre de moyennes mobiles simples et centrées, ainsi qu’une régression des moyennes annuelles, dans le but de visualiser l’évolution générale de la série.\nLes moyennes mobiles permettent de lisser la série et d’atténuer les fluctuations aléatoires. La moyenne mobile simple est calculée sur une fenêtre fixe, tandis que la moyenne mobile centrée est déterminée de manière symétrique autour de chaque point. Cela permet d’identifier la tendance sous-jacente de la série entre 2001 et 2022, qui s’est avérée être décroissante. On se sert également de la moyenne mobile pour éléminer la composante saisonnière de période \\(p\\) de notre série et réduire au maximum l’amplitude des fluctuations irrégulières.\nLa moyenne mobile qui s’ajuste le mieux à la série est celle d’ordre 12.\nDe plus, la courbe de régression des moyennes annuelles permet de modéliser la tendance à long terme de la série en s’appuyant sur les moyennes calculées pour chaque année. On trace sur notre nuage de point une courbe linéaire qui nous indique une tendance décroissante de la production d’éléctricité par combustion."
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html#décomposition-de-la-série",
    "href": "projets/projets_but/serie_temp_charbon.html#décomposition-de-la-série",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "Décomposition de la série",
    "text": "Décomposition de la série\nÀ présent, nous procédons à la décomposition de notre série à l’aide d’un modèle additif. Il s’écrit de la manière suivante :\n\\[ y_i = f_i + s_i + e_i \\quad \\text{pour } i = 1, \\dots, n \\quad \\text{avec }  \\sum_{j=1}^{p} s_j = 0 \\quad \\text{et} \\quad \\sum_{j=1}^{n} e_j = 0 \\]\nDans ce modèle, l’amplitude de la composante saisonnière et du bruit reste constante au cours du temps. Cela se traduit graphiquement par des fluctuations autour de la tendance d’amplitude constante. L’utilisation de la méthode de la bande, qui consiste à tracer la courbe reliant les minima sur une période ainsi que celle reliant les maxima, a montré que ces deux courbes sont parallèles, indiquant ainsi que le modèle est additif.\nOn rappelle qu’une série chronologique résulte de trois composantes fondamentales : la tendance (\\(f_i\\)), la composante saisonnière (\\(s_i\\)) ou saisonnalité, et la composante résiduelle (\\(e_i\\)).\nNous analysons d’abord l’effet des saisons en calculant les coefficients de variation saisonnière (CVS). Ces variations, qui reviennent chaque année à la même période, sont influencées par des phénomènes naturels ou économiques (comme la météo ou les fêtes) mais n’ont pas d’impact durable sur la tendance.\nEnfin, nous nous intéressons à la composante résiduelle, qui regroupe toutes les variations non expliquées par la tendance et la saisonnalité. Ces fluctuations peuvent être dues à des événements imprévus (crises économiques, accidents, conditions exceptionnelles) ou à des variations aléatoires.\n\n\n\nDécomposition de la série"
  },
  {
    "objectID": "projets/projets_but/serie_temp_charbon.html#prévision-de-la-série",
    "href": "projets/projets_but/serie_temp_charbon.html#prévision-de-la-série",
    "title": "Série Temporelle : Production d’éléctricité par combustion aux États-Unis",
    "section": "Prévision de la série",
    "text": "Prévision de la série\nL’objectif de cette étape est de réaliser deux prévisions (2022 et 2023) de la production d’électricité par combustion en utilisant trois méthodes différentes : Trend + Saison, ARMA et Holt-Winters.\nLa méthode Trend + Saison, identifie deux composantes principales dans notre série qui sont la tendance et la saisonnalité. La méthode ARMA repose sur l’idée que la production d’électricité à un instant donné dépend des valeurs passées et des erreurs passées. Pour garantir des prévisions optimales, nous avons sélectionné les paramètres du modèle ARMA en fonction des performances statistiques obtenues. Enfin, le modèle Holt-Winters est une extension de lissage exponentiel qui prend en compte trois éléments, la tendance, la saisonnalité et la moyenne ajustée de la série.\nLa prévision sur les données de 2022 a permis de comparer les valeurs estimées avec les valeurs réelles de la série, afin d’évaluer la précision des trois méthodes utilisées. Pour cela, nous avons calculé l’erreur quadratique moyenne (RMSE) pour chacune des méthodes, ce qui nous a permis d’identifier celle offrant les prévisions les plus fiables.\nParmi les trois approches testées, la méthode Trend + Saison s’est révélée être la plus performante, car elle a obtenu l’erreur quadratique moyenne la plus faible. Cela s’explique par sa capacité à capturer à la fois la tendance de long terme et les variations saisonnières, qui jouent un rôle clé dans l’évolution de la production d’électricité. En revanche, les modèles ARMA et Holt-Winters ont montré des écarts plus importants avec les valeurs réelles, probablement en raison de leurs hypothèses sous-jacentes qui s’adaptent moins bien aux dynamiques spécifiques de notre série temporelle."
  },
  {
    "objectID": "projets/projets_but/louvre.html",
    "href": "projets/projets_but/louvre.html",
    "title": "Nocturne au musée du Louvre",
    "section": "",
    "text": "Tous les vendredis, le musée du Louvre offre un moment de magie à ses visiteurs au milieu de ses collections. Au programme de ces nocturnes hebdomadaires, de nombreuses activités pour petits et grands. Le 24 novembre 2023, mes camarades de l’IUT et moi avons pu participer à l’une de ces soirées. Nous avons décidé de présenter certains tableaux du célèbre musée au filtre de l’IA. Au moment où cette technologie prend une place active dans notre société, avec par exemple ChatGPT, DALL-E ou encore Mistral AI, il est intéressant de pouvoir approfondir l’utilisation de ces outils permettant de générer des éléments en lien, ici, avec l’art, par exemple. De plus, ce sujet fascinant qu’est l’intelligence artificielle est plus ou moins en lien avec notre formation en science des données. Explorer ce domaine où se mêlent mathématiques, informatique et données était particulièrement captivant.\n\nQu’avons nous fait ?\nL’objectif final était de pouvoir le 23 novembre 2024, présenter une œuvre en rapport avec l’IA au visiteur de la nocturne. Pour cela, 2 mois, auparavant, nous avons décidé de choisir un sujet d’intelligence artificielle qui pouvait coïncider avec une œuvre du musée. Après de longues recherches passionnantes, nous avons choisi de présenter les GAN (Generative adversarial networks). C’est une classe d’algorithmes d’apprentissage non supervisé. Ils permettent de générer des images avec un fort degré de réalisme. Nous avons trouvé la technologie et les méthodes très intéressantes, d’autant plus qu’elles aboutissent à des applications concrètes, telles que la génération d’images artistiques. Cependant, ces algorithmes sont utilisés dans bien d’autres domaines tels que la médecine ou encore la finance. Mais concrètement, comment ça marche ?\n\n\nQu’est-ce qu’un GAN ?\nAfin de comprendre comment il fonctionne, on peut imaginer un jeu entre deux joueurs :\n\nL’artiste (Générateur) : L’objectif du joueur est de créer des images qui se rapprochent le plus de la réalité. Pour cela, il apprend de ses erreurs et réessaie en continu d’obtenir l’œuvre la plus proche du réel.\nLe juge (Discriminateur) : L’objectif de ce joueur est de vérifier si les œuvres réalisées par l’artiste peuvent paraître réelles ou si elles sont encore trop fausses. Pour cela, il compare les œuvres de l’artiste à des œuvres réelles faites par des peintres.\n\nTant que l’image n’est pas accepté par le juge, l’artiste continue à produire des œuvres. Voilà, le fonctionnement d’un GAN ou deux réseaux de neurones se font concurrence. De cette manière, il est possible de créer des images, des vidéos ou d’autres contenues de très bonnes qualités.\n\n\n\n\n\n\n\nSchéma GAN\n\n\n\n\nPrésentation au public\nNous avons décidé de présenter au public le travail d’Obvious. Ce collectif de chercheurs, d’artistes travaille avec des modèles d’apprentissage profond pour explorer le potentiel créatif de l’intelligence artificielle. Ils ont justement utilisé des GAN pour générer une famille de 11 tableaux (la famille Belamy). \nUn portrait a retenu notre attention, le portrait d’Edmond de Belamy. Ce tableau est une impression sur toile qui est rentrée dans l’histoire de l’art moderne. Cela, car c’est la première œuvre d’art produite par un logiciel d’intelligence artificielle à être présentée dans une salle des ventes. Pour couronner le tout, il a été vendu 432 500 dollars chez Christie’s le 25 octobre 2018.  De plus, ce tableau est assez troublant. Il est très difficile à première vue de déterminer qu’une machine a pu en être l’auteur.   Obvious à utiliser un GAN, en l’entraînant sur 15 000 portraits classiques réalisés entre le 14e et 20e siècle. L’algorithme devait donc produire un tableau en sortie qui serait très ressemblant aux portraits classiques. Nous avons décidé de comparer le portrait d’Edmond de Belamy à une œuvre du Louvre se trouvant dans la salle 846 de l’aile Richelieu du musée. C’est une peinture datant du 17e siècle réalisée par Jean Bray, peintre néerlandais. Les tableaux ont quelques points en commun : le fond noir, un homme au centre du tableau, le col blanc avec une veste noir.\n\nPortrait de BelamyPortrait de Jean de Bray\n\n\n\n\n\n\n\n\n\nPortrait d’Edmond de Belamy, Collectif Obvious\n\n\n\n\n\n\n\n\n\n\n\nPortrait d’homme, 1658\n\n\n\n\n\nLes visiteurs étaient agréablement surpris par le réalisme du portrait d’Edmond de Belamy, mais aussi par le prix de vente de l’œuvre. Ils pensaient pouvoir reconnaître la réalisation d’une IA."
  },
  {
    "objectID": "projets/but_sd.html",
    "href": "projets/but_sd.html",
    "title": "Projets BUT SD",
    "section": "",
    "text": "Etude statistique dans un essai clinique\n\n\n\nR\n\n\nStatistique\n\n\n\nRéalisation d’une étude statistique dans un essai clinique qui permettera de déterminer l’effet d’un nouveau médicament chez les patients atteint de drépanocytose.\n\n\n\n1 déc. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMigration de données : De SQL à NoSQL\n\n\n\nSQL\n\n\n\nCe projet vise à migrer des données d’un environnement SQL vers un environnement NoSQL pour une entreprise automobile.\n\n\n\n28 déc. 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNocturne au musée du Louvre\n\n\n\nIA\n\n\n\nPrésentation d’une œuvre générée par une IA aux visiteurs du Musée du Louvre.\n\n\n\n24 nov. 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSérie Temporelle : Production d’éléctricité par combustion aux États-Unis\n\n\n\nR\n\n\nRShiny\n\n\nStatistique\n\n\n\nCe projet consiste en l’analyse de la production d’éléctricité par combustion aux États-Unis entre 2001 et 2022. Nous réalisons également une prévision pour l’année 2023.\n\n\n\n19 janv. 2024\n\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "projets/fontaines_eau.html",
    "href": "projets/fontaines_eau.html",
    "title": "Où boire de l’eau à Paris ?",
    "section": "",
    "text": "Ce mini-projet utilise le langage R pour créer une carte interactive des points d’eau publics dans la ville de Paris. Il inclut également une application Shiny, permettant une exploration dynamique et interactive des données."
  },
  {
    "objectID": "projets/fontaines_eau.html#aperçu-de-lapplication",
    "href": "projets/fontaines_eau.html#aperçu-de-lapplication",
    "title": "Où boire de l’eau à Paris ?",
    "section": "1 Aperçu de l’application",
    "text": "1 Aperçu de l’application\n\n\n\n\n\nL’objectif principal de ce projet est de proposer une visualisation géographique claire et accessible des points d’eau répartis dans Paris, grâce à une interface conviviale développée avec Shiny. Les données exploitées proviennent de la Ville de Paris et sont accessibles sur le site data.gouv. Cette application, à la fois fluide et interactive, permet aux utilisateurs d’explorer facilement les différents points d’eau présents dans chaque arrondissement de la capitale."
  },
  {
    "objectID": "projets/fontaines_eau.html#accédez-à-lapplication",
    "href": "projets/fontaines_eau.html#accédez-à-lapplication",
    "title": "Où boire de l’eau à Paris ?",
    "section": "2 Accédez à l’application",
    "text": "2 Accédez à l’application\nAccéder à l’application ici\nRepo github"
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html",
    "href": "projets/projets_but/essai_clinique.html",
    "title": "Etude statistique dans un essai clinique",
    "section": "",
    "text": "Ce projet consiste en la réalisation d’une étude statistique dans le cadre d’un essai clinique. Nous travaillons sur un jeu de données simulées pour effectuer l’analyse statistique d’une étude de phase 3. L’objectif est de fournir au laboratoire une Autorisation de Mise sur le Marché (AMM) avec une indication dans la prise en charge de la drépanocytose.\nLa drépanocytose est une maladie génétique qui affecte les globules rouges et peut entraîner des complications graves. Elle se manifeste notamment par une anémie, des crises douloureuses et un risque accru d’infections.\n\n\n\nSchéma de la drépanocytose\n\n\nLors d’une étude de phase 3, les chercheurs comparent un nouveau traitement prometteur au traitement standard, qui est le traitement reconnu et généralement administré pour une affection ou une maladie. Dans notre étude statistique, le nouveau traitement s’appelle le Voxelotor et nous comparons ces effets à un placebo (traitement qui n’a aucune action spécifique sur le trouble qu’il vise à soulager). Pour cela, nous réalisons des tests statistiques sur des hypothèses définit en amont.\nLe dépôt GitHub contenant le rapport complet du projet, ainsi que le code R, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html#plan-danalyse-statistique",
    "href": "projets/projets_but/essai_clinique.html#plan-danalyse-statistique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Plan d’analyse statistique",
    "text": "Plan d’analyse statistique\nLe plan d’analyse statistique (SAP, disponible ici) décrit de manière détaillée la stratégie d’analyse des données de l’étude. Il garantit la rigueur, la transparence et la reproductibilité des analyses statistiques. Ce plan est généralement rédigé avant la fin de l’essai, afin d’éviter tout biais pouvant découler des résultats observés.\nPour évaluer l’effet du voxelotor sur l’amélioration de l’hémoglobine des patients par rapport au placebo, les analyses définies dans le SAP sont toutes réalisées. L’analyse principale porte sur la réponse en hémoglobine à la semaine 72, qui correspond au maximum du suivi des patients. L’analyse secondaire quant à elle examine le changement de l’hémoglobine entre le début de l’étude et la semaine 72 sur plusieurs marqueurs cliniques.\n\nAnalyse principal\nLa réponse en hémoglobine est définie comme une augmentation de plus de 1g/dL par rapport à la valeur de référence mesurée avant le début du traitement. Cela signifie qu’un patient est considéré comme ayant répondu favorablement au traitement si son taux d’hémoglobine a augmenté d’au moins 1g/dL par rapport à la valeur initiale, mesurée avant l’administration du voxelotor. Si aucune mesure d’hémoglobine n’est disponible à la semaine 72, le patient est imputé comme non-répondeur. Par exemple, si le patient ne se présente pas à la consultation, si les données sont perdues ou invalides, ce patient sera considéré comme n’ayant pas montré de réponse au traitement.\nNous utilisons un test statistique du Chi-2 avec correction de Yates pour comparer les taux de réponse en hémoglobine entre le groupe traité par VOX_1500 et le groupe par le placebo, afin de déterminer s’il existe une différence significative.\n\n\n\nRépartition des traitements dans notre population\n\n\nLes résultats du test indiquent qu’il n’y a pas de preuve statistiquement significative d’une différence entre les taux de réponse en hémoglobine des deux groupes.\nCependant, certains facteurs pourraient être amenés à faire varier l’efficacité du VOX_1500. C’est pourquoi, nous réalisons le même test statistique avec les mêmes hypothèses sur des sous-populations définis à partir des facteurs suivants : l’âge des patients, l’utilisation préalable d’hydroxyurée (HU) et l’historique des crises vaso-occlusives (VOC).\nLes résultats de cette étude, montrent que le VOX_1500 semble plus efficace chez certains groupes de patients. Premièrement, on observe une différence significative en faveur du traitement chez les non-utilisateurs d’HU. Cela indique une meilleure réponse en hémoglobine par rapport au placebo. En revanche, chez les utilisateurs d’HU, aucune différence notable n’a été trouvée entre les deux traitements.\nConcernant les crises vaso-occlusives, les patients ayant eu une seule crise répondent mieux au VOX_15000, alors que ceux ayant eu plusieurs crises ne montrent aucun signe positif supplémentaire par rapport au placebo.\nEnfin, l’analyse par tranche d’âge révèle des résultats similaires entre les groupes VOX_1500 et placebo, tant chez les adolescents que chez les adultes.\nEn somme, ces observations suggèrent que l’efficacité du VOX_1500 pourrait dépendre de certains facteurs cliniques, mais des études supplémentaires seront nécessaires pour valider ces conclusions.\n\n\nAnalyse secondaire\nDans cette secone partie, nous essayons de déterminer s’il existe une différence dans le changement moyen des taux d’hémoglobine entre les deux groupes (VOX_1500 et placebo). Pour répondre à cette question, nous réalisons une analyse statistique à l’aide de l’ANOVA. Cette dernière est une méthode statistique permettant de comparer les moyennes de plusieurs groupes afin de déterminer si au moins un groupe diffère significativement des autres. Plus concrètement, elle permet d’évaluer si les variations observées dans les données sont dues à des différences entre les groupes ou à des variations aléatoires au sein des groupes eux-mêmes. Elle nous est très utile ici, car elle permet d’ajuster les résultats en tenant compte des valeurs d’hémoglobine de départ (baseline) et de l’utilisation préalable d’hydroxyurée (HU) comme facteur supplémentaire.\nLes résultats révèlent que le VOX_1500 entraîne une augmentation significative du taux d’hémoglobine par rapport au placebo.\n\n\n\nReprésentation graphique du changement ajusté d’hémoglobine entre le départ et la semaine 72\n\n\nLes patients traités par VOX_1500 présentent une augmentation moyenne de 0,51 g/dL d’hémoglobine, contre 0,23 g/dL pour le groupe placebo. Cette différence est statistiquement significative au risque 5 %. Nous observons, un très faible impact des taux d’hémoglobine au départ sur l’évolution des niveaux observés. Cependant, l’utilisation préalable d’hydroxyurée influence fortement les résultats, les patients sous ce traitement montrant des évolutions différentes de ceux n’en prenant pas. Le traitement peut donc offrir un bénéfice notable en termes d’augmentation de l’hémoglobine, en particulier chez les patients n’utilisant pas d’hydroxyurée.\n\n\nMarqueurs d’hémolyse\nNous analysons ensuite l’évolution des marqueurs d’hémolyse en utilisant la même méthode. Ces indicateurs biologiques permettent d’évaluer la destruction des globules rouges dans le sang. Deux marqueurs clésretiennent notre attention :\n\nBilirubine non conjuguée : elle augmente lorsque les globules rouges sont détruits, reflétant une hémolyse active.\nPourcentage de réticulocytes : ces globules rouges immatures sont produits en réponse à la destruction des globules matures. Un taux élevé traduit une compensation de l’organisme face à l’hémolyse.\n\nL’analyse de ces paramètres permet de mieux comprendre l’impact du traitement sur la santé des globules rouges.\n\n\n\nVariations dans les marqueurs d’hémolyse entre la baseline et la semaine 72\n\n\nL’analyse des marqueurs d’hémolyse révèle que le traitement VOX_1500 a un effet significatif sur la réduction de la bilirubine non conjuguée et des réticulocytes, deux indicateurs clés de la destruction des globules rouges.\n\n\n\nReprésentation graphique du changement dans les marqueurs d’hémolyse\n\n\nLe traitement VOX_1500 entraîne une diminution moyenne de -8,09 g/dL contre -4,56 g/dL pour le placebo. Cette différence statistiquement significative confirme l’effet notable du traitement. De plus, le VOX_1500 montre également un impact significatif, sans que les valeurs initiales ou l’utilisation d’Hydroxyurée n’influencent les résultats.\nOn en conclut que le traitement VOX_1500 a un effet plus marqué que le placebo sur la réduction de la bilirubine non conjuguée et des réticulocytes, comme le montre la différence significative dans les changements moyens ajustés pour ces deux marqueurs d’hémolyse. Le traitement pourrait ainsi être plus efficace pour réduire les caractéristiques d’hémolyse par rapport au placebo."
  },
  {
    "objectID": "projets/projets_but/essai_clinique.html#nouvel-essai-clinique",
    "href": "projets/projets_but/essai_clinique.html#nouvel-essai-clinique",
    "title": "Etude statistique dans un essai clinique",
    "section": "Nouvel essai clinique",
    "text": "Nouvel essai clinique\nAu vu des facteurs influents vus dans l’analyse principal. Il se peut que le laboratoire envisage de lancer un nouvel essai clinique afin d’évaluer l’efficacité du VOX_1500 chez les adolescents (patient de 12 à 18 ans). En utilisant le même critère principal que précédemment et en se basant sur les résultats obtenus pour le voxelotor dans cette population, tout en supposant un taux de réponse de 10% pour le groupe placebo, nous procédons au calcul du nombre de patients nécessaires pour démontrer l’efficacité du traitement. Ce calcul est effectué en visant une puissance statistique de 90% et un risque alpha de 5%, avec un ratio de traitement 1:1.\n\n\n\nProgramme R du calcul du nombre de patients\n\n\nLa taille de l’échantillon nécessaire pour ce nouvel essai clinique est de 48 patients au total, répartis également entre les deux groupes, soit 24 patients par groupe (placebo et le VOX_1500). Cette estimation garantit une puissance de 90% pour détecter une différence significative entre les groupes 29 avec un risque alpha de 5%."
  },
  {
    "objectID": "projets/projets_but/migration.html",
    "href": "projets/projets_but/migration.html",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "",
    "text": "Ce projet vise à migrer des données d’un environnement SQL vers un environnement NoSQL. Concrètement, il s’agit de transférer les informations stockées dans une base de données relationnelle traditionnelle, où les données sont organisées en tables avec des relations fixes, vers une base de données NoSQL, qui offre une structure plus flexible adaptée aux données non structurées ou semi-structurées.\nNous travaillons avec les données d’une entreprise de voitures qui rencontre des problèmes avec sa base de données actuelle : les requêtes sont lentes et des défaillances serveur entraînent des pertes de données. Pour résoudre ces problèmes, nous avons décidé de passer à un environnement NoSQL. Cette technologie permet de stocker des données sous une forme non structurée, offrant ainsi plus de flexibilité et de performance. Cette migration vise à améliorer la performance des requêtes et à préparer l’infrastructure pour une croissance future.\nLe dépôt GitHub contenant le rapport complet du projet, ainsi que les requêtes SQL et NoSQL associées, est disponible ici."
  },
  {
    "objectID": "projets/projets_but/migration.html#base-de-données-relationnelle",
    "href": "projets/projets_but/migration.html#base-de-données-relationnelle",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Base de données relationnelle",
    "text": "Base de données relationnelle\nLa base de données relationnelle initiale contient des informations sur les véhicules, les clients, les commandes, les employés… La représentation des données est claire et bien organisée. Chaque table dispose de relations avec d’autres tables, ce qui permet de structurer efficacement les informations et de faciliter les requêtes complexes.\n\n\n\nSchéma relationnel de la bdd initial\n\n\nDans un premier temps, nous avons créé des requêtes SQL sur cette base de données. Ces requêtes serviront de tests pour évaluer le succès de la migration. Nous comparerons les résultats obtenus dans la base de données relationnelle avec ceux obtenus dans la base NoSQL pour vérifier l’intégrité et la performance de la migration.\n\n\n\nRequêtes SQL\n\n\nLa base de données est au format SQLite. Nous avons importé le module sqlite3 en Python pour établir la connexion et interagir avec la base. Ensuite, nous avons utilisé la bibliothèque Pandas et notamment sa fonction read_sql_query(), pour exécuter et lire les résultats des requêtes SQL."
  },
  {
    "objectID": "projets/projets_but/migration.html#algorithme-de-migration",
    "href": "projets/projets_but/migration.html#algorithme-de-migration",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Algorithme de migration",
    "text": "Algorithme de migration\nIl existe plusieurs types de bases de données NoSQL (Clé-valeur, Document, Colonne et Graphe), chacun adapté à ses propres cas d’usage et ayant ses propres avantages et inconvénients, notamment en termes de scalabilité et de flexibilité. Le choix dépend donc de plusieurs facteurs clés, comme la structure des données, les exigences de performance…\nEn ce qui nous concerne, nous pouvons réaliser une migration vers un environnement NoSQL, car l’entreprise dispose d’une grande quantité de données structurées en constante croissance. De plus, il est possible d’améliorer significativement les performances d’accès aux données en optimisant le traitement de données plus importantes et en réduisant le temps de latence. On souhaite donc une solution qui offre plus de flexibilité et évolutivité, tout en préservant l’intégrité des données de notre base initial.\nAprès mûre réflexion, le format document est celui s’adaptant le mieux à notre objectif. En effet, il permet de structurer naturellement les entités de manière hiérarchique. Par exemple, un client peut être représenté par un document contenant ses commandes, chaque commande incluant les produits associés. De plus, il offre une grande flexibilité, permettant de traiter différents types de données sans modifications complexes du schéma. Enfin, il permet une scalabilité horizontale grâce à la partition de document, c-à-d si les données augmentent, on peut facilement ajouter de nouveaux serveurs pour stocker et gérer plus de documents, sans tout restructurer.\n\n\n\nExemple d’une modélisation au format Document\n\n\nCe modèle présente quelques inconvénients, notamment des performances limitées pour les requêtes complexes ou les jointures entre documents. De plus, les mises à jour simultanées de documents imbriqués peuvent être plus difficiles à gérer.\nNous avons décidé de structurer nos données autour de quatres collections : customers, payments, orders et employees."
  },
  {
    "objectID": "projets/projets_but/migration.html#script-de-migration",
    "href": "projets/projets_but/migration.html#script-de-migration",
    "title": "Migration de données : De SQL à NoSQL",
    "section": "Script de migration",
    "text": "Script de migration\nAvant de développer un script de migration, nous avons d’abord établi un pseudo-algorithme dans l’objecitf de structurer et organiser la logique de notre programme.\nEnsuite, pour la migration de nos données, nous avons utilisé SQLite comme source et MongoDB comme destination, en exploitant les bibliothèques Python sqlite3, pymongo et pandas. Le processus inclut l’extraction des données de SQLite, leur transformation au format document compatible avec MongoDB, et leur insertion dans les collections appropriées."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rachid Sahli",
    "section": "",
    "text": "Je suis étudiant en Science des Données à l’IUT Paris Rives de Seine et alternant à l’INSEE. Au sein de la Direction des Statistiques Démographiques et Sociales, je travaille sur des problématiques liées à la mesure de la couverture des bases de sondage.\nJe travaille principalement avec R et Python et j’ai créé un site de ressources appelé PratiqueR. Vous pouvez également me retrouver sur YouTube, où je partage des vidéos sur R et son environnement.\nEn-dehors de mes études, je consacre mon temps libre à la construction de robots et je partage mes idées sur mon blog, Jiqiren.\nPassionné par les statistiques, la robotique 🦿🤖, le vélo 🚲, le cinéma 🎞️, et la programmation 👾, j’aime explorer et partager mes découvertes dans ces domaines.\nJe suis très curieux et toujours ouvert à de nouvelles idées. N’hésitez pas à me contacter pour discuter d’un projet ou explorer des opportunités de collaboration.\n\n\nFormation\n\n\n\n\n\n\nIUT de Paris - Rives de Seine (Université Paris Cité) | Paris, 75016\n\n\n\nBUT Science des données, parcours exploration et modélisation statistique | Sept 2022 - Juin 2025\nCours suivis : Statistique inférentielle, paramétrique et non-paramétrique, Modélisation statistique, Algèbre linéaire, Analyse, Probabilités, Machine learning, Data mining, Programmation, Base de données\n\n\n\n\nExpérience\n\n\n\n\n\n\nInstitut national de la statistique et des études économiques (INSEE) | Montrouge, 92120\n\n\n\nStatisticien | Sept 2023 - Sept 2025 Au sein de la Direction des statistiques démographiques et sociales, j’ai mené des travaux d’appariement de données administratives. L’objectif de ces travaux était de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j’ai comparé différents algorithmes d’appariement à l’aide d’analyses statistiques. De plus, j’ai mis en place des modèles de classification exploitant les données administratives afin de réaliser des prédictions."
  },
  {
    "objectID": "blog/titanic/titanic.html",
    "href": "blog/titanic/titanic.html",
    "title": "Peut-on prédire les survivants du Titanic ?",
    "section": "",
    "text": "Le naufrage du Titanic est l’un des plus terribles qu’il ait pu exister. Le 15 avril 1912, lors de son voyage inaugural, ce paquebot de 269 mètres de long (une trentaine de mètres de moins que la tour Eiffel) a coulé. Le trajet était le suivant, le 10 avril 1912, le RMS Titanic, quittait Southampton en Angleterre pour une traversée qui devait le conduire jusqu’à New York aux Etats-Unis, après une escale à Cherbourg et en Irlande.\nIl était qualifié d’insubmersible, mais sa collision avec un iceberg en a décidé autrement. Malheureusement, le nombre de canots de sauvetage était inférieur au nombre de passagers à bord. Cela a entraîné la mort de 1502 des 2224 passagers et membre d’équipage. Miracle ! Il y a eu 711 rescapés.\nBien que la chance ait joué un rôle dans la survie des passagers, certaines catégories de personnes semblent avoir eu un avantage. Est-il possible de prédire ces chances de survie à l’avance ? Si oui, quel type de personnes avaient le plus de chance de survivre ?\nNous allons utilisé des méthodes d’apprentissage supervisé pour répondre à ces questions. L’objectif de l’apprentissage supervisé est de prévoir l’étiquette \\(Y\\) ou la valeur de \\(Y\\) (régression) associée à une nouvelle entrée, où il est sous-entendu que (\\(X, Y\\)) est une nouvelle réalisation des données, indépendante de l’échantillon observé. Ici, nous utilserons 4 algorithmes différents : l’algorithme des \\(k\\) plus proches voisins, la régression logistique, un arbre de décision et l’algorithme SVM.\nNous utiliserons Python et la librairie Scikit-learn, bibliothèque open source d’apprentissage automatique dédiée au machine learning pour réalisé une classification. Ici, on est dans un cas de classification et non de régression, car notre variable cible (\\(Y\\)) est une étiquette (Survécu ou Non survivant)."
  },
  {
    "objectID": "blog/titanic/titanic.html#visualisation-graphique",
    "href": "blog/titanic/titanic.html#visualisation-graphique",
    "title": "Peut-on prédire les survivants du Titanic ?",
    "section": "Visualisation graphique",
    "text": "Visualisation graphique\nLa section précédente nous a permis d’identifier les incohérences et anomalies dans nos données. Dans cette même optique, nous allons maintenant les analyser visuellement afin de mieux comprendre leur répartition.\nNous présentons ici seulement quelques graphiques intéressants, mais il existe bien d’autres visualisations possibles tout aussi intéressantes.\n\n\nCliquer pour voir le code\ncmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\ncolors = [mcolors.rgb2hex(c) for c in cmap.colors]\n\nfig = px.scatter(titanic, \nx=\"Age\", y=\"Fare\", \ncolor=\"SibSp\", size=\"Pclass\",\ncolor_continuous_scale=colors,\nlabels={\"Age\": \"Age\", \"Fare\": \"Fare\"},\nlog_x=True, log_y=True)\n\nfig.update_traces(marker=dict(sizemode='area', \nopacity=0.7, line=dict(width=0)),\nselector=dict(mode='markers'))\n\nfig.update_layout(\n  template=\"plotly_white\",\n  title_text='Relation entre l\\'âge et le prix du Billet selon le Nombre de frères/sœurs et conjoints',\n  )\n\nfig.show()\n\n\n                                                \n\n\nLes passagers les plus âgés, ayant payé les tarifs les plus élevés, voyagent principalement en 1ʳᵉ classe et sont souvent seuls ou avec un membre de leur famille. En revanche, ceux ayant payé les tarifs les plus bas sont majoritairement en 2ᵉ ou 3ᵉ classe, avec souvent plusieurs frères et sœurs à bord.\n\n\nCliquer pour voir le code\ncolor_map = {0: 'red', 1: 'green'}\n\nfig = px.strip(titanic, \n               x=\"Sex\", y=\"Age\", \n               color=\"Survived\", \n               category_orders={\"Sex\": [\"male\", \"female\"]}, \n               labels={\"Age\": \"Age\", \"Sex\": \"Sex\"},\n               color_discrete_map=color_map)\n\nfig.update_traces(marker=dict(size=8, opacity=0.7, line=dict(width=0)),\n                  selector=dict(mode='markers'))\n\nfig.update_layout(\n    yaxis_title=\"\",\n    template=\"plotly_white\",\n    title_text='Répartition de l\\'âge selon le sexe et la survie à bord'\n)\n\nfig.show()\n\n\n                                                \n\n\nLes femmes (35,2 %) sont beaucoup moins représentées dans le jeu de données que les hommes (64,8 %). On observe qu’il y a une densité élevée de survivantes dans la tranche des jeunes adultes et des adultes moyens. Les hommes sont sur-représentés parmi les non-survivants, avec une concentration plus marquée parmi les adultes jeunes et moyens.\nNous savons que les femmes et les enfants avaient des priorités d’évacuation pendant le naufrage et elles avaient donc une probabilité de survie plus élevée que celle des hommes.\n\n\nCliquer pour voir le code\nfig = px.histogram(titanic, \n                   x=\"Age\", \n                   color=\"Survived\", \n                   facet_col=\"Survived\",\n                   nbins=20,\n                   color_discrete_map={0: 'red', 1: 'green'},\n                   labels={\"Age\": \"Age\", \"Survived\": \"Survived\"},\n                   histnorm=\"percent\")\n\nfig.update_traces(marker_line_color=\"black\",\n                  marker_line_width=1)\n\nfig.update_layout(\n    title=\"Répartition de l'âge en fonction de la survie\",\n    template=\"plotly_white\",\n    xaxis_title=\"Âge\",\n    yaxis_title=\"Fréquence (%)\"\n)\n\nfig.show()"
  },
  {
    "objectID": "blog/titanic/titanic.html#traitement-des-données",
    "href": "blog/titanic/titanic.html#traitement-des-données",
    "title": "Peut-on prédire les survivants du Titanic ?",
    "section": "Traitement des données",
    "text": "Traitement des données\nMaintenant que nous avons une vue d’ensemble de nos données, nous allons nous concentrer sur le traitement des anomalies identifiées précédemment, en vue de préparer efficacement la construction du modèle.\nNous commençons par convertir la variable Sexe en une variable binaire. Ici, nous la remplaçons directement en créant une nouvelle version par-dessus. Cependant, il est recommandé de créer une variable distincte afin de préserver l’intégrité des données d’origine.\n---\ntitanic['Sex'] = (titanic['Sex'] == 'female').astype(int)\n---\n\n\nAvant conversion :\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: object\n\nAprès conversion :\n0    0\n1    1\n2    1\n3    1\n4    0\nName: Sex, dtype: int64\n\n\nNous passons maintenant au traitement des données manquantes. Il existe plusieurs approches pour compléter les données manquantes. Par exemple, la suppresion des observations, l’imputation de ces données par une valeur tel que la moyenne, la médiane … On introduirait cependant un biais sur cette valeur. Il est également possible d’imputer les données manquantes à l’aide d’une méthode statistique (régression, \\(k\\) plus proches voisins).\n\n\nCliquer pour voir le code\ntitanic.count().plot(kind='barh', title=\"Nombre d'occurrences par variable avant traitement\",\n                                                 xlabel=\"Nombre d'occurences\",\n                                                 ylabel=\"Variable\")\n\n\n\n\n\n\n\n\n\nDans notre jeu de données, il manque des données pour les variables Age, Embarked et Cabin.\nLa variable Cabin est très peu renseignée. Il est inutile d’imputer les valeurs manquantes par une autre valeur, nous choisissons donc de supprimer cette variable. Toutefois, nous décidons de créer une nouvelle variable indiquant si cette donnée a été renseignée ou non {0; 1}.\n---\ntitanic['CabinMissing'] = titanic.Cabin.isnull().astype(int) # Variable indicatrice\n\ntitanic = titanic.drop('Cabin', axis=1) # Suppresion de Cabin\n\ntitanic['CabinMissing'] # Visualisation de CabinMissing\n---\n\n\n0      1\n1      0\n2      1\n3      0\n4      1\n      ..\n886    1\n887    0\n888    1\n889    0\n890    1\nName: CabinMissing, Length: 891, dtype: int64\n\n\nLa variable Age est bien plus renseigné que la variable Cabin, on décide d’imputer ces valeurs manquantes par l’âge médian. Évidemment, cela biaisera notre distribution.\nL’âge médian des passagers est de 28 ans. Nous imputerons les 177 valeurs manquantes par cet valeur.\n---\ntitanic.Age.median()\n---\n\n\n28.0\n\n\n---\nage_median = titanic.Age.median()\n\ntitanic['Age'] = titanic.Age.fillna(median_age)\n---\nCi-dessous, nous observons les distributions de l’âge avant et après imputation. On remarque clairement que l’imputation a modifié la distribution des données.\n\n\nCliquer pour voir le code\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=[\"Avant imputation\", \"Après imputation\"], shared_yaxes=True)\n\nfig.add_trace(\n    go.Histogram(x=titanic[\"Age\"], nbinsx=20, name=\"Age\", marker_color=\"blue\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=titanic[\"Age_2\"], nbinsx=20, name=\"Age_2\", marker_color=\"darkcyan\"),\n    row=1, col=2\n)\n\nfig.update_traces(marker_line_color=\"black\",\n                  marker_line_width=1)\n\nfig.update_layout(\n    title=\"Distribution de l'âge des passagers du Titanic\",\n    template=\"plotly_white\",\n    xaxis_title=\"Âge\",\n    yaxis_title=\"Nombre de passagers\",\n    showlegend=False\n)\n\nfig.show()\n\n\n                                                \n\n\nNous remplaçons les deux valeurs manquantes de la variable Embarked, représentant le port de départ, par le port de départ majoritaire des passagers.\n---\nmaj_port = titanic[\"Embarked\"].mode()[0]\n\ntitanic[\"Embarked\"].fillna(maj_port, inplace=True)\n---\nEnfin, nous excluons les variables suivantes :\n\nPassengerId : Il s’agit d’un simple identifiant sans influence sur la survie.\nName : Cette variable contient des informations textuelles difficiles à exploiter directement.\nTicket : Elle présente trop de valeurs uniques et apporte peu d’informations pertinentes.\n\n---\ntitanic = titanic.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"])\n---\nLe jeu de données est désormais complet, sans aucune valeur manquante.\n\n\nCliquer pour voir le code\ntitanic.count().plot(kind='barh', color='green', title=\"Nombre d'occurrences par variable après traitement\",\n                                                 xlabel=\"Nombre d'occurences\",\n                                                 ylabel=\"Variable\")"
  },
  {
    "objectID": "blog/titanic/titanic.html#variables-pertinentes",
    "href": "blog/titanic/titanic.html#variables-pertinentes",
    "title": "Peut-on prédire les survivants du Titanic ?",
    "section": "Variables pertinentes",
    "text": "Variables pertinentes\nNous commençons par mesurer la corrélation entre les variables explicatives et notre variable cible afin d’identifier celles qui ont le plus d’influence sur la prédiction.\n---\ncor = titanic.corr(numeric_only=True)[\"Survived\"].sort_values(ascending=False)\ncor\n---\n\n\nSurvived        1.000000\nSex             0.543351\nFare            0.257307\nParch           0.081629\nSibSp          -0.035322\nAge            -0.064910\nCabinMissing   -0.316912\nPclass         -0.338481\nName: Survived, dtype: float64"
  },
  {
    "objectID": "blog/reg_logistique.html",
    "href": "blog/reg_logistique.html",
    "title": "Régression logistique en pratique",
    "section": "",
    "text": "Introduction\nCe billet de blog a pour objectif d’étudier le modèle de régression logistique et de le comparer ainsi à la méthode des \\(k\\) plus proches voisins. Ces deux méthodes sont des algorithmes d’apprentissage supervisé. Le but de l’apprentissage supervisé est de prévoir l’étiquette (classification) \\(Y\\) ou la valeur de \\(Y\\) (régression) associée à une nouvelle entrée \\(X\\), où il est sous-entendu que (\\(X,Y\\)) est une nouvelle réalisation des données, indépendante de l’échantillon observé.\nLa régression logistique est une méthode de classification. Ce modèle de classification binaire permet d’expliquer une variable (\\(Y\\)) par p variables explicatives (\\(X_1,...,X_j\\). La variable \\(Y\\) ne peut prendre que deux modalités \\(\\left\\{0 ;1\\right\\}\\). Les variables \\(X_j\\) sont exclusivement continues ou binaires (on re-code les variables qualitatives avec des 0 et 1).\nL’algorithme des \\(k\\) plus proches voisins fonctionne de la façon suivante pour la classification. On détermine les \\(k\\) plus proches \\(X_i\\) de l’échantillon par rapport à \\(X\\) et on attribue la modalité dominante parmi les \\(k\\) modalités observées (on parle de vote majoritaire).\nNous allons utiliser ces deux méthodes de classification afin de prédire, en se basant sur des mesures diagnostiques, si un patient est atteint du diabète ou non.\n\n\nImport et préparation des données\nDans un premier temps, nous importons le jeu de données “diabetes.csv”. Nous allons mettre en pratique nos méthodes de régression logistique et \\(k\\) plus proches voisins sur ce dernier. Ce jeu de données est issu de l’Institut national du diabète et des maladies digestives et rénales. Tous les patients ici sont des femmes d’au moins 21 ans d’origine Pima. Il contient les mesures suivantes.\n\nDescription des variables\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\nPregnancies\nNombre de grossesses\nQuanti continue\n\n\nGlucose\nConcentration de glucose plasmatique\nQuanti discrète\n\n\nBloodPressure\nPression artérielle diastolique (mm Hg)\nQuanti discrète\n\n\nSkinThickness\nÉpaisseur du pli cutané du triceps (mm)\nQuanti discrète\n\n\nInsulin\nInsuline sérique à 2 heures (mu U/ml)\nQuanti discrète\n\n\nBMI\nIndice de masse corporelle (poids en kg / (taille en m)²)\nQuanti continue\n\n\nDiabetesPedigree\nFonction de prédisposition au diabète\nQuanti continue\n\n\nAge\nÂge (années)\nQuanti discrète\n\n\nOutcome\nStatut diabétique (oui ou non)\nQuanti discrète\n\n\n\n\n# Import library ---------------\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(class)\n\n# Import data ---------------\ndiabetes &lt;- read.csv(\"data_blog/diabetes.csv\")\n\ndiabetes$Outcome &lt;- as.factor(diabetes$Outcome)\n\nNous avons en même temps importé 3 packages qui nous seront utiles dans la réalisation de notre travail. À savoir, ggplot pour la réalisation de graphique, et class pour la classification de l’algorithme des \\(k\\) plus proches voisins.\nVoici un petit aperçu de nos données.\n\nhead(diabetes)\n\n  Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n6           5     116            74             0       0 25.6\n  DiabetesPedigreeFunction Age Outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n6                    0.201  30       0\n\n\nNotre jeu de données ne présente pas de valeurs manquantes (NA), mais contient des observations pour lesquelles la valeur est égale à 0. Nous considérerons ces valeurs comme manquantes et les supprimons. Nous faisons cela uniquement sur 5 variables pour qui l’on juge que la valeur 0 est une vraie donnée manquante. Par exemple, nous ne le faisons pas pour la variable Pregnancies pour qui le 0 veut tout simplement dire que la personne n’a jamais été enceinte. Nous conservons alors 392 individus sur les 768 initiaux.\n\n# Suppression des 0\ndiabetes &lt;- diabetes[diabetes$SkinThickness!=0,]\ndiabetes &lt;- diabetes[diabetes$Insulin!=0,]\ndiabetes &lt;- diabetes[diabetes$Glucose!=0,]\ndiabetes &lt;- diabetes[diabetes$BloodPressure!=0,]\ndiabetes &lt;- diabetes[diabetes$BMI!=0,] \n\nCette suppression à pour objectif de ne pas fausser les relations que nous cherchons à modéliser. Par exemple, avoir une insuline à 0 n’est premièrement pas cohérent et peut influencer de manière disproportionnée le modèle, car les autres observations sont éloignées. En somme, nous essayons de garantir que le modèle représente fidèlement la relation entre les variables.\n\n\nStatistique descriptive\nÀ présent, nous allons étudier nos données de manière descriptive afin d’en obtenir un aperçu et ainsi de visualiser la répartition de la variable \\(Y\\) (Outcome). De plus, nous analyserons les corrélations entre les co-variables elles-mêmes, ainsi qu’entre celles-ci et la variable \\(Y\\).\n\nsummary(diabetes)\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   : 56.0   Min.   : 24.00   Min.   : 7.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.:21.00  \n Median : 2.000   Median :119.0   Median : 70.00   Median :29.00  \n Mean   : 3.301   Mean   :122.6   Mean   : 70.66   Mean   :29.15  \n 3rd Qu.: 5.000   3rd Qu.:143.0   3rd Qu.: 78.00   3rd Qu.:37.00  \n Max.   :17.000   Max.   :198.0   Max.   :110.00   Max.   :63.00  \n    Insulin            BMI        DiabetesPedigreeFunction      Age       \n Min.   : 14.00   Min.   :18.20   Min.   :0.0850           Min.   :21.00  \n 1st Qu.: 76.75   1st Qu.:28.40   1st Qu.:0.2697           1st Qu.:23.00  \n Median :125.50   Median :33.20   Median :0.4495           Median :27.00  \n Mean   :156.06   Mean   :33.09   Mean   :0.5230           Mean   :30.86  \n 3rd Qu.:190.00   3rd Qu.:37.10   3rd Qu.:0.6870           3rd Qu.:36.00  \n Max.   :846.00   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n Outcome\n 0:262  \n 1:130  \n        \n        \n        \n        \n\n\n\n\n\n\n\n\n\n\n\nDans notre échantillon, 262 patients (67 %) ne sont pas atteints de diabète, tandis que 130 patients (33 %) en sont atteints. Les personnes non-diabétiques sont donc largement plus nombreuses.\nEnviron 14,3 % des patients n’ont jamais eu de grossesse. Il y a 23,7 % des patients qui ont eu une unique grossesse, et nous constatons une diminution progressive du nombre de patients à mesure que le nombre de grossesses augmente. La majorité des patients ont eu au moins une grossesse, ce qui indique que les grossesses sont relativement courantes dans cet échantillon.\n\n\n\n\n\n\n\n\n\nOn observe ci-dessous, l’âge des patients. Le plus jeune patient a 21 ans, tandis que le plus âgé a 81 ans. Ce dernier se distingue clairement en haut de la boîte à moustaches, où il se situe assez éloigné des autres patients. On note également 5 autres valeurs aberrantes avec un âge très élevé. L’âge médian dans l’échantillon est de 29 ans. Il est proche du Q1, on peut donc penser à une représentation assez élevée de patients plutôt jeunes.\n\n\n\n\n\n\n\n\n\nÀ présent, nous observons les différentes corrélations entre nos variables quantitatives continues. L’objectif du coefficient de corrélation est de muserer la force de relation linéaire entre deux variables. Il se calcule de la manière suivante :\n\\[\nR(x, y) = \\frac{\\text{Cov}(x, y)}{\\sigma_x \\sigma_y}\n\\]\nLe coefficient de corrélation est compris entre -1 et 1 :\n\nSi \\(R(x,y)\\) est proche de 0 : La relation linéaire est nulle.\nSi \\(R(x,y)\\) est proche de -1 : La relation linéaire est forte mais négative.\nSi \\(R(x,y)\\) est proche de 1 : La relation linéaire est forte.\n\n\ncor(diabetes[,c(-1,-9)])\n\n                           Glucose BloodPressure SkinThickness   Insulin\nGlucose                  1.0000000     0.2100266     0.1988558 0.5812230\nBloodPressure            0.2100266     1.0000000     0.2325712 0.0985115\nSkinThickness            0.1988558     0.2325712     1.0000000 0.1821991\nInsulin                  0.5812230     0.0985115     0.1821991 1.0000000\nBMI                      0.2095159     0.3044034     0.6643549 0.2263965\nDiabetesPedigreeFunction 0.1401802    -0.0159711     0.1604985 0.1359058\nAge                      0.3436415     0.3000389     0.1677611 0.2170820\n                               BMI DiabetesPedigreeFunction        Age\nGlucose                  0.2095159               0.14018018 0.34364150\nBloodPressure            0.3044034              -0.01597110 0.30003895\nSkinThickness            0.6643549               0.16049853 0.16776114\nInsulin                  0.2263965               0.13590578 0.21708199\nBMI                      1.0000000               0.15877104 0.06981380\nDiabetesPedigreeFunction 0.1587710               1.00000000 0.08502911\nAge                      0.0698138               0.08502911 1.00000000\n\n\nLes variables sont toutes moyennement corrélées positivement.\nNous représentons ci-dessous les relations entre les co-variables et notre variable \\(Y\\) (Outcome).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn observe une corrélation entre nos variables quantitatives continues et la variable Outcome (\\(Y\\)). On le voit par exemple à travers le lien entre la variable Glucose et la variable \\(Y\\). Les patients atteints de diabète présentent un taux de glucose significativement plus élevé que ceux qui ne sont pas atteints par la maladie.\n\n\nModèle de regression logistique\nNous pouvons mettre en place notre modèle de régression logistique. Nous commençons par le modèle complet. C’est-à-dire que nous sélectionnons toutes les co-variables pour expliquer \\(Y\\). On rappel qu’on cherche à prédire les valeurs de la variable Outcom (\\(Y\\)) à l’aide des variables explicatives présentent dans notre jeu de données.\nOn utilise pour cela la fonction glm() (Fitting Generalized Linear Models).\n\nmodel_complet &lt;- glm(Outcome~., family = \"binomial\", data = diabetes)\nsummary(model_complet)\n\n\nCall:\nglm(formula = Outcome ~ ., family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7823  -0.6603  -0.3642   0.6409   2.5612  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -1.004e+01  1.218e+00  -8.246  &lt; 2e-16 ***\nPregnancies               8.216e-02  5.543e-02   1.482  0.13825    \nGlucose                   3.827e-02  5.768e-03   6.635 3.24e-11 ***\nBloodPressure            -1.420e-03  1.183e-02  -0.120  0.90446    \nSkinThickness             1.122e-02  1.708e-02   0.657  0.51128    \nInsulin                  -8.253e-04  1.306e-03  -0.632  0.52757    \nBMI                       7.054e-02  2.734e-02   2.580  0.00989 ** \nDiabetesPedigreeFunction  1.141e+00  4.274e-01   2.669  0.00760 ** \nAge                       3.395e-02  1.838e-02   1.847  0.06474 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.02  on 383  degrees of freedom\nAIC: 362.02\n\nNumber of Fisher Scoring iterations: 5\n\n\nLe paramètre family = binomial indique que nous utilisons une régression logistique et que notre variable est binaire.\nNous nous concentrons sur la significativité statistique de chaque coefficient. En effet, la \\(p-valeur\\) nous indique la probabilité que l’effet de la variable explicative soit dû au hasard. Plus sa valeur est faible, plus l’effet de la variable explicative sur notre variable \\(Y\\) (Outcome) est statistiquement significatif. Au contraire, si elle est supérieure à 0.05, cela suggère que l’effet de la variable explicative sur notre variable \\(Y\\) n’est pas significatif et pourrait être dû au hasard.\nIci, les variables explicatives avec des p-valeurs inférieur à 5 % sont :\n\nGlucose (\\(p-valeur\\) = \\(3,24 * 10^{-11}\\)) : L’effet de la variable glucose sur la probabilité de la variable diabète est significatif.\nBMI (\\(p-valeur\\) = \\(0.00989\\)) : L’effet de la variable BMI sur la probabilité de la variable diabète est significatif.\nDiabetesPedigreeFunction (\\(p-valeur\\) = \\(0.00760\\)) : L’effet de la variable DiabetesPedigreeFunction sur la probabilité de la variable diabète est significatif.\n\nLes variables restantes ont des \\(p-valeur\\) &gt; \\(5 \\%\\), on en conclut donc qu’elles ne sont pas liées au risque de diabète.\n\n\nCalcul des odds-ratios et de leurs intervalles de confiance\nL’odds-ratio est une mesure statistique exprimant le degré de dépendance entre des variables aléatoires qualitatives. Il permet de mesurer l’effet d’un facteur en comparant les chances qu’un évènement se produise dans un groupe par rapport à un autre groupe.\nIci, l’odds-ratio va représenter l’impact de nos variables explicatives sur les chances que notre variable \\(Y\\) (Outcome) prenne la valeur diabétique (1) ou non-diabétique.\nNous calculons ci-dessous les odds-ratios de toutes les variables, ainsi que leurs intervalles de confiance qui nous permetteront d’être sur à 95 % que l’odds-ratio se trouve entre les bornes inférieurs et supérieurs.\nAfin d’obtenir les odds-ratios à partir des coefficients de régression logistique, nous avons pris l’exponentielle des coefficients.\n\n# Permet la realisation de tableau\nlibrary(knitr)\n\nkable(exp(model_complet$coefficients[-1]), col.names = c(\"Variable\", \"Odd-ratios\"))\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.0856289\n\n\nGlucose\n1.0390112\n\n\nBloodPressure\n0.9985807\n\n\nSkinThickness\n1.0112846\n\n\nInsulin\n0.9991750\n\n\nBMI\n1.0730849\n\n\nDiabetesPedigreeFunction\n3.1296107\n\n\nAge\n1.0345346\n\n\n\n\nkable(exp(confint(model_complet)[-1, ]), col.names = c(\"Variable\", \"IC_inf\", \"IC_sup\"))\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9743237\n1.211631\n\n\nGlucose\n1.0277173\n1.051303\n\n\nBloodPressure\n0.9757909\n1.022307\n\n\nSkinThickness\n0.9778466\n1.045780\n\n\nInsulin\n0.9966180\n1.001767\n\n\nBMI\n1.0178269\n1.133537\n\n\nDiabetesPedigreeFunction\n1.3783799\n7.368273\n\n\nAge\n0.9985446\n1.073523\n\n\n\n\n\nNous nous intéressons uniquement aux variables ayant un effet significatif sur la probabilité de la variable Outcome. Ce sont les variables Glucose, BMI et DiabetesPedigreeFunction.\nOn note ici qu’une augmentation de 20 de BMI correspond à \\(1.0730849^{20} = 4.09\\) plus de risque d’avoir du diabète que de ne pas en avoir. Une augmentation de 30 de glucose correspond à \\(1.0390112^{30} = 3.15\\) plus de risque d’avoir du diabète que de ne pas en avoir. Enfin, une augmentation de 2 de DiabetesPedigreeFunction correspond à \\(3.1296107^{2} = 9.79\\) plus de risque d’avoir du diabète que de ne pas en avoir.\nNous allons maintenant exhiber des profils d’individus particulièrement à risque d’être diabétiques à l’aide des coefficients de notre modèle de régression logistique.\nPar exemple, un individu particulièrement à risque d’avoir du diabète sera un individu ayant un taux de glucose égale à 160, 40 de BMI et 1.5 de DiabetesPedigreeFunction. Son risque de diabète est alors égal à environ 99.9 %.\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             60*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.9994916 \n\n\nCependant, si ce même individu avait eu un BMI de 20 au lieu de 40, son risque de diabète aurait grandement diminué (71.4 %).\n\nscore &lt;- exp(model_complet$coefficients[1]+\n             160*model_complet$coefficients[3]+\n             30*model_complet$coefficients[7]+\n             1.5*model_complet$coefficients[8])\nexp(score)/(1+exp(score))\n\n(Intercept) \n  0.7137806 \n\n\n\n\nMéthode de sélection de variable\nToutes les variables ne contribuent pas nécessairement à expliquer notre variable Y, nous allons voir comment sélectionner certaines variables afin de simplifier notre modèle, mais aussi pour améliorer la classification.\nIci, nous utiliserons l’AIC (Critère d’information d’Akaike) qui est une mesure de la qualité d’un modèle statistique. L’AIC va nous permettre de comparer les différents modèles en utilisant un critère de vraisemblance. Il représente un compromis entre le biais (qui diminue avec le nombre de paramètres) et la parcimonie (nécessité de décrire les données avec le plus petit nombre de paramètres possible).\n\\[AIC = -2 \\cdot \\log(L) + 2 \\cdot k\\]\n\n\\(L\\) est la vraisemblance maximisée\n\\(k\\) est le nombre de paramètres dans le modèle\n\\(-2 \\cdot \\log(L)\\) est la déviance du modèle. Elle pénalise par 2 fois le nombre de paramètres\n\nLa fonction step() de R, nous permet d’effectuer une sélection descendante. On commence par le modèle complet incluant toutes les variables explicatives, puis on retire progressivement les variables qui contribuent le moins à l’ajustement du modèle, jusqu’à obtenir un modèle optimal.\n\nmodel_final &lt;- step(model_complet)\n\nStart:  AIC=362.02\nOutcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + \n    Insulin + BMI + DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- BloodPressure             1   344.04 360.04\n- Insulin                   1   344.42 360.42\n- SkinThickness             1   344.45 360.45\n&lt;none&gt;                          344.02 362.02\n- Pregnancies               1   346.24 362.24\n- Age                       1   347.55 363.55\n- BMI                       1   350.89 366.89\n- DiabetesPedigreeFunction  1   351.58 367.58\n- Glucose                   1   396.95 412.95\n\nStep:  AIC=360.04\nOutcome ~ Pregnancies + Glucose + SkinThickness + Insulin + BMI + \n    DiabetesPedigreeFunction + Age\n\n                           Df Deviance    AIC\n- Insulin                   1   344.42 358.42\n- SkinThickness             1   344.46 358.46\n&lt;none&gt;                          344.04 360.04\n- Pregnancies               1   346.24 360.24\n- Age                       1   347.60 361.60\n- BMI                       1   351.28 365.28\n- DiabetesPedigreeFunction  1   351.67 365.67\n- Glucose                   1   397.31 411.31\n\nStep:  AIC=358.42\nOutcome ~ Pregnancies + Glucose + SkinThickness + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n- SkinThickness             1   344.89 356.89\n&lt;none&gt;                          344.42 358.42\n- Pregnancies               1   346.74 358.74\n- Age                       1   347.87 359.87\n- BMI                       1   351.32 363.32\n- DiabetesPedigreeFunction  1   351.90 363.90\n- Glucose                   1   411.11 423.11\n\nStep:  AIC=356.89\nOutcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age\n\n                           Df Deviance    AIC\n&lt;none&gt;                          344.89 356.89\n- Pregnancies               1   347.23 357.23\n- Age                       1   348.72 358.72\n- DiabetesPedigreeFunction  1   352.72 362.72\n- BMI                       1   360.44 370.44\n- Glucose                   1   411.85 421.85\n\n\nLe meilleur modèle est celui possédant l’AIC le plus faible.\nIci, le meilleur modèle inclut les variables Age, Pregnancies, BMI, DiabetesPedigreeFunction et Age.\nCi-dessous les résultats de notre modèle.\n\nsummary(model_final)\n\n\nCall:\nglm(formula = Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + \n    Age, family = \"binomial\", data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8827  -0.6535  -0.3694   0.6521   2.5814  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -9.992080   1.086866  -9.193  &lt; 2e-16 ***\nPregnancies               0.083953   0.055031   1.526 0.127117    \nGlucose                   0.036458   0.004978   7.324 2.41e-13 ***\nBMI                       0.078139   0.020605   3.792 0.000149 ***\nDiabetesPedigreeFunction  1.150913   0.424242   2.713 0.006670 ** \nAge                       0.034360   0.017810   1.929 0.053692 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.89  on 386  degrees of freedom\nAIC: 356.89\n\nNumber of Fisher Scoring iterations: 5\n\n\nMis à part la variable Pregnancies, tous ont des \\(p-valeur\\) significative. Elles ont toutes un impact positif sur la probabilité du patient à avoir le diabète.\nMaintenant, nous calculons les odds-ratio de ce modèle.\n\n\n\n\n\nVariable\nOdd-ratios\n\n\n\n\nPregnancies\n1.087578\n\n\nGlucose\n1.037130\n\n\nBMI\n1.081273\n\n\nDiabetesPedigreeFunction\n3.161077\n\n\nAge\n1.034957\n\n\n\n\n\n\n\n\nVariable\nIC_inf\nIC_sup\n\n\n\n\nPregnancies\n0.9769517\n1.212971\n\n\nGlucose\n1.0274079\n1.047716\n\n\nBMI\n1.0395095\n1.127321\n\n\nDiabetesPedigreeFunction\n1.4016639\n7.398164\n\n\nAge\n0.9999761\n1.072664\n\n\n\n\n\n\n\nClassification avec le modèle de régression logistique\nNous allons maintenant classifier nos données à l’aide de notre modèle de régression logistique.\nDans un premier temps, nous allons séparer notre jeu de données en deux sous-échantillons.\n\nEchantillon d’apprentissage : Ce dernier contient 80 % de notre jeu de données. Notre modèle va apprendre à prédire sur cet échantillon.\nEchantillon de test : Cet échantillon contient les 20 % restants. Il va nous servir à tester notre modèle et à comparer les résultats avec les vraies valeurs de cet échantillon.\n\n\n# On fixe la graine (resultat reproductible)\nset.seed(75016)\n\n# Nombres d'observations dans notre jeu de donnees\nn &lt;- nrow(diabetes)\n\n# Nombres d'observations a selectionne dans l'echant d'apprentissage\nN &lt;- floor(n*0.8)\n\n# Selection des individus aleatoirement\nidx &lt;- sample(n, N, replace = F)\n\n# Echantillon d'apprentissage (80%)\ndataL &lt;- diabetes[idx,]\n\n# Echantillon de test (20%)\ndataV &lt;- diabetes[-c(idx),]\n\nOn entraîne notre modèle final (choisi précédemment à l’aide de l’AIC) sur l’échantillon d’apprentissage et on utilise la fonction predict qui permet de prédire pour tout individu de l’échantillon de test, sa probabilité d’être diabétique.\n\n# Entrainement du meilleur modele sur echantillon d'apprentissage (DataL)\nmodel_final_train &lt;- glm(Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + Age , data = dataL, family = \"binomial\")\n\n# Prediction de la probabilite d'etre diabetique sur echantillon de test (DataV)\nprediction_modele &lt;- predict(model_final_train,dataV, type = \"response\")\n\nNous calculons ensuite le taux de mauvaise classification moyen. Ce dernier est tout simplement une comparaison des classifications à leurs vraies étiquettes. On calcule ensuite le pourcentage de données mal classifiées.\nDans notre cas, nous allons créer une règle de classification. Si la prédiction est inférieure à 0.5 alors le patient prendre la valeur 0 (non-diabétique) dans le cas contraire, il prendra la valeur 1 (diabétique).\n\n# Regle de classification\nprediction_regle &lt;- ifelse(prediction_modele &gt;= 0.5, 1,0)\n\nerreur &lt;- mean(prediction_regle != dataV$Outcome)\npaste0(\"Le taux d'erreur moyen est de \", round(erreur,3)*100,\" %\")\n\n[1] \"Le taux d'erreur moyen est de 25.3 %\"\n\n\n\n\nComparaison de la régression logistique avec l’algorithme des k-plus proches voisins\nPrécédemment, nous avons réalisé une classification sur ce même jeu de données en utilisant l’algorithme des \\(k\\) plus proches voisins. La valeur de \\(k\\) optimal était \\(k\\) = 19, c’est-à-dire la valeur de \\(k\\) avec laquelle le taux de mauvaise classification était le plus faible.\nNous mettons donc en place notre algorithme des \\(k\\) plus proches voisins avec \\(k\\) = 19, et on réalise nos prédictions à l’aide de la fonction knn() du package class.\n\nprediction_knn &lt;- knn(train = dataL[,-9], test = dataV[,-9], cl = dataL[,9], k = 19, prob = F)\n# resultat des k plus proches voisins\nprediction_knn\n\n [1] 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0\n[77] 0 0 0\nLevels: 0 1\n\n\nOn calcule ensuite notre taux d’erreur moyen.\n\nerreur_knn &lt;- mean(prediction_knn != dataV[,9])\npaste0(\"Le taux d'erreur moyen est de \", round(erreur_knn,3)*100, \" %\")\n\n[1] \"Le taux d'erreur moyen est de 20.3 %\"\n\n\nMaintenant, que nous avons réalisé nos prédictions avec nos deux méthodes, nous sommes en mesure de comparer les deux taux d’erreur moyens. Le taux d’erreur le plus bas est évidemment préférable. Ici, les taux d’erreur moyens sont proches bien que celui de la régression logistique est plus élevée (25.3 %) que celui de l’algorithme des \\(k\\) plus proches voisins (20.3 %).\n\n\nConclusion\nEnfin, nous allons faire varier le seuil utilisé dans le critère de classification et calculer la sensibilité et la spécificité pour chacune des valeurs du seuil possible.\n\n# Evaluation de 100 seuils differents\nl &lt;- 100\n\n# Predicition\npreds &lt;- predict(model_final_train, dataV, type = \"response\")\n\n# Sequence de seuils\nc &lt;- seq(1.001 * min(preds), 0.999 * max(preds), length.out = l)\n\n# Initialisation des vecteurs sensibilite et specificite\nSe &lt;- rep(NA,l)\nSp &lt;- rep(NA,l)\n\nfor (j in 1:l) {\n  mod.final.classif &lt;- (preds &gt;= c[j])  # Classe (TRUE ou FALSE) de la prédiction\n  pt &lt;- table(mod.final.classif, dataV$Outcome)  # Comparaison des classes prédites\n  \n  if (nrow(pt) &gt;= 2 && ncol(pt) &gt;= 2) {\n\n    Se[j] &lt;- prop.table(pt, margin=2)[2, 2]  # Sensibilité\n    Sp[j] &lt;- prop.table(pt, margin=2)[1, 1]  # Spécificité\n  } else {\n\n    Se[j] &lt;- NA\n    Sp[j] &lt;- NA\n  }\n}\n\npar(mfrow=c(1,2))\nplot(c,Se,main=\"Sensibilité\",type='s', col = \"orange\")\nplot(c,Sp,main=\"Specificité\",type='s', col = \"purple\")\n\n\n\n\n\n\n\n\nSur les deux graphiques ci-dessus, la courbe de sensibilité montre comment le taux de détection des diabétiques varie en fonction des différents seuils de classification. Tandis que la courbe de spécificité montre comment le taux de détection des non-diabétiques varie également en fonction des différents seuils de classification.\nOn cherche à maximiser la sensibilité et la spécificité. Cependant une augmentation de la sensibilité peut entraîner une diminution de la spécificité et vice versa.\nEn somme, ces deux mesures permettent de déterminer à quel point le modèle est efficace dans la classification des cas positifs et négatifs, et d’ajuster les seuils de décision en fonction des besoins spécifiques.\nPuis, nous pouvons calculer la courbe ROC. Elle sert à évaluer la performance d’un modèle de classification binaire et en particulier les modèles qui prédisent une probabilité. Elle représente la sensibilité en fonction de 1 – spécificité pour toutes les valeurs seuils possibles du marqueur étudié.\n\npar(mfrow=c(1,1))\nplot(1-Sp,Se,type='s',main=\"Courbe ROC\")\nabline(0,1,col='blue')\n\n\n\n\n\n\n\n\nNotre classification est plutôt bonne au vu de la courbe ROC. La courbe est assez proche du coin supérieur gauche du graphique, ce qui indique un taux élevé de vrais positifs (sensibilité). Notre modèle arrive assez bien à identifier les personnes diabétiques en minimisant les erreurs de classification des cas négatifs. De plus, la courbe est largement supérieure à la courbe 0,1."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "À propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Université Paris Cité) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des données | Sept 2022 - Juin 2025\nCours suivis : Algèbre, Statistique, Probabilités, Programmation, Base de données, Économie"
  },
  {
    "objectID": "about.html#formation",
    "href": "about.html#formation",
    "title": "À propos",
    "section": "",
    "text": "IUT de Paris - Rives de Seine (Université Paris Cité) | Paris, France\n\n\n\nBachelor universitaire de technologie en Science des données | Sept 2022 - Juin 2025\nCours suivis : Algèbre, Statistique, Probabilités, Programmation, Base de données, Économie"
  },
  {
    "objectID": "about.html#expérience",
    "href": "about.html#expérience",
    "title": "À propos",
    "section": "Expérience",
    "text": "Expérience\n\n\n\n\n\n\nInstitut national de la statistique et des études économiques (INSEE) | Montrouge, France\n\n\n\nProgrammeur Statistique | 2023 - 2025 Au sein de la Direction des statistiques démographiques et sociales, j’ai mené des travaux d’appariement de données administratives. L’objectif de ces travaux était de mesurer et de comparer la couverture de deux bases de sondages. Par ailleurs, j’ai comparé différents algorithmes d’appariement à l’aide d’analyses statistiques.\nMes missions ont inclus la manipulation de données brutes, le nettoyage des données, l’appariement de grands volumes de données, ainsi que leur analyse statistique."
  },
  {
    "objectID": "about.html#compétences",
    "href": "about.html#compétences",
    "title": "À propos",
    "section": "Compétences",
    "text": "Compétences\n\n\n\n\n\n\nProgrammation\n\n\n\nPython | R | SQL | SAS | GIT\n\n\n\n\n\n\n\n\nLangue\n\n\n\nAnglais (B2) | Espagnol (B2) | Arabe (C2)"
  }
]